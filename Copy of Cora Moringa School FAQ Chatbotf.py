{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1sJdHoRhU0ktO1SYGyYAQgCtWTIwU4ebM","timestamp":1681723888986},{"file_id":"1DtHIFGdAgSyufbwquNHP6dnSRiCiaOtm","timestamp":1681410998436},{"file_id":"16Vj5M5zbPq0TO2QB8KMP8E4-o6ZOPOjL","timestamp":1681155851697},{"file_id":"1AeB4F9oAsGLOdPotC-GRyxIuv2jX0ttU","timestamp":1681152278654}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3ylY2RHRaB1","executionInfo":{"status":"ok","timestamp":1681722779791,"user_tz":-180,"elapsed":28177,"user":{"displayName":"Urbanus Kathitu","userId":"01017823760186719028"}},"outputId":"72105eb7-ace1-421e-b272-37aacc536bfd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tflearn\n","  Downloading tflearn-0.5.0.tar.gz (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tflearn) (1.22.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from tflearn) (1.16.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from tflearn) (8.4.0)\n","Building wheels for collected packages: tflearn\n","  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=5a52460f6cd55a52bb9734cae5788e628a07ceb69f87da7c779dca624a962017\n","  Stored in directory: /root/.cache/pip/wheels/4a/d5/f8/9585b4a100c0fd73da204ee785457d67c85e1b9050f009a849\n","Successfully built tflearn\n","Installing collected packages: tflearn\n","Successfully installed tflearn-0.5.0\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["#Importing the necessary libraries\n","!pip install nltk\n","!pip install tflearn\n","import numpy\n","import tflearn\n","import tensorflow\n","import random\n","import json\n","import nltk\n","nltk.download('punkt')\n","from tensorflow.python.framework import ops\n","from nltk.stem.lancaster import LancasterStemmer\n","stemmer = LancasterStemmer()\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9vPN33jHfMTm","executionInfo":{"status":"ok","timestamp":1681722819076,"user_tz":-180,"elapsed":39312,"user":{"displayName":"Urbanus Kathitu","userId":"01017823760186719028"}},"outputId":"567e4ae6-efcc-4779-daa7-7764496c823e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#Loading the Json file with the intents,patterns and responses\n","import json\n","with open('/content/FAQs') as file:\n","\n","    data = json.load(file)\n","print (data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VNwUkVqoR14K","executionInfo":{"status":"ok","timestamp":1681722871355,"user_tz":-180,"elapsed":712,"user":{"displayName":"Urbanus Kathitu","userId":"01017823760186719028"}},"outputId":"41041e46-4759-457c-cbbb-0b80c12ac72e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'intents': [{'tag': 'greeting', 'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day', 'Whats up'], 'responses': ['Hello!', 'Good to see you again!', 'Hi there, how can I help?'], 'context_set': ''}, {'tag': 'goodbye', 'patterns': ['cya', 'See you later', 'Goodbye', 'I am Leaving', 'Have a Good day'], 'responses': ['Sad to see you go :(', 'Talk to you later', 'Goodbye!'], 'context_set': ''}, {'tag': 'accommodation', 'patterns': ['do you offer accommodation', 'are there boarding facilities?', 'do you have hostels ?'], 'responses': ['No, we currently don’t provide accommodation. However, there are a number of private hostels along Ngong road, which are close to the school.'], 'context_set': ''}, {'tag': 'remote', 'patterns': ['Do you offer remote learning courses?', 'Are there online classes?', 'Can i take the course online?', 'Can i take virtual classes?', 'Do you offer part time classes?'], 'responses': ['Moringa School’s remote classes are done live and online; with daily sessions where learners ask questions and interact with tutors and fellow students'], 'context_set': ''}, {'tag': 'data_bundles', 'patterns': ['Do students that are in the remote classes get data bundles to facilitate their studies?', 'What kind of support do remote class students get ?'], 'responses': ['Yes, students that have enrolled to our remote classes are provided with data bundles'], 'context_set': ''}, {'tag': 'price', 'patterns': ['What is the price of remote learning and in-person learning?', 'How much is the course?', 'How much are the fees?'], 'responses': ['The program pacing and course content for both remote and in-person classes are the same. For the in-person classes, students are allowed to be in our premises 2 days per week where they get to meet and interact with their trainers – for the remaining 3 days of the week you will still have live and instructor-led classes but they will be delivered online. For remote classes, all class interactions happen live and in real-time online. Remote classes cost Ksh43,000 and Ksh131,000 for Prep and Core respectively. In-person classes cost Ksh50,000 and Ksh150,000 for Prep and Core respectively.'], 'context_set': ''}, {'tag': 'difference', 'patterns': ['What is the difference between remote learning and in-person learning?'], 'responses': ['With in-person learning, we factor in the utilities and operational costs that help us maintain a great learning environment for our students while being physically present in our premises. We also pay keen attention to the spacing among students during in-person instruction to promote social distancing'], 'context_set': ''}, {'tag': 'certificate', 'patterns': ['Will I get a certificate upon completion?', 'Do i get  certificate?', 'Is there certification?'], 'responses': ['Yes, you will get a digital certificate after the successful completion of the program. A physical certificate can be printed for you at your cost.'], 'context_set': ''}, {'tag': 'programming', 'patterns': ['What if I’m not familiar with programming at all?', 'Do i need to have programming experience?', 'What if i cannot code?'], 'responses': ['Our Moringa Prep course introduces you to basic programming. This course is for beginners to learn the fundamentals of programming and for more established developers to sharpen their grasp on front-end skills. Visit the courses page to learn more.'], 'context_set': ''}, {'tag': 'job', 'patterns': ['Will I get a job after completion of the program?', 'Do you help students get jobs?', 'Am i assured of a job after?'], 'responses': ['That depends entirely on your aptitude and attitude. While we provide world-class teachers and experts to support you, the learning is on you. We put in our best and we expect you to put in yours as well. We do not guarantee jobs but we will help facilitate the process by assisting in your professional development, providing resume-writing workshops, optimising your LinkedIn profile, conducting mock interviews, and hosting recruitment drives with our employer partners.'], 'context_set': ''}, {'tag': 'financial_aid', 'patterns': ['Is financial aid available?', 'What about financial aid?', 'Can i get financial aid?'], 'responses': ['We offer financial aid to our students on a need and merit basis. The options that are currently available include partial scholarships for Moringa Prep and student loans for Moringa Core. For more information on financial aid please visit our financial aid page using this link'], 'context_set': ''}, {'tag': 'installments', 'patterns': ['Are there installment plans ?', 'Can i pay in installments?', 'How can i pay in installmets?'], 'responses': ['All enrolling students are required to pay upfront tuition of Ksh43,000 for live online classes and KSh50,000 for hybrid in-person classes by the last Friday before a Prep class starts. Installments plans for core program are available here: https://moringaschool.com/wp-content/uploads/2022/10/Financing-Moringa-Data-Science-Part-Time-FI-Remote-Installment-Plan-DSF-PT4-May-8th-Class.docx.pdf'], 'context_set': ''}, {'tag': 'requirements', 'patterns': ['What Qualifications should one have?', 'What are the entry level requirements?'], 'responses': ['We are currently registered under TVET (Technical and Vocational Education and Training). We require all our students to have gone through high school as is required by law (TVET) and should be computer proficient.'], 'context_set': ''}, {'tag': 'certification', 'patterns': ['Do you offer certificates and are they recognised by the Kenya Government?'], 'responses': ['Moringa School is registered in Kenya by the Technical & Vocational Education and Training Authority(TVETA). However, we DO NOT offer academic qualifications that can be exchanged for credits in our local colleges and universities. We are in the process of working with TVETA to have our training standards certified which means that we will become the to-go-to when it comes to tech training standards in Kenya. Additionally, we encourage and assist our students to build their portfolios which is a better marketing tool e.g in a course like Software Development which involves coming up with independent projects that build up technical skills.'], 'context_set': ''}, {'tag': 'employment', 'patterns': ['Do you guarantee employment upon completion of this course?', 'Am i assured of employment after completion?'], 'responses': ['NO. What we guarantee is career-ready skills. Our courses are practical and relevant to the market. That is why many of our graduates find jobs. Moringa School offers support through training and informing graduates of job opportunities. Graduates then apply and some get these jobs. Other graduates have found jobs without our direct support while others have ventured into freelancing and entrepreneurship. Moringa School is proud to have helped all these brilliant young people achieve their career goals.'], 'context_set': ''}, {'tag': 'contact', 'patterns': ['How can I contact Moringa School?', 'How do I contact Moringa?', 'How can I reach out to Moringa?', 'Do Moringa have a contact?'], 'responses': ['For General Inquiries you can reach us at : +254205002167,For Admissions you can reach us at : +254207643533 You can also reach us on Whatsapp at : +254795872461'], 'context_set': ''}, {'tag': 'courses', 'patterns': ['What courses do you offer at Moringa?', 'What courses are available at Moringa?', 'What courses can I apply for at Moringa?', 'What courses do you have?'], 'responses': ['software engineering online, software engineering hybrid, software engineering part-time, data science full-time, data science part-time, product design (UI/UX) full-time, product design (UI/UX) part-time'], 'context_set': ''}, {'tag': 'courses', 'patterns': ['Do you have installment plans for prep class?', 'Are there installment plans for prep?'], 'responses': ['No, we do not offer installment plans for the Prep course but we do for the core course'], 'context_set': ''}]}\n"]}]},{"cell_type":"code","source":["#Extracting Data\n","#For each pattern we will turn it into a list of words using nltk.word_tokenizer. \n","#We will then add each pattern into our docs_x list and its associated tag into the docs_y list\n","words = []\n","labels = []\n","docs_x = []\n","docs_y = []\n","\n","for intent in data['intents']:\n","    for pattern in intent['patterns']:\n","        wrds = nltk.word_tokenize(pattern)\n","        words.extend(wrds)\n","        docs_x.append(wrds)\n","        docs_y.append(intent[\"tag\"])\n","        \n","    if intent['tag'] not in labels:\n","        labels.append(intent['tag'])"],"metadata":{"id":"XQomBGt-SH0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Stemming our words\n","words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n","words = sorted(list(set(words)))\n","\n","#create a unique list of stemmed word\n","labels = sorted(labels)"],"metadata":{"id":"pwjmOtwjSQHg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Coverting our sentencing to a bag of words.\n","training = []\n","output = []\n","\n","out_empty = [0 for _ in range(len(labels))]\n","\n","for x, doc in enumerate(docs_x):\n","    bag = []\n","\n","    wrds = [stemmer.stem(w.lower()) for w in doc]\n","\n","    for w in words:\n","        if w in wrds:\n","            bag.append(1)\n","        else:\n","            bag.append(0)\n","\n","    output_row = out_empty[:]\n","    output_row[labels.index(docs_y[x])] = 1\n","\n","    training.append(bag)\n","    output.append(output_row)"],"metadata":{"id":"VIw7aXfoSUTh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Vconvert our training data and output to numpy arraystraining = numpy.array(training)\n","output = numpy.array(output)"],"metadata":{"id":"-dD8zlg5SX6R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#defining the architecture of our model\n","ops.reset_default_graph()\n","\n","net = tflearn.input_data(shape=[None, len(training[0])])\n","net = tflearn.fully_connected(net, 10)\n","net = tflearn.fully_connected(net, 10)\n","net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n","net = tflearn.regression(net)\n","\n","model = tflearn.DNN(net)\n"],"metadata":{"id":"5PMZDk8RScMY","executionInfo":{"status":"ok","timestamp":1681722893105,"user_tz":-180,"elapsed":554,"user":{"displayName":"Urbanus Kathitu","userId":"01017823760186719028"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b47c9702-732c-42f5-b132-967122d57d5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tflearn/initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"]}]},{"cell_type":"code","source":["#Training the Model\n","\n","model.fit(training, output, n_epoch=1000, batch_size=5, show_metric=True)\n"],"metadata":{"id":"oVv-w7ZGVjPG","outputId":"c843413c-c934-4c57-83c0-fd6b5ca66a9f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","| Adam | epoch: 696 | loss: 0.40919 - acc: 0.9749 -- iter: 05/55\n","Training Step: 7647  | total loss: \u001b[1m\u001b[32m0.36854\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 696 | loss: 0.36854 - acc: 0.9774 -- iter: 10/55\n","Training Step: 7648  | total loss: \u001b[1m\u001b[32m0.33208\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 696 | loss: 0.33208 - acc: 0.9797 -- iter: 15/55\n","Training Step: 7649  | total loss: \u001b[1m\u001b[32m0.29922\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 696 | loss: 0.29922 - acc: 0.9817 -- iter: 20/55\n","Training Step: 7650  | total loss: \u001b[1m\u001b[32m0.26948\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 696 | loss: 0.26948 - acc: 0.9835 -- iter: 25/55\n","Training Step: 7651  | total loss: \u001b[1m\u001b[32m0.24300\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 696 | loss: 0.24300 - acc: 0.9852 -- iter: 30/55\n","Training Step: 7652  | total loss: \u001b[1m\u001b[32m0.21968\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 696 | loss: 0.21968 - acc: 0.9867 -- iter: 35/55\n","Training Step: 7653  | total loss: \u001b[1m\u001b[32m0.19792\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 696 | loss: 0.19792 - acc: 0.9880 -- iter: 40/55\n","Training Step: 7654  | total loss: \u001b[1m\u001b[32m0.17853\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 696 | loss: 0.17853 - acc: 0.9892 -- iter: 45/55\n","Training Step: 7655  | total loss: \u001b[1m\u001b[32m0.16093\u001b[0m\u001b[0m | time: 0.059s\n","| Adam | epoch: 696 | loss: 0.16093 - acc: 0.9903 -- iter: 50/55\n","Training Step: 7656  | total loss: \u001b[1m\u001b[32m0.14509\u001b[0m\u001b[0m | time: 0.063s\n","| Adam | epoch: 696 | loss: 0.14509 - acc: 0.9912 -- iter: 55/55\n","--\n","Training Step: 7657  | total loss: \u001b[1m\u001b[32m0.13106\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 697 | loss: 0.13106 - acc: 0.9921 -- iter: 05/55\n","Training Step: 7658  | total loss: \u001b[1m\u001b[32m0.11809\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 697 | loss: 0.11809 - acc: 0.9929 -- iter: 10/55\n","Training Step: 7659  | total loss: \u001b[1m\u001b[32m1.86699\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 697 | loss: 1.86699 - acc: 0.8936 -- iter: 15/55\n","Training Step: 7660  | total loss: \u001b[1m\u001b[32m1.68038\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 697 | loss: 1.68038 - acc: 0.9043 -- iter: 20/55\n","Training Step: 7661  | total loss: \u001b[1m\u001b[32m1.51281\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 697 | loss: 1.51281 - acc: 0.9138 -- iter: 25/55\n","Training Step: 7662  | total loss: \u001b[1m\u001b[32m1.36176\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 697 | loss: 1.36176 - acc: 0.9224 -- iter: 30/55\n","Training Step: 7663  | total loss: \u001b[1m\u001b[32m1.22599\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 697 | loss: 1.22599 - acc: 0.9302 -- iter: 35/55\n","Training Step: 7664  | total loss: \u001b[1m\u001b[32m1.22599\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 697 | loss: 1.22599 - acc: 0.9372 -- iter: 40/55\n","Training Step: 7665  | total loss: \u001b[1m\u001b[32m1.10382\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 697 | loss: 1.10382 - acc: 0.9435 -- iter: 45/55\n","Training Step: 7666  | total loss: \u001b[1m\u001b[32m0.99381\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 697 | loss: 0.99381 - acc: 0.9491 -- iter: 50/55\n","Training Step: 7667  | total loss: \u001b[1m\u001b[32m0.89513\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 697 | loss: 0.89513 - acc: 0.9542 -- iter: 55/55\n","--\n","Training Step: 7668  | total loss: \u001b[1m\u001b[32m0.72583\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 698 | loss: 0.72583 - acc: 0.9588 -- iter: 05/55\n","Training Step: 7669  | total loss: \u001b[1m\u001b[32m0.65439\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 698 | loss: 0.65439 - acc: 0.9629 -- iter: 10/55\n","Training Step: 7670  | total loss: \u001b[1m\u001b[32m0.65439\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 698 | loss: 0.65439 - acc: 0.9629 -- iter: 15/55\n","Training Step: 7671  | total loss: \u001b[1m\u001b[32m0.53056\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 698 | loss: 0.53056 - acc: 0.9700 -- iter: 20/55\n","Training Step: 7672  | total loss: \u001b[1m\u001b[32m0.47819\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 698 | loss: 0.47819 - acc: 0.9730 -- iter: 25/55\n","Training Step: 7673  | total loss: \u001b[1m\u001b[32m0.47819\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 698 | loss: 0.47819 - acc: 0.9757 -- iter: 30/55\n","Training Step: 7674  | total loss: \u001b[1m\u001b[32m0.43104\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 698 | loss: 0.43104 - acc: 0.9781 -- iter: 35/55\n","Training Step: 7675  | total loss: \u001b[1m\u001b[32m0.38819\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 698 | loss: 0.38819 - acc: 0.9803 -- iter: 40/55\n","Training Step: 7676  | total loss: \u001b[1m\u001b[32m0.34978\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 698 | loss: 0.34978 - acc: 0.9803 -- iter: 45/55\n","Training Step: 7677  | total loss: \u001b[1m\u001b[32m0.31521\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 698 | loss: 0.31521 - acc: 0.9840 -- iter: 50/55\n","Training Step: 7678  | total loss: \u001b[1m\u001b[32m0.28393\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 698 | loss: 0.28393 - acc: 0.9856 -- iter: 55/55\n","--\n","Training Step: 7685  | total loss: \u001b[1m\u001b[32m0.14029\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 699 | loss: 0.14029 - acc: 0.9931 -- iter: 35/55\n","Training Step: 7686  | total loss: \u001b[1m\u001b[32m0.12823\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 699 | loss: 0.12823 - acc: 0.9938 -- iter: 40/55\n","Training Step: 7687  | total loss: \u001b[1m\u001b[32m0.11579\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 699 | loss: 0.11579 - acc: 0.9944 -- iter: 45/55\n","Training Step: 7688  | total loss: \u001b[1m\u001b[32m0.10455\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 699 | loss: 0.10455 - acc: 0.9950 -- iter: 50/55\n","Training Step: 7689  | total loss: \u001b[1m\u001b[32m0.09430\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 699 | loss: 0.09430 - acc: 0.9955 -- iter: 55/55\n","--\n","Training Step: 7690  | total loss: \u001b[1m\u001b[32m0.08541\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 700 | loss: 0.08541 - acc: 0.9959 -- iter: 05/55\n","Training Step: 7691  | total loss: \u001b[1m\u001b[32m0.07747\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 700 | loss: 0.07747 - acc: 0.9963 -- iter: 10/55\n","Training Step: 7692  | total loss: \u001b[1m\u001b[32m0.06992\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 700 | loss: 0.06992 - acc: 0.9967 -- iter: 15/55\n","Training Step: 7693  | total loss: \u001b[1m\u001b[32m0.06313\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 700 | loss: 0.06313 - acc: 0.9970 -- iter: 20/55\n","Training Step: 7694  | total loss: \u001b[1m\u001b[32m0.05740\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 700 | loss: 0.05740 - acc: 0.9973 -- iter: 25/55\n","Training Step: 7695  | total loss: \u001b[1m\u001b[32m0.05211\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 700 | loss: 0.05211 - acc: 0.9976 -- iter: 30/55\n","Training Step: 7696  | total loss: \u001b[1m\u001b[32m0.04701\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 700 | loss: 0.04701 - acc: 0.9978 -- iter: 35/55\n","Training Step: 7697  | total loss: \u001b[1m\u001b[32m0.03882\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 700 | loss: 0.03882 - acc: 0.9981 -- iter: 40/55\n","Training Step: 7698  | total loss: \u001b[1m\u001b[32m0.03528\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 700 | loss: 0.03528 - acc: 0.9983 -- iter: 45/55\n","Training Step: 7699  | total loss: \u001b[1m\u001b[32m0.03528\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 700 | loss: 0.03528 - acc: 0.9984 -- iter: 50/55\n","Training Step: 7700  | total loss: \u001b[1m\u001b[32m0.02903\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 700 | loss: 0.02903 - acc: 0.9986 -- iter: 55/55\n","--\n","Training Step: 7701  | total loss: \u001b[1m\u001b[32m0.02658\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 701 | loss: 0.02658 - acc: 0.9987 -- iter: 05/55\n","Training Step: 7702  | total loss: \u001b[1m\u001b[32m0.02415\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 701 | loss: 0.02415 - acc: 0.9989 -- iter: 10/55\n","Training Step: 7703  | total loss: \u001b[1m\u001b[32m0.02252\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 701 | loss: 0.02252 - acc: 0.9990 -- iter: 15/55\n","Training Step: 7704  | total loss: \u001b[1m\u001b[32m0.02106\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 701 | loss: 0.02106 - acc: 0.9991 -- iter: 20/55\n","Training Step: 7705  | total loss: \u001b[1m\u001b[32m0.01942\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 701 | loss: 0.01942 - acc: 0.9992 -- iter: 25/55\n","Training Step: 7706  | total loss: \u001b[1m\u001b[32m0.01756\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 701 | loss: 0.01756 - acc: 0.9992 -- iter: 30/55\n","Training Step: 7707  | total loss: \u001b[1m\u001b[32m0.01651\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 701 | loss: 0.01651 - acc: 0.9993 -- iter: 35/55\n","Training Step: 7708  | total loss: \u001b[1m\u001b[32m0.01588\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 701 | loss: 0.01588 - acc: 0.9994 -- iter: 40/55\n","Training Step: 7709  | total loss: \u001b[1m\u001b[32m0.01453\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 701 | loss: 0.01453 - acc: 0.9994 -- iter: 45/55\n","Training Step: 7710  | total loss: \u001b[1m\u001b[32m0.01360\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 701 | loss: 0.01360 - acc: 0.9995 -- iter: 50/55\n","Training Step: 7711  | total loss: \u001b[1m\u001b[32m0.01360\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 701 | loss: 0.01360 - acc: 0.9996 -- iter: 55/55\n","--\n","Training Step: 7712  | total loss: \u001b[1m\u001b[32m0.01157\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 702 | loss: 0.01157 - acc: 0.9996 -- iter: 05/55\n","Training Step: 7713  | total loss: \u001b[1m\u001b[32m0.01059\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 702 | loss: 0.01059 - acc: 0.9996 -- iter: 10/55\n","Training Step: 7714  | total loss: \u001b[1m\u001b[32m0.00973\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 702 | loss: 0.00973 - acc: 0.9997 -- iter: 15/55\n","Training Step: 7715  | total loss: \u001b[1m\u001b[32m0.00915\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 702 | loss: 0.00915 - acc: 0.9997 -- iter: 20/55\n","Training Step: 7716  | total loss: \u001b[1m\u001b[32m0.00862\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 702 | loss: 0.00862 - acc: 0.9997 -- iter: 25/55\n","Training Step: 7717  | total loss: \u001b[1m\u001b[32m0.00799\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 702 | loss: 0.00799 - acc: 0.9997 -- iter: 30/55\n","Training Step: 7718  | total loss: \u001b[1m\u001b[32m0.00762\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 702 | loss: 0.00762 - acc: 0.9998 -- iter: 35/55\n","Training Step: 7719  | total loss: \u001b[1m\u001b[32m0.00709\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 702 | loss: 0.00709 - acc: 0.9998 -- iter: 40/55\n","Training Step: 7720  | total loss: \u001b[1m\u001b[32m0.00709\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 702 | loss: 0.00709 - acc: 0.9998 -- iter: 45/55\n","Training Step: 7721  | total loss: \u001b[1m\u001b[32m0.00632\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 702 | loss: 0.00632 - acc: 0.9998 -- iter: 50/55\n","Training Step: 7722  | total loss: \u001b[1m\u001b[32m0.00580\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 702 | loss: 0.00580 - acc: 0.9999 -- iter: 55/55\n","--\n","Training Step: 7723  | total loss: \u001b[1m\u001b[32m0.00538\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 703 | loss: 0.00538 - acc: 0.9999 -- iter: 05/55\n","Training Step: 7724  | total loss: \u001b[1m\u001b[32m0.00521\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 703 | loss: 0.00521 - acc: 0.9999 -- iter: 10/55\n","Training Step: 7725  | total loss: \u001b[1m\u001b[32m0.00529\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 703 | loss: 0.00529 - acc: 0.9999 -- iter: 15/55\n","Training Step: 7726  | total loss: \u001b[1m\u001b[32m0.00501\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 703 | loss: 0.00501 - acc: 0.9999 -- iter: 20/55\n","Training Step: 7727  | total loss: \u001b[1m\u001b[32m0.00519\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 703 | loss: 0.00519 - acc: 0.9999 -- iter: 25/55\n","Training Step: 7728  | total loss: \u001b[1m\u001b[32m0.00534\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 703 | loss: 0.00534 - acc: 0.9999 -- iter: 30/55\n","Training Step: 7729  | total loss: \u001b[1m\u001b[32m0.00523\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 703 | loss: 0.00523 - acc: 0.9999 -- iter: 35/55\n","Training Step: 7730  | total loss: \u001b[1m\u001b[32m0.00502\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 703 | loss: 0.00502 - acc: 0.9999 -- iter: 40/55\n","Training Step: 7731  | total loss: \u001b[1m\u001b[32m0.00476\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 703 | loss: 0.00476 - acc: 0.9999 -- iter: 45/55\n","Training Step: 7732  | total loss: \u001b[1m\u001b[32m0.00480\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 703 | loss: 0.00480 - acc: 1.0000 -- iter: 50/55\n","Training Step: 7733  | total loss: \u001b[1m\u001b[32m0.00481\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 703 | loss: 0.00481 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 7734  | total loss: \u001b[1m\u001b[32m0.00446\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 704 | loss: 0.00446 - acc: 1.0000 -- iter: 05/55\n","Training Step: 7735  | total loss: \u001b[1m\u001b[32m0.00429\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 704 | loss: 0.00429 - acc: 1.0000 -- iter: 10/55\n","Training Step: 7736  | total loss: \u001b[1m\u001b[32m0.00416\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 704 | loss: 0.00416 - acc: 1.0000 -- iter: 15/55\n","Training Step: 7737  | total loss: \u001b[1m\u001b[32m0.00404\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 704 | loss: 0.00404 - acc: 1.0000 -- iter: 20/55\n","Training Step: 7738  | total loss: \u001b[1m\u001b[32m0.00406\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 704 | loss: 0.00406 - acc: 1.0000 -- iter: 25/55\n","Training Step: 7739  | total loss: \u001b[1m\u001b[32m0.00410\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 704 | loss: 0.00410 - acc: 1.0000 -- iter: 30/55\n","Training Step: 7740  | total loss: \u001b[1m\u001b[32m0.00413\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 704 | loss: 0.00413 - acc: 1.0000 -- iter: 35/55\n","Training Step: 7741  | total loss: \u001b[1m\u001b[32m0.00387\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 704 | loss: 0.00387 - acc: 1.0000 -- iter: 40/55\n","Training Step: 7742  | total loss: \u001b[1m\u001b[32m0.00360\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 704 | loss: 0.00360 - acc: 1.0000 -- iter: 45/55\n","Training Step: 7743  | total loss: \u001b[1m\u001b[32m0.00366\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 704 | loss: 0.00366 - acc: 1.0000 -- iter: 50/55\n","Training Step: 7744  | total loss: \u001b[1m\u001b[32m0.00358\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 704 | loss: 0.00358 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 7745  | total loss: \u001b[1m\u001b[32m0.00355\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 705 | loss: 0.00355 - acc: 1.0000 -- iter: 05/55\n","Training Step: 7746  | total loss: \u001b[1m\u001b[32m0.00375\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 705 | loss: 0.00375 - acc: 1.0000 -- iter: 10/55\n","Training Step: 7747  | total loss: \u001b[1m\u001b[32m0.00375\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 705 | loss: 0.00375 - acc: 1.0000 -- iter: 15/55\n","Training Step: 7748  | total loss: \u001b[1m\u001b[32m0.00375\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 705 | loss: 0.00375 - acc: 1.0000 -- iter: 20/55\n","Training Step: 7749  | total loss: \u001b[1m\u001b[32m0.00353\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 705 | loss: 0.00353 - acc: 1.0000 -- iter: 25/55\n","Training Step: 7750  | total loss: \u001b[1m\u001b[32m0.00348\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 705 | loss: 0.00348 - acc: 1.0000 -- iter: 30/55\n","Training Step: 7751  | total loss: \u001b[1m\u001b[32m0.00333\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 705 | loss: 0.00333 - acc: 1.0000 -- iter: 35/55\n","Training Step: 7752  | total loss: \u001b[1m\u001b[32m0.00333\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 705 | loss: 0.00333 - acc: 1.0000 -- iter: 40/55\n","Training Step: 7753  | total loss: \u001b[1m\u001b[32m0.00312\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 705 | loss: 0.00312 - acc: 1.0000 -- iter: 45/55\n","Training Step: 7754  | total loss: \u001b[1m\u001b[32m0.00316\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 705 | loss: 0.00316 - acc: 1.0000 -- iter: 50/55\n","Training Step: 7755  | total loss: \u001b[1m\u001b[32m0.00311\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 705 | loss: 0.00311 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 7756  | total loss: \u001b[1m\u001b[32m0.00297\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 706 | loss: 0.00297 - acc: 1.0000 -- iter: 05/55\n","Training Step: 7757  | total loss: \u001b[1m\u001b[32m0.00287\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 706 | loss: 0.00287 - acc: 1.0000 -- iter: 10/55\n","Training Step: 7758  | total loss: \u001b[1m\u001b[32m0.00281\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 706 | loss: 0.00281 - acc: 1.0000 -- iter: 15/55\n","Training Step: 7759  | total loss: \u001b[1m\u001b[32m0.00295\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 706 | loss: 0.00295 - acc: 1.0000 -- iter: 20/55\n","Training Step: 7760  | total loss: \u001b[1m\u001b[32m0.00312\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 706 | loss: 0.00312 - acc: 1.0000 -- iter: 25/55\n","Training Step: 7761  | total loss: \u001b[1m\u001b[32m0.00304\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 706 | loss: 0.00304 - acc: 1.0000 -- iter: 30/55\n","Training Step: 7762  | total loss: \u001b[1m\u001b[32m0.00309\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 706 | loss: 0.00309 - acc: 1.0000 -- iter: 35/55\n","Training Step: 7763  | total loss: \u001b[1m\u001b[32m0.00316\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 706 | loss: 0.00316 - acc: 1.0000 -- iter: 40/55\n","Training Step: 7764  | total loss: \u001b[1m\u001b[32m0.00322\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 706 | loss: 0.00322 - acc: 1.0000 -- iter: 45/55\n","Training Step: 7765  | total loss: \u001b[1m\u001b[32m0.00324\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 706 | loss: 0.00324 - acc: 1.0000 -- iter: 50/55\n","Training Step: 7766  | total loss: \u001b[1m\u001b[32m0.00310\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 706 | loss: 0.00310 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 7767  | total loss: \u001b[1m\u001b[32m0.00314\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 707 | loss: 0.00314 - acc: 1.0000 -- iter: 05/55\n","Training Step: 7768  | total loss: \u001b[1m\u001b[32m0.00335\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 707 | loss: 0.00335 - acc: 1.0000 -- iter: 10/55\n","Training Step: 7769  | total loss: \u001b[1m\u001b[32m0.00339\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 707 | loss: 0.00339 - acc: 1.0000 -- iter: 15/55\n","Training Step: 7770  | total loss: \u001b[1m\u001b[32m0.00343\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 707 | loss: 0.00343 - acc: 1.0000 -- iter: 20/55\n","Training Step: 7771  | total loss: \u001b[1m\u001b[32m0.00320\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 707 | loss: 0.00320 - acc: 1.0000 -- iter: 25/55\n","Training Step: 7772  | total loss: \u001b[1m\u001b[32m0.00306\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 707 | loss: 0.00306 - acc: 1.0000 -- iter: 30/55\n","Training Step: 7773  | total loss: \u001b[1m\u001b[32m0.00324\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 707 | loss: 0.00324 - acc: 1.0000 -- iter: 35/55\n","Training Step: 7774  | total loss: \u001b[1m\u001b[32m0.00316\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 707 | loss: 0.00316 - acc: 1.0000 -- iter: 40/55\n","Training Step: 7775  | total loss: \u001b[1m\u001b[32m0.00315\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 707 | loss: 0.00315 - acc: 1.0000 -- iter: 45/55\n","Training Step: 7776  | total loss: \u001b[1m\u001b[32m0.00315\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 707 | loss: 0.00315 - acc: 1.0000 -- iter: 50/55\n","Training Step: 7777  | total loss: \u001b[1m\u001b[32m0.00315\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 707 | loss: 0.00315 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 7778  | total loss: \u001b[1m\u001b[32m0.00291\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 708 | loss: 0.00291 - acc: 1.0000 -- iter: 05/55\n","Training Step: 7779  | total loss: \u001b[1m\u001b[32m0.00287\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 708 | loss: 0.00287 - acc: 1.0000 -- iter: 10/55\n","Training Step: 7780  | total loss: \u001b[1m\u001b[32m0.00283\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 708 | loss: 0.00283 - acc: 1.0000 -- iter: 15/55\n","Training Step: 7781  | total loss: \u001b[1m\u001b[32m0.00280\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 708 | loss: 0.00280 - acc: 1.0000 -- iter: 20/55\n","Training Step: 7782  | total loss: \u001b[1m\u001b[32m0.00275\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 708 | loss: 0.00275 - acc: 1.0000 -- iter: 25/55\n","Training Step: 7783  | total loss: \u001b[1m\u001b[32m0.00275\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 708 | loss: 0.00275 - acc: 1.0000 -- iter: 30/55\n","Training Step: 7784  | total loss: \u001b[1m\u001b[32m0.00285\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 708 | loss: 0.00285 - acc: 1.0000 -- iter: 35/55\n","Training Step: 7785  | total loss: \u001b[1m\u001b[32m0.00269\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 708 | loss: 0.00269 - acc: 1.0000 -- iter: 40/55\n","Training Step: 7786  | total loss: \u001b[1m\u001b[32m0.00287\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 708 | loss: 0.00287 - acc: 1.0000 -- iter: 45/55\n","Training Step: 7787  | total loss: \u001b[1m\u001b[32m0.00284\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 708 | loss: 0.00284 - acc: 1.0000 -- iter: 50/55\n","Training Step: 7788  | total loss: \u001b[1m\u001b[32m0.00282\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 708 | loss: 0.00282 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 7789  | total loss: \u001b[1m\u001b[32m0.00281\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 709 | loss: 0.00281 - acc: 1.0000 -- iter: 05/55\n","Training Step: 7790  | total loss: \u001b[1m\u001b[32m0.00298\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 709 | loss: 0.00298 - acc: 1.0000 -- iter: 10/55\n","Training Step: 7791  | total loss: \u001b[1m\u001b[32m0.00299\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 709 | loss: 0.00299 - acc: 1.0000 -- iter: 15/55\n","Training Step: 7792  | total loss: \u001b[1m\u001b[32m0.00273\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 709 | loss: 0.00273 - acc: 1.0000 -- iter: 20/55\n","Training Step: 7793  | total loss: \u001b[1m\u001b[32m0.00273\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 709 | loss: 0.00273 - acc: 1.0000 -- iter: 25/55\n","Training Step: 7794  | total loss: \u001b[1m\u001b[32m0.00264\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 709 | loss: 0.00264 - acc: 1.0000 -- iter: 30/55\n","Training Step: 7795  | total loss: \u001b[1m\u001b[32m0.00270\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 709 | loss: 0.00270 - acc: 1.0000 -- iter: 35/55\n","Training Step: 7796  | total loss: \u001b[1m\u001b[32m0.00298\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 709 | loss: 0.00298 - acc: 1.0000 -- iter: 40/55\n","Training Step: 7797  | total loss: \u001b[1m\u001b[32m0.00284\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 709 | loss: 0.00284 - acc: 1.0000 -- iter: 45/55\n","Training Step: 7798  | total loss: \u001b[1m\u001b[32m0.00263\u001b[0m\u001b[0m | time: 0.062s\n","| Adam | epoch: 709 | loss: 0.00263 - acc: 1.0000 -- iter: 50/55\n","Training Step: 7799  | total loss: \u001b[1m\u001b[32m0.00290\u001b[0m\u001b[0m | time: 0.066s\n","| Adam | epoch: 709 | loss: 0.00290 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 7800  | total loss: \u001b[1m\u001b[32m0.00290\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 710 | loss: 0.00290 - acc: 1.0000 -- iter: 05/55\n","Training Step: 7801  | total loss: \u001b[1m\u001b[32m0.00317\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 710 | loss: 0.00317 - acc: 1.0000 -- iter: 10/55\n","Training Step: 7802  | total loss: \u001b[1m\u001b[32m0.00317\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 710 | loss: 0.00317 - acc: 1.0000 -- iter: 15/55\n","Training Step: 7803  | total loss: \u001b[1m\u001b[32m0.00319\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 710 | loss: 0.00319 - acc: 1.0000 -- iter: 20/55\n","Training Step: 7804  | total loss: \u001b[1m\u001b[32m0.00305\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 710 | loss: 0.00305 - acc: 1.0000 -- iter: 25/55\n","Training Step: 7805  | total loss: \u001b[1m\u001b[32m0.00283\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 710 | loss: 0.00283 - acc: 1.0000 -- iter: 30/55\n","Training Step: 7806  | total loss: \u001b[1m\u001b[32m0.00260\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 710 | loss: 0.00260 - acc: 1.0000 -- iter: 35/55\n","Training Step: 7807  | total loss: \u001b[1m\u001b[32m0.00259\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 710 | loss: 0.00259 - acc: 1.0000 -- iter: 40/55\n","Training Step: 7808  | total loss: \u001b[1m\u001b[32m0.00255\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 710 | loss: 0.00255 - acc: 1.0000 -- iter: 45/55\n","Training Step: 7809  | total loss: \u001b[1m\u001b[32m0.00281\u001b[0m\u001b[0m | time: 0.069s\n","| Adam | epoch: 710 | loss: 0.00281 - acc: 1.0000 -- iter: 50/55\n","Training Step: 7810  | total loss: \u001b[1m\u001b[32m0.00281\u001b[0m\u001b[0m | time: 0.073s\n","| Adam | epoch: 710 | loss: 0.00281 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 7811  | total loss: \u001b[1m\u001b[32m0.00287\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 711 | loss: 0.00287 - acc: 1.0000 -- iter: 05/55\n","Training Step: 7812  | total loss: \u001b[1m\u001b[32m0.00284\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 711 | loss: 0.00284 - acc: 1.0000 -- iter: 10/55\n","Training Step: 7813  | total loss: \u001b[1m\u001b[32m0.00303\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 711 | loss: 0.00303 - acc: 1.0000 -- iter: 15/55\n","Training Step: 7814  | total loss: \u001b[1m\u001b[32m0.00301\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 711 | loss: 0.00301 - acc: 1.0000 -- iter: 20/55\n","Training Step: 7815  | total loss: \u001b[1m\u001b[32m0.00295\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 711 | loss: 0.00295 - acc: 1.0000 -- iter: 25/55\n","Training Step: 7816  | total loss: \u001b[1m\u001b[32m0.00292\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 711 | loss: 0.00292 - acc: 1.0000 -- iter: 30/55\n","Training Step: 7817  | total loss: \u001b[1m\u001b[32m0.00285\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 711 | loss: 0.00285 - acc: 1.0000 -- iter: 35/55\n","Training Step: 7818  | total loss: \u001b[1m\u001b[32m0.00281\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 711 | loss: 0.00281 - acc: 1.0000 -- iter: 40/55\n","Training Step: 7819  | total loss: \u001b[1m\u001b[32m0.00283\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 711 | loss: 0.00283 - acc: 1.0000 -- iter: 45/55\n","Training Step: 7820  | total loss: \u001b[1m\u001b[32m0.00283\u001b[0m\u001b[0m | time: 0.066s\n","| Adam | epoch: 711 | loss: 0.00283 - acc: 1.0000 -- iter: 50/55\n","Training Step: 7821  | total loss: \u001b[1m\u001b[32m0.00263\u001b[0m\u001b[0m | time: 0.073s\n","| Adam | epoch: 711 | loss: 0.00263 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 7822  | total loss: \u001b[1m\u001b[32m0.00276\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 712 | loss: 0.00276 - acc: 1.0000 -- iter: 05/55\n","Training Step: 7823  | total loss: \u001b[1m\u001b[32m0.00290\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 712 | loss: 0.00290 - acc: 1.0000 -- iter: 10/55\n","Training Step: 7824  | total loss: \u001b[1m\u001b[32m0.00298\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 712 | loss: 0.00298 - acc: 1.0000 -- iter: 15/55\n","Training Step: 7825  | total loss: \u001b[1m\u001b[32m0.00298\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 712 | loss: 0.00298 - acc: 1.0000 -- iter: 20/55\n","Training Step: 7826  | total loss: \u001b[1m\u001b[32m0.00281\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 712 | loss: 0.00281 - acc: 1.0000 -- iter: 25/55\n","Training Step: 7827  | total loss: \u001b[1m\u001b[32m0.00270\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 712 | loss: 0.00270 - acc: 1.0000 -- iter: 30/55\n","Training Step: 7828  | total loss: \u001b[1m\u001b[32m0.00277\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 712 | loss: 0.00277 - acc: 1.0000 -- iter: 35/55\n","Training Step: 7829  | total loss: \u001b[1m\u001b[32m0.00277\u001b[0m\u001b[0m | time: 0.060s\n","| Adam | epoch: 712 | loss: 0.00277 - acc: 1.0000 -- iter: 40/55\n","Training Step: 7830  | total loss: \u001b[1m\u001b[32m0.00255\u001b[0m\u001b[0m | time: 0.068s\n","| Adam | epoch: 712 | loss: 0.00255 - acc: 1.0000 -- iter: 45/55\n","Training Step: 7831  | total loss: \u001b[1m\u001b[32m0.00255\u001b[0m\u001b[0m | time: 0.076s\n","| Adam | epoch: 712 | loss: 0.00255 - acc: 1.0000 -- iter: 50/55\n","Training Step: 7832  | total loss: \u001b[1m\u001b[32m0.00272\u001b[0m\u001b[0m | time: 0.084s\n","| Adam | epoch: 712 | loss: 0.00272 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 7833  | total loss: \u001b[1m\u001b[32m0.00265\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 713 | loss: 0.00265 - acc: 1.0000 -- iter: 05/55\n","Training Step: 7834  | total loss: \u001b[1m\u001b[32m0.00297\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 713 | loss: 0.00297 - acc: 1.0000 -- iter: 10/55\n","Training Step: 7835  | total loss: \u001b[1m\u001b[32m0.00297\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 713 | loss: 0.00297 - acc: 1.0000 -- iter: 15/55\n","Training Step: 7836  | total loss: \u001b[1m\u001b[32m0.00267\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 713 | loss: 0.00267 - acc: 1.0000 -- iter: 20/55\n","Training Step: 7837  | total loss: \u001b[1m\u001b[32m0.00267\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 713 | loss: 0.00267 - acc: 1.0000 -- iter: 25/55\n","Training Step: 7838  | total loss: \u001b[1m\u001b[32m0.00244\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 713 | loss: 0.00244 - acc: 1.0000 -- iter: 30/55\n","Training Step: 7839  | total loss: \u001b[1m\u001b[32m0.00244\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 713 | loss: 0.00244 - acc: 1.0000 -- iter: 35/55\n","Training Step: 7840  | total loss: \u001b[1m\u001b[32m1.43732\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 713 | loss: 1.43732 - acc: 0.9100 -- iter: 40/55\n","Training Step: 7841  | total loss: \u001b[1m\u001b[32m1.43732\u001b[0m\u001b[0m | time: 0.064s\n","| Adam | epoch: 713 | loss: 1.43732 - acc: 0.9100 -- iter: 45/55\n","Training Step: 7842  | total loss: \u001b[1m\u001b[32m1.29408\u001b[0m\u001b[0m | time: 0.071s\n","| Adam | epoch: 713 | loss: 1.29408 - acc: 0.9190 -- iter: 50/55\n","Training Step: 7843  | total loss: \u001b[1m\u001b[32m1.16508\u001b[0m\u001b[0m | time: 0.078s\n","| Adam | epoch: 713 | loss: 1.16508 - acc: 0.9271 -- iter: 55/55\n","--\n","Training Step: 7844  | total loss: \u001b[1m\u001b[32m0.94417\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 714 | loss: 0.94417 - acc: 0.9344 -- iter: 05/55\n","Training Step: 7845  | total loss: \u001b[1m\u001b[32m0.94417\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 714 | loss: 0.94417 - acc: 0.9410 -- iter: 10/55\n","Training Step: 7846  | total loss: \u001b[1m\u001b[32m0.76535\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 714 | loss: 0.76535 - acc: 0.9469 -- iter: 15/55\n","Training Step: 7847  | total loss: \u001b[1m\u001b[32m0.76535\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 714 | loss: 0.76535 - acc: 0.9522 -- iter: 20/55\n","Training Step: 7848  | total loss: \u001b[1m\u001b[32m0.62026\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 714 | loss: 0.62026 - acc: 0.9570 -- iter: 25/55\n","Training Step: 7849  | total loss: \u001b[1m\u001b[32m0.62026\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 714 | loss: 0.62026 - acc: 0.9613 -- iter: 30/55\n","Training Step: 7850  | total loss: \u001b[1m\u001b[32m0.50279\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 714 | loss: 0.50279 - acc: 0.9651 -- iter: 35/55\n","Training Step: 7851  | total loss: \u001b[1m\u001b[32m0.50279\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 714 | loss: 0.50279 - acc: 0.9686 -- iter: 40/55\n","Training Step: 7852  | total loss: \u001b[1m\u001b[32m0.40833\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 714 | loss: 0.40833 - acc: 0.9746 -- iter: 45/55\n","Training Step: 7853  | total loss: \u001b[1m\u001b[32m0.36781\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 714 | loss: 0.36781 - acc: 0.9771 -- iter: 50/55\n","Training Step: 7854  | total loss: \u001b[1m\u001b[32m0.33142\u001b[0m\u001b[0m | time: 0.063s\n","| Adam | epoch: 714 | loss: 0.33142 - acc: 0.9794 -- iter: 55/55\n","--\n","Training Step: 7855  | total loss: \u001b[1m\u001b[32m0.33142\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 715 | loss: 0.33142 - acc: 0.9794 -- iter: 05/55\n","Training Step: 7856  | total loss: \u001b[1m\u001b[32m0.26891\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 715 | loss: 0.26891 - acc: 0.9833 -- iter: 10/55\n","Training Step: 7857  | total loss: \u001b[1m\u001b[32m0.24239\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 715 | loss: 0.24239 - acc: 0.9850 -- iter: 15/55\n","Training Step: 7858  | total loss: \u001b[1m\u001b[32m0.21835\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 715 | loss: 0.21835 - acc: 0.9865 -- iter: 20/55\n","Training Step: 7859  | total loss: \u001b[1m\u001b[32m0.21835\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 715 | loss: 0.21835 - acc: 0.9865 -- iter: 25/55\n","Training Step: 7860  | total loss: \u001b[1m\u001b[32m0.17702\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 715 | loss: 0.17702 - acc: 0.9891 -- iter: 30/55\n","Training Step: 7861  | total loss: \u001b[1m\u001b[32m0.15949\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 715 | loss: 0.15949 - acc: 0.9902 -- iter: 35/55\n","Training Step: 7862  | total loss: \u001b[1m\u001b[32m0.14384\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 715 | loss: 0.14384 - acc: 0.9911 -- iter: 40/55\n","Training Step: 7863  | total loss: \u001b[1m\u001b[32m0.13003\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 715 | loss: 0.13003 - acc: 0.9920 -- iter: 45/55\n","Training Step: 7864  | total loss: \u001b[1m\u001b[32m0.11740\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 715 | loss: 0.11740 - acc: 0.9928 -- iter: 50/55\n","Training Step: 7865  | total loss: \u001b[1m\u001b[32m0.10592\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 715 | loss: 0.10592 - acc: 0.9935 -- iter: 55/55\n","--\n","Training Step: 7866  | total loss: \u001b[1m\u001b[32m0.10592\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 716 | loss: 0.10592 - acc: 0.9942 -- iter: 05/55\n","Training Step: 7867  | total loss: \u001b[1m\u001b[32m0.09556\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 716 | loss: 0.09556 - acc: 0.9948 -- iter: 10/55\n","Training Step: 7868  | total loss: \u001b[1m\u001b[32m0.08610\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 716 | loss: 0.08610 - acc: 0.9953 -- iter: 15/55\n","Training Step: 7869  | total loss: \u001b[1m\u001b[32m0.07817\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 716 | loss: 0.07817 - acc: 0.9958 -- iter: 20/55\n","Training Step: 7870  | total loss: \u001b[1m\u001b[32m0.07062\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 716 | loss: 0.07062 - acc: 0.9962 -- iter: 25/55\n","Training Step: 7871  | total loss: \u001b[1m\u001b[32m0.06386\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 716 | loss: 0.06386 - acc: 0.9966 -- iter: 30/55\n","Training Step: 7872  | total loss: \u001b[1m\u001b[32m0.05759\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 716 | loss: 0.05759 - acc: 0.9969 -- iter: 35/55\n","Training Step: 7873  | total loss: \u001b[1m\u001b[32m0.04713\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 716 | loss: 0.04713 - acc: 0.9972 -- iter: 40/55\n","Training Step: 7874  | total loss: \u001b[1m\u001b[32m0.04713\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 716 | loss: 0.04713 - acc: 0.9972 -- iter: 45/55\n","Training Step: 7875  | total loss: \u001b[1m\u001b[32m0.04272\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 716 | loss: 0.04272 - acc: 0.9975 -- iter: 50/55\n","Training Step: 7876  | total loss: \u001b[1m\u001b[32m0.03554\u001b[0m\u001b[0m | time: 0.062s\n","| Adam | epoch: 716 | loss: 0.03554 - acc: 0.9977 -- iter: 55/55\n","--\n","Training Step: 7877  | total loss: \u001b[1m\u001b[32m0.03554\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 717 | loss: 0.03554 - acc: 0.9980 -- iter: 05/55\n","Training Step: 7878  | total loss: \u001b[1m\u001b[32m0.03220\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 717 | loss: 0.03220 - acc: 0.9982 -- iter: 10/55\n","Training Step: 7879  | total loss: \u001b[1m\u001b[32m0.02926\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 717 | loss: 0.02926 - acc: 0.9984 -- iter: 15/55\n","Training Step: 7880  | total loss: \u001b[1m\u001b[32m0.02654\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 717 | loss: 0.02654 - acc: 0.9985 -- iter: 20/55\n","Training Step: 7881  | total loss: \u001b[1m\u001b[32m0.02442\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 717 | loss: 0.02442 - acc: 0.9987 -- iter: 25/55\n","Training Step: 7882  | total loss: \u001b[1m\u001b[32m0.02012\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 717 | loss: 0.02012 - acc: 0.9988 -- iter: 30/55\n","Training Step: 7883  | total loss: \u001b[1m\u001b[32m0.02012\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 717 | loss: 0.02012 - acc: 0.9989 -- iter: 35/55\n","Training Step: 7884  | total loss: \u001b[1m\u001b[32m0.01832\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 717 | loss: 0.01832 - acc: 0.9990 -- iter: 40/55\n","Training Step: 7885  | total loss: \u001b[1m\u001b[32m0.01669\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 717 | loss: 0.01669 - acc: 0.9991 -- iter: 45/55\n","Training Step: 7886  | total loss: \u001b[1m\u001b[32m0.01589\u001b[0m\u001b[0m | time: 0.062s\n","| Adam | epoch: 717 | loss: 0.01589 - acc: 0.9992 -- iter: 50/55\n","Training Step: 7887  | total loss: \u001b[1m\u001b[32m1.43996\u001b[0m\u001b[0m | time: 0.068s\n","| Adam | epoch: 717 | loss: 1.43996 - acc: 0.9993 -- iter: 55/55\n","--\n","Training Step: 7888  | total loss: \u001b[1m\u001b[32m1.43996\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 718 | loss: 1.43996 - acc: 0.8994 -- iter: 05/55\n","Training Step: 7889  | total loss: \u001b[1m\u001b[32m1.16697\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 718 | loss: 1.16697 - acc: 0.9094 -- iter: 10/55\n","Training Step: 7890  | total loss: \u001b[1m\u001b[32m1.16697\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 718 | loss: 1.16697 - acc: 0.9185 -- iter: 15/55\n","Training Step: 7891  | total loss: \u001b[1m\u001b[32m1.05047\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 718 | loss: 1.05047 - acc: 0.9266 -- iter: 20/55\n","Training Step: 7892  | total loss: \u001b[1m\u001b[32m0.94590\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 718 | loss: 0.94590 - acc: 0.9340 -- iter: 25/55\n","Training Step: 7893  | total loss: \u001b[1m\u001b[32m0.76689\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 718 | loss: 0.76689 - acc: 0.9406 -- iter: 30/55\n","Training Step: 7894  | total loss: \u001b[1m\u001b[32m0.76689\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 718 | loss: 0.76689 - acc: 0.9465 -- iter: 35/55\n","Training Step: 7895  | total loss: \u001b[1m\u001b[32m0.69054\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 718 | loss: 0.69054 - acc: 0.9519 -- iter: 40/55\n","Training Step: 7896  | total loss: \u001b[1m\u001b[32m0.62184\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 718 | loss: 0.62184 - acc: 0.9567 -- iter: 45/55\n","Training Step: 7897  | total loss: \u001b[1m\u001b[32m0.56002\u001b[0m\u001b[0m | time: 0.065s\n","| Adam | epoch: 718 | loss: 0.56002 - acc: 0.9610 -- iter: 50/55\n","Training Step: 7898  | total loss: \u001b[1m\u001b[32m0.50855\u001b[0m\u001b[0m | time: 0.072s\n","| Adam | epoch: 718 | loss: 0.50855 - acc: 0.9649 -- iter: 55/55\n","--\n","Training Step: 7899  | total loss: \u001b[1m\u001b[32m0.45813\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 719 | loss: 0.45813 - acc: 0.9684 -- iter: 05/55\n","Training Step: 7900  | total loss: \u001b[1m\u001b[32m0.41253\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 719 | loss: 0.41253 - acc: 0.9716 -- iter: 10/55\n","Training Step: 7901  | total loss: \u001b[1m\u001b[32m0.37184\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 719 | loss: 0.37184 - acc: 0.9744 -- iter: 15/55\n","Training Step: 7902  | total loss: \u001b[1m\u001b[32m0.33518\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 719 | loss: 0.33518 - acc: 0.9770 -- iter: 20/55\n","Training Step: 7903  | total loss: \u001b[1m\u001b[32m0.30191\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 719 | loss: 0.30191 - acc: 0.9793 -- iter: 25/55\n","Training Step: 7904  | total loss: \u001b[1m\u001b[32m0.27213\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 719 | loss: 0.27213 - acc: 0.9814 -- iter: 30/55\n","Training Step: 7905  | total loss: \u001b[1m\u001b[32m0.24508\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 719 | loss: 0.24508 - acc: 0.9849 -- iter: 35/55\n","Training Step: 7906  | total loss: \u001b[1m\u001b[32m0.22091\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 719 | loss: 0.22091 - acc: 0.9849 -- iter: 40/55\n","Training Step: 7907  | total loss: \u001b[1m\u001b[32m0.19888\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 719 | loss: 0.19888 - acc: 0.9878 -- iter: 45/55\n","Training Step: 7908  | total loss: \u001b[1m\u001b[32m0.17937\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 719 | loss: 0.17937 - acc: 0.9890 -- iter: 50/55\n","Training Step: 7909  | total loss: \u001b[1m\u001b[32m0.16181\u001b[0m\u001b[0m | time: 0.062s\n","| Adam | epoch: 719 | loss: 0.16181 - acc: 0.9901 -- iter: 55/55\n","--\n","Training Step: 7910  | total loss: \u001b[1m\u001b[32m0.14614\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 720 | loss: 0.14614 - acc: 0.9911 -- iter: 05/55\n","Training Step: 7911  | total loss: \u001b[1m\u001b[32m0.13184\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 720 | loss: 0.13184 - acc: 0.9120 -- iter: 10/55\n","Training Step: 7912  | total loss: \u001b[1m\u001b[32m1.80825\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 720 | loss: 1.80825 - acc: 0.9208 -- iter: 15/55\n","Training Step: 7913  | total loss: \u001b[1m\u001b[32m1.62792\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 720 | loss: 1.62792 - acc: 0.9287 -- iter: 20/55\n","Training Step: 7914  | total loss: \u001b[1m\u001b[32m1.46528\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 720 | loss: 1.46528 - acc: 0.9287 -- iter: 25/55\n","Training Step: 7915  | total loss: \u001b[1m\u001b[32m1.31909\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 720 | loss: 1.31909 - acc: 0.9358 -- iter: 30/55\n","Training Step: 7916  | total loss: \u001b[1m\u001b[32m1.18771\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 720 | loss: 1.18771 - acc: 0.9422 -- iter: 35/55\n","Training Step: 7917  | total loss: \u001b[1m\u001b[32m1.06979\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 720 | loss: 1.06979 - acc: 0.9532 -- iter: 40/55\n","Training Step: 7918  | total loss: \u001b[1m\u001b[32m0.96310\u001b[0m\u001b[0m | time: 0.059s\n","| Adam | epoch: 720 | loss: 0.96310 - acc: 0.9579 -- iter: 45/55\n","Training Step: 7919  | total loss: \u001b[1m\u001b[32m0.86687\u001b[0m\u001b[0m | time: 0.065s\n","| Adam | epoch: 720 | loss: 0.86687 - acc: 0.9621 -- iter: 50/55\n","Training Step: 7920  | total loss: \u001b[1m\u001b[32m0.78040\u001b[0m\u001b[0m | time: 0.072s\n","| Adam | epoch: 720 | loss: 0.78040 - acc: 0.9659 -- iter: 55/55\n","--\n","Training Step: 7921  | total loss: \u001b[1m\u001b[32m0.70259\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 721 | loss: 0.70259 - acc: 0.9693 -- iter: 05/55\n","Training Step: 7922  | total loss: \u001b[1m\u001b[32m0.63268\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 721 | loss: 0.63268 - acc: 0.9693 -- iter: 10/55\n","Training Step: 7923  | total loss: \u001b[1m\u001b[32m0.56964\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 721 | loss: 0.56964 - acc: 0.9724 -- iter: 15/55\n","Training Step: 7924  | total loss: \u001b[1m\u001b[32m0.51310\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 721 | loss: 0.51310 - acc: 0.9751 -- iter: 20/55\n","Training Step: 7925  | total loss: \u001b[1m\u001b[32m0.46253\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 721 | loss: 0.46253 - acc: 0.9776 -- iter: 25/55\n","Training Step: 7926  | total loss: \u001b[1m\u001b[32m0.41661\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 721 | loss: 0.41661 - acc: 0.9799 -- iter: 30/55\n","Training Step: 7927  | total loss: \u001b[1m\u001b[32m0.37528\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 721 | loss: 0.37528 - acc: 0.9819 -- iter: 35/55\n","Training Step: 7928  | total loss: \u001b[1m\u001b[32m0.33854\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 721 | loss: 0.33854 - acc: 0.9837 -- iter: 40/55\n","Training Step: 7929  | total loss: \u001b[1m\u001b[32m0.30514\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 721 | loss: 0.30514 - acc: 0.9853 -- iter: 45/55\n","Training Step: 7930  | total loss: \u001b[1m\u001b[32m0.27473\u001b[0m\u001b[0m | time: 0.060s\n","| Adam | epoch: 721 | loss: 0.27473 - acc: 0.9868 -- iter: 50/55\n","Training Step: 7931  | total loss: \u001b[1m\u001b[32m0.24759\u001b[0m\u001b[0m | time: 0.067s\n","| Adam | epoch: 721 | loss: 0.24759 - acc: 0.9881 -- iter: 55/55\n","--\n","Training Step: 7932  | total loss: \u001b[1m\u001b[32m0.22304\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 722 | loss: 0.22304 - acc: 0.9893 -- iter: 05/55\n","Training Step: 7933  | total loss: \u001b[1m\u001b[32m0.20094\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 722 | loss: 0.20094 - acc: 0.9904 -- iter: 10/55\n","Training Step: 7934  | total loss: \u001b[1m\u001b[32m0.18146\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 722 | loss: 0.18146 - acc: 0.9913 -- iter: 15/55\n","Training Step: 7935  | total loss: \u001b[1m\u001b[32m0.14747\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 722 | loss: 0.14747 - acc: 0.9922 -- iter: 20/55\n","Training Step: 7936  | total loss: \u001b[1m\u001b[32m0.14747\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 722 | loss: 0.14747 - acc: 0.9930 -- iter: 25/55\n","Training Step: 7937  | total loss: \u001b[1m\u001b[32m0.13301\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 722 | loss: 0.13301 - acc: 0.9937 -- iter: 30/55\n","Training Step: 7938  | total loss: \u001b[1m\u001b[32m0.11976\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 722 | loss: 0.11976 - acc: 0.9943 -- iter: 35/55\n","Training Step: 7939  | total loss: \u001b[1m\u001b[32m0.10810\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 722 | loss: 0.10810 - acc: 0.9949 -- iter: 40/55\n","Training Step: 7940  | total loss: \u001b[1m\u001b[32m0.09786\u001b[0m\u001b[0m | time: 0.059s\n","| Adam | epoch: 722 | loss: 0.09786 - acc: 0.9954 -- iter: 45/55\n","Training Step: 7941  | total loss: \u001b[1m\u001b[32m0.08842\u001b[0m\u001b[0m | time: 0.066s\n","| Adam | epoch: 722 | loss: 0.08842 - acc: 0.9959 -- iter: 50/55\n","Training Step: 7942  | total loss: \u001b[1m\u001b[32m0.07985\u001b[0m\u001b[0m | time: 0.073s\n","| Adam | epoch: 722 | loss: 0.07985 - acc: 0.9963 -- iter: 55/55\n","--\n","Training Step: 7943  | total loss: \u001b[1m\u001b[32m0.07232\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 723 | loss: 0.07232 - acc: 0.9966 -- iter: 05/55\n","Training Step: 7944  | total loss: \u001b[1m\u001b[32m0.06533\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 723 | loss: 0.06533 - acc: 0.9970 -- iter: 10/55\n","Training Step: 7945  | total loss: \u001b[1m\u001b[32m0.05399\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 723 | loss: 0.05399 - acc: 0.9973 -- iter: 15/55\n","Training Step: 7946  | total loss: \u001b[1m\u001b[32m0.05399\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 723 | loss: 0.05399 - acc: 0.9976 -- iter: 20/55\n","Training Step: 8008  | total loss: \u001b[1m\u001b[32m2.63330\u001b[0m\u001b[0m | time: 0.069s\n","| Adam | epoch: 728 | loss: 2.63330 - acc: 0.8825 -- iter: 55/55\n","--\n","Training Step: 8009  | total loss: \u001b[1m\u001b[32m2.37018\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 729 | loss: 2.37018 - acc: 0.8942 -- iter: 05/55\n","Training Step: 8010  | total loss: \u001b[1m\u001b[32m2.13528\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 729 | loss: 2.13528 - acc: 0.9048 -- iter: 10/55\n","Training Step: 8011  | total loss: \u001b[1m\u001b[32m1.92557\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 729 | loss: 1.92557 - acc: 0.9143 -- iter: 15/55\n","Training Step: 8012  | total loss: \u001b[1m\u001b[32m1.73320\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 729 | loss: 1.73320 - acc: 0.9229 -- iter: 20/55\n","Training Step: 8013  | total loss: \u001b[1m\u001b[32m1.56032\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 729 | loss: 1.56032 - acc: 0.9306 -- iter: 25/55\n","Training Step: 8014  | total loss: \u001b[1m\u001b[32m1.40487\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 729 | loss: 1.40487 - acc: 0.9375 -- iter: 30/55\n","Training Step: 8015  | total loss: \u001b[1m\u001b[32m1.26493\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 729 | loss: 1.26493 - acc: 0.9438 -- iter: 35/55\n","Training Step: 8016  | total loss: \u001b[1m\u001b[32m1.13875\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 729 | loss: 1.13875 - acc: 0.9494 -- iter: 40/55\n","Training Step: 8017  | total loss: \u001b[1m\u001b[32m1.02519\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 729 | loss: 1.02519 - acc: 0.9545 -- iter: 45/55\n","Training Step: 8018  | total loss: \u001b[1m\u001b[32m0.92319\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 729 | loss: 0.92319 - acc: 0.9590 -- iter: 50/55\n","Training Step: 8019  | total loss: \u001b[1m\u001b[32m0.74853\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 729 | loss: 0.74853 - acc: 0.9631 -- iter: 55/55\n","--\n","Training Step: 8020  | total loss: \u001b[1m\u001b[32m0.67429\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 730 | loss: 0.67429 - acc: 0.9668 -- iter: 05/55\n","Training Step: 8021  | total loss: \u001b[1m\u001b[32m0.60728\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 730 | loss: 0.60728 - acc: 0.9701 -- iter: 10/55\n","Training Step: 8022  | total loss: \u001b[1m\u001b[32m0.54681\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 730 | loss: 0.54681 - acc: 0.9731 -- iter: 15/55\n","Training Step: 8023  | total loss: \u001b[1m\u001b[32m0.49268\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 730 | loss: 0.49268 - acc: 0.9758 -- iter: 20/55\n","Training Step: 8024  | total loss: \u001b[1m\u001b[32m0.44480\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 730 | loss: 0.44480 - acc: 0.9758 -- iter: 25/55\n","Training Step: 8025  | total loss: \u001b[1m\u001b[32m0.40050\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 730 | loss: 0.40050 - acc: 0.9782 -- iter: 30/55\n","Training Step: 8026  | total loss: \u001b[1m\u001b[32m0.36095\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 730 | loss: 0.36095 - acc: 0.9824 -- iter: 35/55\n","Training Step: 8027  | total loss: \u001b[1m\u001b[32m0.36095\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 730 | loss: 0.36095 - acc: 0.9824 -- iter: 40/55\n","Training Step: 8028  | total loss: \u001b[1m\u001b[32m0.29269\u001b[0m\u001b[0m | time: 0.060s\n","| Adam | epoch: 730 | loss: 0.29269 - acc: 0.9857 -- iter: 45/55\n","Training Step: 8029  | total loss: \u001b[1m\u001b[32m0.26358\u001b[0m\u001b[0m | time: 0.067s\n","| Adam | epoch: 730 | loss: 0.26358 - acc: 0.9871 -- iter: 50/55\n","Training Step: 8030  | total loss: \u001b[1m\u001b[32m0.26358\u001b[0m\u001b[0m | time: 0.071s\n","| Adam | epoch: 730 | loss: 0.26358 - acc: 0.9884 -- iter: 55/55\n","--\n","Training Step: 8031  | total loss: \u001b[1m\u001b[32m0.23762\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 731 | loss: 0.23762 - acc: 0.8896 -- iter: 05/55\n","Training Step: 8032  | total loss: \u001b[1m\u001b[32m1.86421\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 731 | loss: 1.86421 - acc: 0.9006 -- iter: 10/55\n","Training Step: 8033  | total loss: \u001b[1m\u001b[32m1.67900\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 731 | loss: 1.67900 - acc: 0.9006 -- iter: 15/55\n","Training Step: 8034  | total loss: \u001b[1m\u001b[32m1.51159\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 731 | loss: 1.51159 - acc: 0.9106 -- iter: 20/55\n","Training Step: 8035  | total loss: \u001b[1m\u001b[32m1.36075\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 731 | loss: 1.36075 - acc: 0.9195 -- iter: 25/55\n","Training Step: 8036  | total loss: \u001b[1m\u001b[32m1.22504\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 731 | loss: 1.22504 - acc: 0.9276 -- iter: 30/55\n","Training Step: 8037  | total loss: \u001b[1m\u001b[32m1.10278\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 731 | loss: 1.10278 - acc: 0.9348 -- iter: 35/55\n","Training Step: 8038  | total loss: \u001b[1m\u001b[32m0.99274\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 731 | loss: 0.99274 - acc: 0.9413 -- iter: 40/55\n","Training Step: 8039  | total loss: \u001b[1m\u001b[32m0.89448\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 731 | loss: 0.89448 - acc: 0.9472 -- iter: 45/55\n","Training Step: 8040  | total loss: \u001b[1m\u001b[32m0.80613\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 731 | loss: 0.80613 - acc: 0.9525 -- iter: 50/55\n","Training Step: 8041  | total loss: \u001b[1m\u001b[32m0.72587\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 731 | loss: 0.72587 - acc: 0.9572 -- iter: 55/55\n","--\n","Training Step: 8042  | total loss: \u001b[1m\u001b[32m0.65356\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 732 | loss: 0.65356 - acc: 0.9615 -- iter: 05/55\n","Training Step: 8043  | total loss: \u001b[1m\u001b[32m0.58864\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 732 | loss: 0.58864 - acc: 0.9654 -- iter: 10/55\n","Training Step: 8044  | total loss: \u001b[1m\u001b[32m0.53002\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 732 | loss: 0.53002 - acc: 0.9688 -- iter: 15/55\n","Training Step: 8045  | total loss: \u001b[1m\u001b[32m0.53002\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 732 | loss: 0.53002 - acc: 0.9747 -- iter: 20/55\n","Training Step: 8046  | total loss: \u001b[1m\u001b[32m0.47741\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 732 | loss: 0.47741 - acc: 0.9773 -- iter: 25/55\n","Training Step: 8047  | total loss: \u001b[1m\u001b[32m0.42982\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 732 | loss: 0.42982 - acc: 0.9773 -- iter: 30/55\n","Training Step: 8048  | total loss: \u001b[1m\u001b[32m0.38740\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 732 | loss: 0.38740 - acc: 0.9795 -- iter: 35/55\n","Training Step: 8049  | total loss: \u001b[1m\u001b[32m0.31577\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 732 | loss: 0.31577 - acc: 0.9816 -- iter: 40/55\n","Training Step: 8050  | total loss: \u001b[1m\u001b[32m0.28458\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 732 | loss: 0.28458 - acc: 0.9834 -- iter: 45/55\n","Training Step: 8051  | total loss: \u001b[1m\u001b[32m0.25649\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 732 | loss: 0.25649 - acc: 0.9851 -- iter: 50/55\n","Training Step: 8052  | total loss: \u001b[1m\u001b[32m0.23121\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 732 | loss: 0.23121 - acc: 0.9866 -- iter: 55/55\n","--\n","Training Step: 8053  | total loss: \u001b[1m\u001b[32m0.20888\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 733 | loss: 0.20888 - acc: 0.9879 -- iter: 05/55\n","Training Step: 8054  | total loss: \u001b[1m\u001b[32m0.18835\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 733 | loss: 0.18835 - acc: 0.9891 -- iter: 10/55\n","Training Step: 8055  | total loss: \u001b[1m\u001b[32m0.16977\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 733 | loss: 0.16977 - acc: 0.9912 -- iter: 15/55\n","Training Step: 8056  | total loss: \u001b[1m\u001b[32m0.16977\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 733 | loss: 0.16977 - acc: 0.9921 -- iter: 20/55\n","Training Step: 8057  | total loss: \u001b[1m\u001b[32m0.15350\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 733 | loss: 0.15350 - acc: 0.9929 -- iter: 25/55\n","Training Step: 8058  | total loss: \u001b[1m\u001b[32m0.13849\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 733 | loss: 0.13849 - acc: 0.9936 -- iter: 30/55\n","Training Step: 8059  | total loss: \u001b[1m\u001b[32m0.12491\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 733 | loss: 0.12491 - acc: 0.9942 -- iter: 35/55\n","Training Step: 8060  | total loss: \u001b[1m\u001b[32m0.11266\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 733 | loss: 0.11266 - acc: 0.9948 -- iter: 40/55\n","Training Step: 8061  | total loss: \u001b[1m\u001b[32m0.10157\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 733 | loss: 0.10157 - acc: 0.9948 -- iter: 45/55\n","Training Step: 8062  | total loss: \u001b[1m\u001b[32m0.09267\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 733 | loss: 0.09267 - acc: 0.9953 -- iter: 50/55\n","Training Step: 8063  | total loss: \u001b[1m\u001b[32m0.08397\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 733 | loss: 0.08397 - acc: 0.9958 -- iter: 55/55\n","--\n","Training Step: 8064  | total loss: \u001b[1m\u001b[32m0.07624\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 734 | loss: 0.07624 - acc: 0.9962 -- iter: 05/55\n","Training Step: 8065  | total loss: \u001b[1m\u001b[32m0.06928\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 734 | loss: 0.06928 - acc: 0.9966 -- iter: 10/55\n","Training Step: 8066  | total loss: \u001b[1m\u001b[32m0.06267\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 734 | loss: 0.06267 - acc: 0.9969 -- iter: 15/55\n","Training Step: 8067  | total loss: \u001b[1m\u001b[32m0.05663\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 734 | loss: 0.05663 - acc: 0.9972 -- iter: 20/55\n","Training Step: 8068  | total loss: \u001b[1m\u001b[32m0.05144\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 734 | loss: 0.05144 - acc: 0.9975 -- iter: 25/55\n","Training Step: 8069  | total loss: \u001b[1m\u001b[32m0.04754\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 734 | loss: 0.04754 - acc: 0.9980 -- iter: 30/55\n","Training Step: 8070  | total loss: \u001b[1m\u001b[32m0.04295\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 734 | loss: 0.04295 - acc: 0.9982 -- iter: 35/55\n","Training Step: 8071  | total loss: \u001b[1m\u001b[32m0.03884\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 734 | loss: 0.03884 - acc: 0.9984 -- iter: 40/55\n","Training Step: 8072  | total loss: \u001b[1m\u001b[32m0.03542\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 734 | loss: 0.03542 - acc: 0.9985 -- iter: 45/55\n","Training Step: 8073  | total loss: \u001b[1m\u001b[32m0.03275\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 734 | loss: 0.03275 - acc: 0.9987 -- iter: 50/55\n","Training Step: 8074  | total loss: \u001b[1m\u001b[32m0.02994\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 734 | loss: 0.02994 - acc: 0.9988 -- iter: 55/55\n","--\n","Training Step: 8075  | total loss: \u001b[1m\u001b[32m0.02713\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 735 | loss: 0.02713 - acc: 0.9989 -- iter: 05/55\n","Training Step: 8076  | total loss: \u001b[1m\u001b[32m0.02460\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 735 | loss: 0.02460 - acc: 0.9990 -- iter: 10/55\n","Training Step: 8077  | total loss: \u001b[1m\u001b[32m0.02233\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 735 | loss: 0.02233 - acc: 0.9991 -- iter: 15/55\n","Training Step: 8078  | total loss: \u001b[1m\u001b[32m0.02041\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 735 | loss: 0.02041 - acc: 0.9992 -- iter: 20/55\n","Training Step: 8079  | total loss: \u001b[1m\u001b[32m0.01862\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 735 | loss: 0.01862 - acc: 0.9993 -- iter: 25/55\n","Training Step: 8080  | total loss: \u001b[1m\u001b[32m0.01713\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 735 | loss: 0.01713 - acc: 0.9994 -- iter: 30/55\n","Training Step: 8081  | total loss: \u001b[1m\u001b[32m0.01559\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 735 | loss: 0.01559 - acc: 0.9994 -- iter: 35/55\n","Training Step: 8082  | total loss: \u001b[1m\u001b[32m0.01449\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 735 | loss: 0.01449 - acc: 0.9995 -- iter: 40/55\n","Training Step: 8083  | total loss: \u001b[1m\u001b[32m0.01355\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 735 | loss: 0.01355 - acc: 0.9995 -- iter: 45/55\n","Training Step: 8084  | total loss: \u001b[1m\u001b[32m0.01259\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 735 | loss: 0.01259 - acc: 0.9996 -- iter: 50/55\n","Training Step: 8085  | total loss: \u001b[1m\u001b[32m0.01144\u001b[0m\u001b[0m | time: 0.062s\n","| Adam | epoch: 735 | loss: 0.01144 - acc: 0.9996 -- iter: 55/55\n","--\n","Training Step: 8086  | total loss: \u001b[1m\u001b[32m0.01044\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 736 | loss: 0.01044 - acc: 0.9997 -- iter: 05/55\n","Training Step: 8087  | total loss: \u001b[1m\u001b[32m0.00991\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 736 | loss: 0.00991 - acc: 0.9997 -- iter: 10/55\n","Training Step: 8088  | total loss: \u001b[1m\u001b[32m0.00917\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 736 | loss: 0.00917 - acc: 0.9997 -- iter: 15/55\n","Training Step: 8089  | total loss: \u001b[1m\u001b[32m0.00850\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 736 | loss: 0.00850 - acc: 0.9998 -- iter: 20/55\n","Training Step: 8090  | total loss: \u001b[1m\u001b[32m0.00797\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 736 | loss: 0.00797 - acc: 0.9998 -- iter: 25/55\n","Training Step: 8091  | total loss: \u001b[1m\u001b[32m0.00786\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 736 | loss: 0.00786 - acc: 0.9998 -- iter: 30/55\n","Training Step: 8092  | total loss: \u001b[1m\u001b[32m0.00766\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 736 | loss: 0.00766 - acc: 0.9998 -- iter: 35/55\n","Training Step: 8093  | total loss: \u001b[1m\u001b[32m0.00707\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 736 | loss: 0.00707 - acc: 0.9998 -- iter: 40/55\n","Training Step: 8094  | total loss: \u001b[1m\u001b[32m0.00692\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 736 | loss: 0.00692 - acc: 0.9998 -- iter: 45/55\n","Training Step: 8095  | total loss: \u001b[1m\u001b[32m0.00667\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 736 | loss: 0.00667 - acc: 0.9999 -- iter: 50/55\n","Training Step: 8096  | total loss: \u001b[1m\u001b[32m0.00615\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 736 | loss: 0.00615 - acc: 0.9999 -- iter: 55/55\n","--\n","Training Step: 8097  | total loss: \u001b[1m\u001b[32m0.00583\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 737 | loss: 0.00583 - acc: 0.9999 -- iter: 05/55\n","Training Step: 8098  | total loss: \u001b[1m\u001b[32m0.00561\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 737 | loss: 0.00561 - acc: 0.9999 -- iter: 10/55\n","Training Step: 8099  | total loss: \u001b[1m\u001b[32m0.00568\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 737 | loss: 0.00568 - acc: 0.9999 -- iter: 15/55\n","Training Step: 8100  | total loss: \u001b[1m\u001b[32m0.00539\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 737 | loss: 0.00539 - acc: 0.9999 -- iter: 20/55\n","Training Step: 8101  | total loss: \u001b[1m\u001b[32m0.00514\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 737 | loss: 0.00514 - acc: 0.9999 -- iter: 25/55\n","Training Step: 8102  | total loss: \u001b[1m\u001b[32m0.00509\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 737 | loss: 0.00509 - acc: 0.9999 -- iter: 30/55\n","Training Step: 8103  | total loss: \u001b[1m\u001b[32m0.00506\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 737 | loss: 0.00506 - acc: 0.9999 -- iter: 35/55\n","Training Step: 8104  | total loss: \u001b[1m\u001b[32m0.00469\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 737 | loss: 0.00469 - acc: 0.9999 -- iter: 40/55\n","Training Step: 8105  | total loss: \u001b[1m\u001b[32m0.00482\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 737 | loss: 0.00482 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8106  | total loss: \u001b[1m\u001b[32m0.00461\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 737 | loss: 0.00461 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8107  | total loss: \u001b[1m\u001b[32m0.00448\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 737 | loss: 0.00448 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8108  | total loss: \u001b[1m\u001b[32m0.00434\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 738 | loss: 0.00434 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8109  | total loss: \u001b[1m\u001b[32m0.00412\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 738 | loss: 0.00412 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8110  | total loss: \u001b[1m\u001b[32m0.00393\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 738 | loss: 0.00393 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8111  | total loss: \u001b[1m\u001b[32m0.00362\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 738 | loss: 0.00362 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8112  | total loss: \u001b[1m\u001b[32m0.00407\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 738 | loss: 0.00407 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8113  | total loss: \u001b[1m\u001b[32m0.00447\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 738 | loss: 0.00447 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8114  | total loss: \u001b[1m\u001b[32m0.00423\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 738 | loss: 0.00423 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8115  | total loss: \u001b[1m\u001b[32m0.00418\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 738 | loss: 0.00418 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8116  | total loss: \u001b[1m\u001b[32m0.00409\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 738 | loss: 0.00409 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8117  | total loss: \u001b[1m\u001b[32m0.00420\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 738 | loss: 0.00420 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8118  | total loss: \u001b[1m\u001b[32m0.00419\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 738 | loss: 0.00419 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8119  | total loss: \u001b[1m\u001b[32m0.00393\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 739 | loss: 0.00393 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8120  | total loss: \u001b[1m\u001b[32m0.00365\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 739 | loss: 0.00365 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8121  | total loss: \u001b[1m\u001b[32m0.00363\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 739 | loss: 0.00363 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8122  | total loss: \u001b[1m\u001b[32m0.00375\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 739 | loss: 0.00375 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8123  | total loss: \u001b[1m\u001b[32m0.00365\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 739 | loss: 0.00365 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8124  | total loss: \u001b[1m\u001b[32m0.00360\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 739 | loss: 0.00360 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8125  | total loss: \u001b[1m\u001b[32m0.00353\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 739 | loss: 0.00353 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8126  | total loss: \u001b[1m\u001b[32m0.00353\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 739 | loss: 0.00353 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8127  | total loss: \u001b[1m\u001b[32m0.00346\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 739 | loss: 0.00346 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8128  | total loss: \u001b[1m\u001b[32m0.00346\u001b[0m\u001b[0m | time: 0.063s\n","| Adam | epoch: 739 | loss: 0.00346 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8129  | total loss: \u001b[1m\u001b[32m0.00338\u001b[0m\u001b[0m | time: 0.074s\n","| Adam | epoch: 739 | loss: 0.00338 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8130  | total loss: \u001b[1m\u001b[32m0.00320\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 740 | loss: 0.00320 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8131  | total loss: \u001b[1m\u001b[32m0.00322\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 740 | loss: 0.00322 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8132  | total loss: \u001b[1m\u001b[32m0.00329\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 740 | loss: 0.00329 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8133  | total loss: \u001b[1m\u001b[32m0.00341\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 740 | loss: 0.00341 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8134  | total loss: \u001b[1m\u001b[32m0.00332\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 740 | loss: 0.00332 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8135  | total loss: \u001b[1m\u001b[32m0.00332\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 740 | loss: 0.00332 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8136  | total loss: \u001b[1m\u001b[32m0.00332\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 740 | loss: 0.00332 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8137  | total loss: \u001b[1m\u001b[32m0.00348\u001b[0m\u001b[0m | time: 0.066s\n","| Adam | epoch: 740 | loss: 0.00348 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8138  | total loss: \u001b[1m\u001b[32m0.00334\u001b[0m\u001b[0m | time: 0.070s\n","| Adam | epoch: 740 | loss: 0.00334 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8139  | total loss: \u001b[1m\u001b[32m0.00335\u001b[0m\u001b[0m | time: 0.075s\n","| Adam | epoch: 740 | loss: 0.00335 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8140  | total loss: \u001b[1m\u001b[32m0.00325\u001b[0m\u001b[0m | time: 0.081s\n","| Adam | epoch: 740 | loss: 0.00325 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8141  | total loss: \u001b[1m\u001b[32m0.00326\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 741 | loss: 0.00326 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8142  | total loss: \u001b[1m\u001b[32m0.00305\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 741 | loss: 0.00305 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8143  | total loss: \u001b[1m\u001b[32m0.00330\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 741 | loss: 0.00330 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8144  | total loss: \u001b[1m\u001b[32m0.00317\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 741 | loss: 0.00317 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8145  | total loss: \u001b[1m\u001b[32m0.00300\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 741 | loss: 0.00300 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8146  | total loss: \u001b[1m\u001b[32m0.00295\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 741 | loss: 0.00295 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8147  | total loss: \u001b[1m\u001b[32m0.00318\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 741 | loss: 0.00318 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8148  | total loss: \u001b[1m\u001b[32m0.00318\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 741 | loss: 0.00318 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8149  | total loss: \u001b[1m\u001b[32m0.00339\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 741 | loss: 0.00339 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8150  | total loss: \u001b[1m\u001b[32m0.00336\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 741 | loss: 0.00336 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8151  | total loss: \u001b[1m\u001b[32m0.00340\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 741 | loss: 0.00340 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8152  | total loss: \u001b[1m\u001b[32m1.35739\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 742 | loss: 1.35739 - acc: 0.9000 -- iter: 05/55\n","Training Step: 8153  | total loss: \u001b[1m\u001b[32m1.22205\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 742 | loss: 1.22205 - acc: 0.9100 -- iter: 10/55\n","Training Step: 8154  | total loss: \u001b[1m\u001b[32m0.99023\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 742 | loss: 0.99023 - acc: 0.9190 -- iter: 15/55\n","Training Step: 8155  | total loss: \u001b[1m\u001b[32m0.89144\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 742 | loss: 0.89144 - acc: 0.9271 -- iter: 20/55\n","Training Step: 8156  | total loss: \u001b[1m\u001b[32m0.80265\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 742 | loss: 0.80265 - acc: 0.9344 -- iter: 25/55\n","Training Step: 8157  | total loss: \u001b[1m\u001b[32m0.72278\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 742 | loss: 0.72278 - acc: 0.9410 -- iter: 30/55\n","Training Step: 8158  | total loss: \u001b[1m\u001b[32m0.65102\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 742 | loss: 0.65102 - acc: 0.9469 -- iter: 35/55\n","Training Step: 8159  | total loss: \u001b[1m\u001b[32m0.58631\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 742 | loss: 0.58631 - acc: 0.9522 -- iter: 40/55\n","Training Step: 8160  | total loss: \u001b[1m\u001b[32m0.52807\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 742 | loss: 0.52807 - acc: 0.9570 -- iter: 45/55\n","Training Step: 8161  | total loss: \u001b[1m\u001b[32m0.47544\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 742 | loss: 0.47544 - acc: 0.9613 -- iter: 50/55\n","Training Step: 8162  | total loss: \u001b[1m\u001b[32m0.42798\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 742 | loss: 0.42798 - acc: 0.9651 -- iter: 55/55\n","--\n","Training Step: 8163  | total loss: \u001b[1m\u001b[32m1.65821\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 743 | loss: 1.65821 - acc: 0.9686 -- iter: 05/55\n","Training Step: 8164  | total loss: \u001b[1m\u001b[32m1.49262\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 743 | loss: 1.49262 - acc: 0.8918 -- iter: 10/55\n","Training Step: 8165  | total loss: \u001b[1m\u001b[32m1.34368\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 743 | loss: 1.34368 - acc: 0.9026 -- iter: 15/55\n","Training Step: 8166  | total loss: \u001b[1m\u001b[32m1.20968\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 743 | loss: 1.20968 - acc: 0.9123 -- iter: 20/55\n","Training Step: 8167  | total loss: \u001b[1m\u001b[32m1.08926\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 743 | loss: 1.08926 - acc: 0.9211 -- iter: 25/55\n","Training Step: 8168  | total loss: \u001b[1m\u001b[32m0.98058\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 743 | loss: 0.98058 - acc: 0.9361 -- iter: 30/55\n","Training Step: 8169  | total loss: \u001b[1m\u001b[32m0.88264\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 743 | loss: 0.88264 - acc: 0.9425 -- iter: 35/55\n","Training Step: 8170  | total loss: \u001b[1m\u001b[32m0.79528\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 743 | loss: 0.79528 - acc: 0.9482 -- iter: 40/55\n","Training Step: 8171  | total loss: \u001b[1m\u001b[32m0.79528\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 743 | loss: 0.79528 - acc: 0.9534 -- iter: 45/55\n","Training Step: 8172  | total loss: \u001b[1m\u001b[32m0.71658\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 743 | loss: 0.71658 - acc: 0.9581 -- iter: 50/55\n","Training Step: 8173  | total loss: \u001b[1m\u001b[32m0.58157\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 743 | loss: 0.58157 - acc: 0.9623 -- iter: 55/55\n","--\n","Training Step: 8174  | total loss: \u001b[1m\u001b[32m0.52361\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 744 | loss: 0.52361 - acc: 0.9660 -- iter: 05/55\n","Training Step: 8175  | total loss: \u001b[1m\u001b[32m0.47178\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 744 | loss: 0.47178 - acc: 0.9694 -- iter: 10/55\n","Training Step: 8176  | total loss: \u001b[1m\u001b[32m0.42505\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 744 | loss: 0.42505 - acc: 0.9725 -- iter: 15/55\n","Training Step: 8177  | total loss: \u001b[1m\u001b[32m0.38328\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 744 | loss: 0.38328 - acc: 0.9752 -- iter: 20/55\n","Training Step: 8178  | total loss: \u001b[1m\u001b[32m0.34539\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 744 | loss: 0.34539 - acc: 0.9777 -- iter: 25/55\n","Training Step: 8179  | total loss: \u001b[1m\u001b[32m0.31174\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 744 | loss: 0.31174 - acc: 0.9799 -- iter: 30/55\n","Training Step: 8180  | total loss: \u001b[1m\u001b[32m0.31174\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 744 | loss: 0.31174 - acc: 0.9799 -- iter: 35/55\n","Training Step: 8181  | total loss: \u001b[1m\u001b[32m0.25293\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 744 | loss: 0.25293 - acc: 0.9838 -- iter: 40/55\n","Training Step: 8182  | total loss: \u001b[1m\u001b[32m0.22812\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 744 | loss: 0.22812 - acc: 0.9854 -- iter: 45/55\n","Training Step: 8183  | total loss: \u001b[1m\u001b[32m0.20549\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 744 | loss: 0.20549 - acc: 0.9868 -- iter: 50/55\n","Training Step: 8184  | total loss: \u001b[1m\u001b[32m0.18513\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 744 | loss: 0.18513 - acc: 0.9882 -- iter: 55/55\n","--\n","Training Step: 8185  | total loss: \u001b[1m\u001b[32m0.18513\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 745 | loss: 0.18513 - acc: 0.9893 -- iter: 05/55\n","Training Step: 8186  | total loss: \u001b[1m\u001b[32m0.16687\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 745 | loss: 0.16687 - acc: 0.9904 -- iter: 10/55\n","Training Step: 8187  | total loss: \u001b[1m\u001b[32m0.13615\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 745 | loss: 0.13615 - acc: 0.9914 -- iter: 15/55\n","Training Step: 8188  | total loss: \u001b[1m\u001b[32m0.12298\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 745 | loss: 0.12298 - acc: 0.9922 -- iter: 20/55\n","Training Step: 8189  | total loss: \u001b[1m\u001b[32m0.12298\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 745 | loss: 0.12298 - acc: 0.9930 -- iter: 25/55\n","Training Step: 8190  | total loss: \u001b[1m\u001b[32m0.11079\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 745 | loss: 0.11079 - acc: 0.9937 -- iter: 30/55\n","Training Step: 8191  | total loss: \u001b[1m\u001b[32m0.10026\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 745 | loss: 0.10026 - acc: 0.9943 -- iter: 35/55\n","Training Step: 8192  | total loss: \u001b[1m\u001b[32m0.09059\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 745 | loss: 0.09059 - acc: 0.9949 -- iter: 40/55\n","Training Step: 8193  | total loss: \u001b[1m\u001b[32m0.08207\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 745 | loss: 0.08207 - acc: 0.9954 -- iter: 45/55\n","Training Step: 8194  | total loss: \u001b[1m\u001b[32m0.07435\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 745 | loss: 0.07435 - acc: 0.9959 -- iter: 50/55\n","Training Step: 8195  | total loss: \u001b[1m\u001b[32m0.06733\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 745 | loss: 0.06733 - acc: 0.9963 -- iter: 55/55\n","--\n","Training Step: 8196  | total loss: \u001b[1m\u001b[32m0.06084\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 746 | loss: 0.06084 - acc: 0.9967 -- iter: 05/55\n","Training Step: 8197  | total loss: \u001b[1m\u001b[32m0.05498\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 746 | loss: 0.05498 - acc: 0.9970 -- iter: 10/55\n","Training Step: 8198  | total loss: \u001b[1m\u001b[32m0.04984\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 746 | loss: 0.04984 - acc: 0.9973 -- iter: 15/55\n","Training Step: 8199  | total loss: \u001b[1m\u001b[32m0.04506\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 746 | loss: 0.04506 - acc: 0.9976 -- iter: 20/55\n","Training Step: 8200  | total loss: \u001b[1m\u001b[32m0.04082\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 746 | loss: 0.04082 - acc: 0.9978 -- iter: 25/55\n","Training Step: 8201  | total loss: \u001b[1m\u001b[32m0.03695\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 746 | loss: 0.03695 - acc: 0.9980 -- iter: 30/55\n","Training Step: 8202  | total loss: \u001b[1m\u001b[32m0.03362\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 746 | loss: 0.03362 - acc: 0.9982 -- iter: 35/55\n","Training Step: 8203  | total loss: \u001b[1m\u001b[32m0.03073\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 746 | loss: 0.03073 - acc: 0.9984 -- iter: 40/55\n","Training Step: 8204  | total loss: \u001b[1m\u001b[32m0.02812\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 746 | loss: 0.02812 - acc: 0.9986 -- iter: 45/55\n","Training Step: 8205  | total loss: \u001b[1m\u001b[32m0.02574\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 746 | loss: 0.02574 - acc: 0.9987 -- iter: 50/55\n","Training Step: 8206  | total loss: \u001b[1m\u001b[32m0.02380\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 746 | loss: 0.02380 - acc: 0.9988 -- iter: 55/55\n","--\n","Training Step: 8207  | total loss: \u001b[1m\u001b[32m0.02162\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 747 | loss: 0.02162 - acc: 0.9990 -- iter: 05/55\n","Training Step: 8208  | total loss: \u001b[1m\u001b[32m0.01984\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 747 | loss: 0.01984 - acc: 0.9991 -- iter: 10/55\n","Training Step: 8209  | total loss: \u001b[1m\u001b[32m0.01823\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 747 | loss: 0.01823 - acc: 0.9991 -- iter: 15/55\n","Training Step: 8210  | total loss: \u001b[1m\u001b[32m0.01658\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 747 | loss: 0.01658 - acc: 0.9991 -- iter: 20/55\n","Training Step: 8211  | total loss: \u001b[1m\u001b[32m0.01505\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 747 | loss: 0.01505 - acc: 0.9992 -- iter: 25/55\n","Training Step: 8212  | total loss: \u001b[1m\u001b[32m0.01375\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 747 | loss: 0.01375 - acc: 0.9993 -- iter: 30/55\n","Training Step: 8213  | total loss: \u001b[1m\u001b[32m0.01259\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 747 | loss: 0.01259 - acc: 0.9994 -- iter: 35/55\n","Training Step: 8214  | total loss: \u001b[1m\u001b[32m0.01162\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 747 | loss: 0.01162 - acc: 0.9995 -- iter: 40/55\n","Training Step: 8215  | total loss: \u001b[1m\u001b[32m0.01081\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 747 | loss: 0.01081 - acc: 0.9995 -- iter: 45/55\n","Training Step: 8216  | total loss: \u001b[1m\u001b[32m0.01016\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 747 | loss: 0.01016 - acc: 0.9996 -- iter: 50/55\n","Training Step: 8217  | total loss: \u001b[1m\u001b[32m0.00959\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 747 | loss: 0.00959 - acc: 0.9996 -- iter: 55/55\n","--\n","Training Step: 8218  | total loss: \u001b[1m\u001b[32m0.00870\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 748 | loss: 0.00870 - acc: 0.9997 -- iter: 05/55\n","Training Step: 8219  | total loss: \u001b[1m\u001b[32m0.00825\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 748 | loss: 0.00825 - acc: 0.9997 -- iter: 10/55\n","Training Step: 8220  | total loss: \u001b[1m\u001b[32m0.00768\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 748 | loss: 0.00768 - acc: 0.9997 -- iter: 15/55\n","Training Step: 8221  | total loss: \u001b[1m\u001b[32m0.00717\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 748 | loss: 0.00717 - acc: 0.9998 -- iter: 20/55\n","Training Step: 8222  | total loss: \u001b[1m\u001b[32m0.00689\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 748 | loss: 0.00689 - acc: 0.9998 -- iter: 25/55\n","Training Step: 8223  | total loss: \u001b[1m\u001b[32m0.00640\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 748 | loss: 0.00640 - acc: 0.9998 -- iter: 30/55\n","Training Step: 8224  | total loss: \u001b[1m\u001b[32m0.00609\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 748 | loss: 0.00609 - acc: 0.9998 -- iter: 35/55\n","Training Step: 8225  | total loss: \u001b[1m\u001b[32m0.00598\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 748 | loss: 0.00598 - acc: 0.9998 -- iter: 40/55\n","Training Step: 8226  | total loss: \u001b[1m\u001b[32m0.00561\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 748 | loss: 0.00561 - acc: 0.9999 -- iter: 45/55\n","Training Step: 8227  | total loss: \u001b[1m\u001b[32m0.00520\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 748 | loss: 0.00520 - acc: 0.9999 -- iter: 50/55\n","Training Step: 8228  | total loss: \u001b[1m\u001b[32m0.00494\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 748 | loss: 0.00494 - acc: 0.9999 -- iter: 55/55\n","--\n","Training Step: 8229  | total loss: \u001b[1m\u001b[32m0.00482\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 749 | loss: 0.00482 - acc: 0.9999 -- iter: 05/55\n","Training Step: 8230  | total loss: \u001b[1m\u001b[32m0.00448\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 749 | loss: 0.00448 - acc: 0.9999 -- iter: 10/55\n","Training Step: 8231  | total loss: \u001b[1m\u001b[32m0.00425\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 749 | loss: 0.00425 - acc: 0.9999 -- iter: 15/55\n","Training Step: 8232  | total loss: \u001b[1m\u001b[32m0.00423\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 749 | loss: 0.00423 - acc: 0.9999 -- iter: 20/55\n","Training Step: 8233  | total loss: \u001b[1m\u001b[32m0.00421\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 749 | loss: 0.00421 - acc: 0.9999 -- iter: 25/55\n","Training Step: 8234  | total loss: \u001b[1m\u001b[32m0.00413\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 749 | loss: 0.00413 - acc: 0.9999 -- iter: 30/55\n","Training Step: 8235  | total loss: \u001b[1m\u001b[32m0.00414\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 749 | loss: 0.00414 - acc: 0.9999 -- iter: 35/55\n","Training Step: 8236  | total loss: \u001b[1m\u001b[32m0.00399\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 749 | loss: 0.00399 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8237  | total loss: \u001b[1m\u001b[32m0.00369\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 749 | loss: 0.00369 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8238  | total loss: \u001b[1m\u001b[32m0.00358\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 749 | loss: 0.00358 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8239  | total loss: \u001b[1m\u001b[32m0.00356\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 749 | loss: 0.00356 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8240  | total loss: \u001b[1m\u001b[32m0.00362\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 750 | loss: 0.00362 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8241  | total loss: \u001b[1m\u001b[32m0.00372\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 750 | loss: 0.00372 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8242  | total loss: \u001b[1m\u001b[32m0.00343\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 750 | loss: 0.00343 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8243  | total loss: \u001b[1m\u001b[32m0.00339\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 750 | loss: 0.00339 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8244  | total loss: \u001b[1m\u001b[32m0.00341\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 750 | loss: 0.00341 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8245  | total loss: \u001b[1m\u001b[32m0.00343\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 750 | loss: 0.00343 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8246  | total loss: \u001b[1m\u001b[32m0.00326\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 750 | loss: 0.00326 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8247  | total loss: \u001b[1m\u001b[32m0.00327\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 750 | loss: 0.00327 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8248  | total loss: \u001b[1m\u001b[32m0.00333\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 750 | loss: 0.00333 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8249  | total loss: \u001b[1m\u001b[32m0.00337\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 750 | loss: 0.00337 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8250  | total loss: \u001b[1m\u001b[32m0.00330\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 750 | loss: 0.00330 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8251  | total loss: \u001b[1m\u001b[32m0.00322\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 751 | loss: 0.00322 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8252  | total loss: \u001b[1m\u001b[32m0.00324\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 751 | loss: 0.00324 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8253  | total loss: \u001b[1m\u001b[32m0.00304\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 751 | loss: 0.00304 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8254  | total loss: \u001b[1m\u001b[32m0.00285\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 751 | loss: 0.00285 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8255  | total loss: \u001b[1m\u001b[32m0.00310\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 751 | loss: 0.00310 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8256  | total loss: \u001b[1m\u001b[32m0.00298\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 751 | loss: 0.00298 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8257  | total loss: \u001b[1m\u001b[32m0.00286\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 751 | loss: 0.00286 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8258  | total loss: \u001b[1m\u001b[32m0.00285\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 751 | loss: 0.00285 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8259  | total loss: \u001b[1m\u001b[32m0.00289\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 751 | loss: 0.00289 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8260  | total loss: \u001b[1m\u001b[32m0.00294\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 751 | loss: 0.00294 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8261  | total loss: \u001b[1m\u001b[32m0.00272\u001b[0m\u001b[0m | time: 0.060s\n","| Adam | epoch: 751 | loss: 0.00272 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8262  | total loss: \u001b[1m\u001b[32m0.00252\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 752 | loss: 0.00252 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8263  | total loss: \u001b[1m\u001b[32m0.00261\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 752 | loss: 0.00261 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8264  | total loss: \u001b[1m\u001b[32m0.00248\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 752 | loss: 0.00248 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8265  | total loss: \u001b[1m\u001b[32m0.00228\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 752 | loss: 0.00228 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8266  | total loss: \u001b[1m\u001b[32m0.00246\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 752 | loss: 0.00246 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8267  | total loss: \u001b[1m\u001b[32m0.00257\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 752 | loss: 0.00257 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8268  | total loss: \u001b[1m\u001b[32m0.00265\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 752 | loss: 0.00265 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8269  | total loss: \u001b[1m\u001b[32m0.00273\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 752 | loss: 0.00273 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8270  | total loss: \u001b[1m\u001b[32m0.00290\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 752 | loss: 0.00290 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8271  | total loss: \u001b[1m\u001b[32m0.00293\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 752 | loss: 0.00293 - acc: 0.9000 -- iter: 50/55\n","Training Step: 8272  | total loss: \u001b[1m\u001b[32m1.25218\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 752 | loss: 1.25218 - acc: 0.9100 -- iter: 55/55\n","--\n","Training Step: 8273  | total loss: \u001b[1m\u001b[32m1.12714\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 753 | loss: 1.12714 - acc: 0.9190 -- iter: 05/55\n","Training Step: 8274  | total loss: \u001b[1m\u001b[32m1.01471\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 753 | loss: 1.01471 - acc: 0.9271 -- iter: 10/55\n","Training Step: 8275  | total loss: \u001b[1m\u001b[32m0.91357\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 753 | loss: 0.91357 - acc: 0.9344 -- iter: 15/55\n","Training Step: 8276  | total loss: \u001b[1m\u001b[32m0.82250\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 753 | loss: 0.82250 - acc: 0.9410 -- iter: 20/55\n","Training Step: 8277  | total loss: \u001b[1m\u001b[32m0.74071\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 753 | loss: 0.74071 - acc: 0.9469 -- iter: 25/55\n","Training Step: 8278  | total loss: \u001b[1m\u001b[32m0.66695\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 753 | loss: 0.66695 - acc: 0.9469 -- iter: 30/55\n","Training Step: 8279  | total loss: \u001b[1m\u001b[32m0.60051\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 753 | loss: 0.60051 - acc: 0.9570 -- iter: 35/55\n","Training Step: 8280  | total loss: \u001b[1m\u001b[32m0.60051\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 753 | loss: 0.60051 - acc: 0.9570 -- iter: 40/55\n","Training Step: 8281  | total loss: \u001b[1m\u001b[32m0.48688\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 753 | loss: 0.48688 - acc: 0.9613 -- iter: 45/55\n","Training Step: 8282  | total loss: \u001b[1m\u001b[32m0.43854\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 753 | loss: 0.43854 - acc: 0.9651 -- iter: 50/55\n","Training Step: 8283  | total loss: \u001b[1m\u001b[32m1.26821\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 753 | loss: 1.26821 - acc: 0.9686 -- iter: 55/55\n","--\n","Training Step: 8284  | total loss: \u001b[1m\u001b[32m1.14170\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 754 | loss: 1.14170 - acc: 0.8918 -- iter: 05/55\n","Training Step: 8285  | total loss: \u001b[1m\u001b[32m1.14170\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 754 | loss: 1.14170 - acc: 0.9026 -- iter: 10/55\n","Training Step: 8286  | total loss: \u001b[1m\u001b[32m1.02772\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 754 | loss: 1.02772 - acc: 0.9123 -- iter: 15/55\n","Training Step: 8287  | total loss: \u001b[1m\u001b[32m0.92568\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 754 | loss: 0.92568 - acc: 0.9211 -- iter: 20/55\n","Training Step: 8288  | total loss: \u001b[1m\u001b[32m0.75042\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 754 | loss: 0.75042 - acc: 0.9290 -- iter: 25/55\n","Training Step: 8289  | total loss: \u001b[1m\u001b[32m0.67588\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 754 | loss: 0.67588 - acc: 0.9361 -- iter: 30/55\n","Training Step: 8290  | total loss: \u001b[1m\u001b[32m0.60874\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 754 | loss: 0.60874 - acc: 0.9425 -- iter: 35/55\n","Training Step: 8291  | total loss: \u001b[1m\u001b[32m0.54817\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 754 | loss: 0.54817 - acc: 0.9534 -- iter: 40/55\n","Training Step: 8292  | total loss: \u001b[1m\u001b[32m0.49366\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 754 | loss: 0.49366 - acc: 0.9581 -- iter: 45/55\n","Training Step: 8293  | total loss: \u001b[1m\u001b[32m0.44493\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 754 | loss: 0.44493 - acc: 0.9623 -- iter: 50/55\n","Training Step: 8294  | total loss: \u001b[1m\u001b[32m0.44493\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 754 | loss: 0.44493 - acc: 0.9660 -- iter: 55/55\n","--\n","Training Step: 8295  | total loss: \u001b[1m\u001b[32m0.40077\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 755 | loss: 0.40077 - acc: 0.8894 -- iter: 05/55\n","Training Step: 8296  | total loss: \u001b[1m\u001b[32m1.40416\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 755 | loss: 1.40416 - acc: 0.9005 -- iter: 10/55\n","Training Step: 8297  | total loss: \u001b[1m\u001b[32m1.26413\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 755 | loss: 1.26413 - acc: 0.9104 -- iter: 15/55\n","Training Step: 8298  | total loss: \u001b[1m\u001b[32m1.13792\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 755 | loss: 1.13792 - acc: 0.9104 -- iter: 20/55\n","Training Step: 8299  | total loss: \u001b[1m\u001b[32m1.02439\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 755 | loss: 1.02439 - acc: 0.9194 -- iter: 25/55\n","Training Step: 8300  | total loss: \u001b[1m\u001b[32m0.83113\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 755 | loss: 0.83113 - acc: 0.9275 -- iter: 30/55\n","Training Step: 8301  | total loss: \u001b[1m\u001b[32m0.74849\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 755 | loss: 0.74849 - acc: 0.9347 -- iter: 35/55\n","Training Step: 8302  | total loss: \u001b[1m\u001b[32m0.74849\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 755 | loss: 0.74849 - acc: 0.9412 -- iter: 40/55\n","Training Step: 8303  | total loss: \u001b[1m\u001b[32m0.67435\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 755 | loss: 0.67435 - acc: 0.9471 -- iter: 45/55\n","Training Step: 8304  | total loss: \u001b[1m\u001b[32m0.60713\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 755 | loss: 0.60713 - acc: 0.9524 -- iter: 50/55\n","Training Step: 8305  | total loss: \u001b[1m\u001b[32m0.54663\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 755 | loss: 0.54663 - acc: 0.9572 -- iter: 55/55\n","--\n","Training Step: 8306  | total loss: \u001b[1m\u001b[32m0.49242\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 756 | loss: 0.49242 - acc: 0.9614 -- iter: 05/55\n","Training Step: 8307  | total loss: \u001b[1m\u001b[32m0.44352\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 756 | loss: 0.44352 - acc: 0.9653 -- iter: 10/55\n","Training Step: 8308  | total loss: \u001b[1m\u001b[32m1.87460\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 756 | loss: 1.87460 - acc: 0.8999 -- iter: 15/55\n","Training Step: 8309  | total loss: \u001b[1m\u001b[32m1.68750\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 756 | loss: 1.68750 - acc: 0.9099 -- iter: 20/55\n","Training Step: 8310  | total loss: \u001b[1m\u001b[32m1.51918\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 756 | loss: 1.51918 - acc: 0.9099 -- iter: 25/55\n","Training Step: 8311  | total loss: \u001b[1m\u001b[32m1.36757\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 756 | loss: 1.36757 - acc: 0.9189 -- iter: 30/55\n","Training Step: 8312  | total loss: \u001b[1m\u001b[32m1.23104\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 756 | loss: 1.23104 - acc: 0.9270 -- iter: 35/55\n","Training Step: 8313  | total loss: \u001b[1m\u001b[32m1.10918\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 756 | loss: 1.10918 - acc: 0.9343 -- iter: 40/55\n","Training Step: 8314  | total loss: \u001b[1m\u001b[32m0.99892\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 756 | loss: 0.99892 - acc: 0.9409 -- iter: 45/55\n","Training Step: 8315  | total loss: \u001b[1m\u001b[32m0.89933\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 756 | loss: 0.89933 - acc: 0.9468 -- iter: 50/55\n","Training Step: 8316  | total loss: \u001b[1m\u001b[32m0.80971\u001b[0m\u001b[0m | time: 0.064s\n","| Adam | epoch: 756 | loss: 0.80971 - acc: 0.9521 -- iter: 55/55\n","--\n","Training Step: 8317  | total loss: \u001b[1m\u001b[32m0.72893\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 757 | loss: 0.72893 - acc: 0.9569 -- iter: 05/55\n","Training Step: 8318  | total loss: \u001b[1m\u001b[32m0.65619\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 757 | loss: 0.65619 - acc: 0.9651 -- iter: 10/55\n","Training Step: 8319  | total loss: \u001b[1m\u001b[32m0.59175\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 757 | loss: 0.59175 - acc: 0.9686 -- iter: 15/55\n","Training Step: 8320  | total loss: \u001b[1m\u001b[32m0.53324\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 757 | loss: 0.53324 - acc: 0.9717 -- iter: 20/55\n","Training Step: 8321  | total loss: \u001b[1m\u001b[32m0.48011\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 757 | loss: 0.48011 - acc: 0.9746 -- iter: 25/55\n","Training Step: 8322  | total loss: \u001b[1m\u001b[32m0.43247\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 757 | loss: 0.43247 - acc: 0.9771 -- iter: 30/55\n","Training Step: 8323  | total loss: \u001b[1m\u001b[32m0.38937\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 757 | loss: 0.38937 - acc: 0.9794 -- iter: 35/55\n","Training Step: 8324  | total loss: \u001b[1m\u001b[32m0.35144\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 757 | loss: 0.35144 - acc: 0.9815 -- iter: 40/55\n","Training Step: 8325  | total loss: \u001b[1m\u001b[32m0.31660\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 757 | loss: 0.31660 - acc: 0.9833 -- iter: 45/55\n","Training Step: 8326  | total loss: \u001b[1m\u001b[32m0.28518\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 757 | loss: 0.28518 - acc: 0.9850 -- iter: 50/55\n","Training Step: 8327  | total loss: \u001b[1m\u001b[32m0.25701\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 757 | loss: 0.25701 - acc: 0.9865 -- iter: 55/55\n","--\n","Training Step: 8328  | total loss: \u001b[1m\u001b[32m0.23166\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 758 | loss: 0.23166 - acc: 0.9878 -- iter: 05/55\n","Training Step: 8329  | total loss: \u001b[1m\u001b[32m0.20916\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 758 | loss: 0.20916 - acc: 0.9890 -- iter: 10/55\n","Training Step: 8330  | total loss: \u001b[1m\u001b[32m0.18927\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 758 | loss: 0.18927 - acc: 0.9890 -- iter: 15/55\n","Training Step: 8331  | total loss: \u001b[1m\u001b[32m0.17075\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 758 | loss: 0.17075 - acc: 0.9911 -- iter: 20/55\n","Training Step: 8332  | total loss: \u001b[1m\u001b[32m0.15431\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 758 | loss: 0.15431 - acc: 0.9920 -- iter: 25/55\n","Training Step: 8333  | total loss: \u001b[1m\u001b[32m0.13935\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 758 | loss: 0.13935 - acc: 0.9928 -- iter: 30/55\n","Training Step: 8334  | total loss: \u001b[1m\u001b[32m0.12577\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 758 | loss: 0.12577 - acc: 0.9935 -- iter: 35/55\n","Training Step: 8335  | total loss: \u001b[1m\u001b[32m0.11345\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 758 | loss: 0.11345 - acc: 0.9942 -- iter: 40/55\n","Training Step: 8336  | total loss: \u001b[1m\u001b[32m0.10308\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 758 | loss: 0.10308 - acc: 0.9948 -- iter: 45/55\n","Training Step: 8337  | total loss: \u001b[1m\u001b[32m0.09325\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 758 | loss: 0.09325 - acc: 0.9953 -- iter: 50/55\n","Training Step: 8338  | total loss: \u001b[1m\u001b[32m0.08421\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 758 | loss: 0.08421 - acc: 0.9958 -- iter: 55/55\n","--\n","Training Step: 8339  | total loss: \u001b[1m\u001b[32m0.07608\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 759 | loss: 0.07608 - acc: 0.9962 -- iter: 05/55\n","Training Step: 8340  | total loss: \u001b[1m\u001b[32m0.06877\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 759 | loss: 0.06877 - acc: 0.9966 -- iter: 10/55\n","Training Step: 8341  | total loss: \u001b[1m\u001b[32m0.06217\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 759 | loss: 0.06217 - acc: 0.9969 -- iter: 15/55\n","Training Step: 8342  | total loss: \u001b[1m\u001b[32m0.05659\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 759 | loss: 0.05659 - acc: 0.9972 -- iter: 20/55\n","Training Step: 8343  | total loss: \u001b[1m\u001b[32m0.05129\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 759 | loss: 0.05129 - acc: 0.9975 -- iter: 25/55\n","Training Step: 8344  | total loss: \u001b[1m\u001b[32m0.04653\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 759 | loss: 0.04653 - acc: 0.9977 -- iter: 30/55\n","Training Step: 8345  | total loss: \u001b[1m\u001b[32m0.04214\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 759 | loss: 0.04214 - acc: 0.9980 -- iter: 35/55\n","Training Step: 8346  | total loss: \u001b[1m\u001b[32m0.04214\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 759 | loss: 0.04214 - acc: 0.9982 -- iter: 40/55\n","Training Step: 8347  | total loss: \u001b[1m\u001b[32m0.03877\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 759 | loss: 0.03877 - acc: 0.9984 -- iter: 45/55\n","Training Step: 8348  | total loss: \u001b[1m\u001b[32m0.03528\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 759 | loss: 0.03528 - acc: 0.9985 -- iter: 50/55\n","Training Step: 8349  | total loss: \u001b[1m\u001b[32m0.03233\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 759 | loss: 0.03233 - acc: 0.9987 -- iter: 55/55\n","--\n","Training Step: 8350  | total loss: \u001b[1m\u001b[32m0.02923\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 760 | loss: 0.02923 - acc: 0.9988 -- iter: 05/55\n","Training Step: 8351  | total loss: \u001b[1m\u001b[32m0.02718\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 760 | loss: 0.02718 - acc: 0.9989 -- iter: 10/55\n","Training Step: 8352  | total loss: \u001b[1m\u001b[32m0.02483\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 760 | loss: 0.02483 - acc: 0.9990 -- iter: 15/55\n","Training Step: 8353  | total loss: \u001b[1m\u001b[32m0.02272\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 760 | loss: 0.02272 - acc: 0.9991 -- iter: 20/55\n","Training Step: 8354  | total loss: \u001b[1m\u001b[32m0.02085\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 760 | loss: 0.02085 - acc: 0.9992 -- iter: 25/55\n","Training Step: 8355  | total loss: \u001b[1m\u001b[32m0.01892\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 760 | loss: 0.01892 - acc: 0.9993 -- iter: 30/55\n","Training Step: 8356  | total loss: \u001b[1m\u001b[32m0.01733\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 760 | loss: 0.01733 - acc: 0.9994 -- iter: 35/55\n","Training Step: 8357  | total loss: \u001b[1m\u001b[32m0.01602\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 760 | loss: 0.01602 - acc: 0.9994 -- iter: 40/55\n","Training Step: 8358  | total loss: \u001b[1m\u001b[32m0.01484\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 760 | loss: 0.01484 - acc: 0.9995 -- iter: 45/55\n","Training Step: 8359  | total loss: \u001b[1m\u001b[32m0.01353\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 760 | loss: 0.01353 - acc: 0.9995 -- iter: 50/55\n","Training Step: 8360  | total loss: \u001b[1m\u001b[32m0.01266\u001b[0m\u001b[0m | time: 0.064s\n","| Adam | epoch: 760 | loss: 0.01266 - acc: 0.9996 -- iter: 55/55\n","--\n","Training Step: 8361  | total loss: \u001b[1m\u001b[32m0.01177\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 761 | loss: 0.01177 - acc: 0.9996 -- iter: 05/55\n","Training Step: 8362  | total loss: \u001b[1m\u001b[32m0.01088\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 761 | loss: 0.01088 - acc: 0.9996 -- iter: 10/55\n","Training Step: 8363  | total loss: \u001b[1m\u001b[32m0.01014\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 761 | loss: 0.01014 - acc: 0.9997 -- iter: 15/55\n","Training Step: 8364  | total loss: \u001b[1m\u001b[32m0.00968\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 761 | loss: 0.00968 - acc: 0.9997 -- iter: 20/55\n","Training Step: 8365  | total loss: \u001b[1m\u001b[32m0.00925\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 761 | loss: 0.00925 - acc: 0.9997 -- iter: 25/55\n","Training Step: 8366  | total loss: \u001b[1m\u001b[32m0.00899\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 761 | loss: 0.00899 - acc: 0.9998 -- iter: 30/55\n","Training Step: 8367  | total loss: \u001b[1m\u001b[32m0.00798\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 761 | loss: 0.00798 - acc: 0.9998 -- iter: 35/55\n","Training Step: 8368  | total loss: \u001b[1m\u001b[32m0.00798\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 761 | loss: 0.00798 - acc: 0.9998 -- iter: 40/55\n","Training Step: 8369  | total loss: \u001b[1m\u001b[32m0.00762\u001b[0m\u001b[0m | time: 0.066s\n","| Adam | epoch: 761 | loss: 0.00762 - acc: 0.9998 -- iter: 45/55\n","Training Step: 8370  | total loss: \u001b[1m\u001b[32m0.00701\u001b[0m\u001b[0m | time: 0.073s\n","| Adam | epoch: 761 | loss: 0.00701 - acc: 0.9998 -- iter: 50/55\n","Training Step: 8371  | total loss: \u001b[1m\u001b[32m0.00642\u001b[0m\u001b[0m | time: 0.082s\n","| Adam | epoch: 761 | loss: 0.00642 - acc: 0.9999 -- iter: 55/55\n","--\n","Training Step: 8372  | total loss: \u001b[1m\u001b[32m0.00643\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 762 | loss: 0.00643 - acc: 0.9999 -- iter: 05/55\n","Training Step: 8373  | total loss: \u001b[1m\u001b[32m0.00610\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 762 | loss: 0.00610 - acc: 0.9999 -- iter: 10/55\n","Training Step: 8374  | total loss: \u001b[1m\u001b[32m0.00595\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 762 | loss: 0.00595 - acc: 0.9999 -- iter: 15/55\n","Training Step: 8375  | total loss: \u001b[1m\u001b[32m0.00590\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 762 | loss: 0.00590 - acc: 0.9999 -- iter: 20/55\n","Training Step: 8376  | total loss: \u001b[1m\u001b[32m0.00561\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 762 | loss: 0.00561 - acc: 0.9999 -- iter: 25/55\n","Training Step: 8377  | total loss: \u001b[1m\u001b[32m0.00534\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 762 | loss: 0.00534 - acc: 0.9999 -- iter: 30/55\n","Training Step: 8378  | total loss: \u001b[1m\u001b[32m0.00547\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 762 | loss: 0.00547 - acc: 0.9999 -- iter: 35/55\n","Training Step: 8379  | total loss: \u001b[1m\u001b[32m0.00528\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 762 | loss: 0.00528 - acc: 0.9999 -- iter: 40/55\n","Training Step: 8380  | total loss: \u001b[1m\u001b[32m0.00483\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 762 | loss: 0.00483 - acc: 0.9999 -- iter: 45/55\n","Training Step: 8381  | total loss: \u001b[1m\u001b[32m0.00474\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 762 | loss: 0.00474 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8382  | total loss: \u001b[1m\u001b[32m0.00480\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 762 | loss: 0.00480 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8383  | total loss: \u001b[1m\u001b[32m0.00456\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 763 | loss: 0.00456 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8384  | total loss: \u001b[1m\u001b[32m0.00443\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 763 | loss: 0.00443 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8385  | total loss: \u001b[1m\u001b[32m0.00415\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 763 | loss: 0.00415 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8386  | total loss: \u001b[1m\u001b[32m0.00428\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 763 | loss: 0.00428 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8387  | total loss: \u001b[1m\u001b[32m0.00436\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 763 | loss: 0.00436 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8388  | total loss: \u001b[1m\u001b[32m0.00443\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 763 | loss: 0.00443 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8389  | total loss: \u001b[1m\u001b[32m0.00457\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 763 | loss: 0.00457 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8390  | total loss: \u001b[1m\u001b[32m0.00433\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 763 | loss: 0.00433 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8391  | total loss: \u001b[1m\u001b[32m0.00433\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 763 | loss: 0.00433 - acc: 0.9000 -- iter: 45/55\n","Training Step: 8392  | total loss: \u001b[1m\u001b[32m1.11568\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 763 | loss: 1.11568 - acc: 0.9100 -- iter: 50/55\n","Training Step: 8393  | total loss: \u001b[1m\u001b[32m1.00424\u001b[0m\u001b[0m | time: 0.062s\n","| Adam | epoch: 763 | loss: 1.00424 - acc: 0.9100 -- iter: 55/55\n","--\n","Training Step: 8394  | total loss: \u001b[1m\u001b[32m0.90416\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 764 | loss: 0.90416 - acc: 0.9190 -- iter: 05/55\n","Training Step: 8395  | total loss: \u001b[1m\u001b[32m0.90416\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 764 | loss: 0.90416 - acc: 0.9271 -- iter: 10/55\n","Training Step: 8396  | total loss: \u001b[1m\u001b[32m0.81444\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 764 | loss: 0.81444 - acc: 0.9344 -- iter: 15/55\n","Training Step: 8397  | total loss: \u001b[1m\u001b[32m0.73336\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 764 | loss: 0.73336 - acc: 0.9409 -- iter: 20/55\n","Training Step: 8398  | total loss: \u001b[1m\u001b[32m0.66032\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 764 | loss: 0.66032 - acc: 0.9468 -- iter: 25/55\n","Training Step: 8399  | total loss: \u001b[1m\u001b[32m0.59461\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 764 | loss: 0.59461 - acc: 0.9522 -- iter: 30/55\n","Training Step: 8400  | total loss: \u001b[1m\u001b[32m0.48363\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 764 | loss: 0.48363 - acc: 0.9569 -- iter: 35/55\n","Training Step: 8401  | total loss: \u001b[1m\u001b[32m0.43552\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 764 | loss: 0.43552 - acc: 0.9613 -- iter: 40/55\n","Training Step: 8402  | total loss: \u001b[1m\u001b[32m0.39217\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 764 | loss: 0.39217 - acc: 0.9651 -- iter: 45/55\n","Training Step: 8403  | total loss: \u001b[1m\u001b[32m1.24792\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 764 | loss: 1.24792 - acc: 0.9686 -- iter: 50/55\n","Training Step: 8404  | total loss: \u001b[1m\u001b[32m1.12339\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 764 | loss: 1.12339 - acc: 0.8718 -- iter: 55/55\n","--\n","Training Step: 8405  | total loss: \u001b[1m\u001b[32m1.01124\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 765 | loss: 1.01124 - acc: 0.8846 -- iter: 05/55\n","Training Step: 8406  | total loss: \u001b[1m\u001b[32m0.91043\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 765 | loss: 0.91043 - acc: 0.9065 -- iter: 10/55\n","Training Step: 8407  | total loss: \u001b[1m\u001b[32m0.81977\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 765 | loss: 0.81977 - acc: 0.9159 -- iter: 15/55\n","Training Step: 8408  | total loss: \u001b[1m\u001b[32m0.73850\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 765 | loss: 0.73850 - acc: 0.9243 -- iter: 20/55\n","Training Step: 8409  | total loss: \u001b[1m\u001b[32m0.66509\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 765 | loss: 0.66509 - acc: 0.9318 -- iter: 25/55\n","Training Step: 8410  | total loss: \u001b[1m\u001b[32m0.59923\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 765 | loss: 0.59923 - acc: 0.9387 -- iter: 30/55\n","Training Step: 8411  | total loss: \u001b[1m\u001b[32m0.53954\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 765 | loss: 0.53954 - acc: 0.9448 -- iter: 35/55\n","Training Step: 8412  | total loss: \u001b[1m\u001b[32m0.48582\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 765 | loss: 0.48582 - acc: 0.9503 -- iter: 40/55\n","Training Step: 8413  | total loss: \u001b[1m\u001b[32m0.43764\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 765 | loss: 0.43764 - acc: 0.9553 -- iter: 45/55\n","Training Step: 8414  | total loss: \u001b[1m\u001b[32m0.39465\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 765 | loss: 0.39465 - acc: 0.9598 -- iter: 50/55\n","Training Step: 8415  | total loss: \u001b[1m\u001b[32m0.39465\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 765 | loss: 0.39465 - acc: 0.9638 -- iter: 55/55\n","--\n","Training Step: 8416  | total loss: \u001b[1m\u001b[32m0.35593\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 766 | loss: 0.35593 - acc: 0.9674 -- iter: 05/55\n","Training Step: 8417  | total loss: \u001b[1m\u001b[32m0.28898\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 766 | loss: 0.28898 - acc: 0.9707 -- iter: 10/55\n","Training Step: 8418  | total loss: \u001b[1m\u001b[32m0.26077\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 766 | loss: 0.26077 - acc: 0.9736 -- iter: 15/55\n","Training Step: 8419  | total loss: \u001b[1m\u001b[32m0.23512\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 766 | loss: 0.23512 - acc: 0.9762 -- iter: 20/55\n","Training Step: 8420  | total loss: \u001b[1m\u001b[32m0.21199\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 766 | loss: 0.21199 - acc: 0.9786 -- iter: 25/55\n","Training Step: 8421  | total loss: \u001b[1m\u001b[32m0.19115\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 766 | loss: 0.19115 - acc: 0.9808 -- iter: 30/55\n","Training Step: 8422  | total loss: \u001b[1m\u001b[32m0.17237\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 766 | loss: 0.17237 - acc: 0.9827 -- iter: 35/55\n","Training Step: 8423  | total loss: \u001b[1m\u001b[32m0.15574\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 766 | loss: 0.15574 - acc: 0.9844 -- iter: 40/55\n","Training Step: 8424  | total loss: \u001b[1m\u001b[32m0.14078\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 766 | loss: 0.14078 - acc: 0.9860 -- iter: 45/55\n","Training Step: 8425  | total loss: \u001b[1m\u001b[32m0.12731\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 766 | loss: 0.12731 - acc: 0.9874 -- iter: 50/55\n","Training Step: 8426  | total loss: \u001b[1m\u001b[32m0.11588\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 766 | loss: 0.11588 - acc: 0.9886 -- iter: 55/55\n","--\n","Training Step: 8427  | total loss: \u001b[1m\u001b[32m1.39987\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 767 | loss: 1.39987 - acc: 0.9098 -- iter: 05/55\n","Training Step: 8428  | total loss: \u001b[1m\u001b[32m1.26041\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 767 | loss: 1.26041 - acc: 0.9188 -- iter: 10/55\n","Training Step: 8429  | total loss: \u001b[1m\u001b[32m1.13488\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 767 | loss: 1.13488 - acc: 0.9269 -- iter: 15/55\n","Training Step: 8430  | total loss: \u001b[1m\u001b[32m1.02186\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 767 | loss: 1.02186 - acc: 0.9342 -- iter: 20/55\n","Training Step: 8431  | total loss: \u001b[1m\u001b[32m0.92012\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 767 | loss: 0.92012 - acc: 0.9408 -- iter: 25/55\n","Training Step: 8432  | total loss: \u001b[1m\u001b[32m0.82833\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 767 | loss: 0.82833 - acc: 0.9467 -- iter: 30/55\n","Training Step: 8433  | total loss: \u001b[1m\u001b[32m0.74738\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 767 | loss: 0.74738 - acc: 0.9520 -- iter: 35/55\n","Training Step: 8434  | total loss: \u001b[1m\u001b[32m0.67329\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 767 | loss: 0.67329 - acc: 0.9568 -- iter: 40/55\n","Training Step: 8435  | total loss: \u001b[1m\u001b[32m0.60626\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 767 | loss: 0.60626 - acc: 0.9612 -- iter: 45/55\n","Training Step: 8436  | total loss: \u001b[1m\u001b[32m0.54594\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 767 | loss: 0.54594 - acc: 0.9650 -- iter: 50/55\n","Training Step: 8437  | total loss: \u001b[1m\u001b[32m0.49167\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 767 | loss: 0.49167 - acc: 0.9685 -- iter: 55/55\n","--\n","Training Step: 8438  | total loss: \u001b[1m\u001b[32m0.44285\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 768 | loss: 0.44285 - acc: 0.9717 -- iter: 05/55\n","Training Step: 8439  | total loss: \u001b[1m\u001b[32m0.39889\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 768 | loss: 0.39889 - acc: 0.9745 -- iter: 10/55\n","Training Step: 8440  | total loss: \u001b[1m\u001b[32m0.35952\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 768 | loss: 0.35952 - acc: 0.9771 -- iter: 15/55\n","Training Step: 8441  | total loss: \u001b[1m\u001b[32m0.32404\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 768 | loss: 0.32404 - acc: 0.9794 -- iter: 20/55\n","Training Step: 8442  | total loss: \u001b[1m\u001b[32m0.29198\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 768 | loss: 0.29198 - acc: 0.9814 -- iter: 25/55\n","Training Step: 8443  | total loss: \u001b[1m\u001b[32m0.26332\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 768 | loss: 0.26332 - acc: 0.9833 -- iter: 30/55\n","Training Step: 8444  | total loss: \u001b[1m\u001b[32m0.23726\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 768 | loss: 0.23726 - acc: 0.9850 -- iter: 35/55\n","Training Step: 8445  | total loss: \u001b[1m\u001b[32m0.21502\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 768 | loss: 0.21502 - acc: 0.9865 -- iter: 40/55\n","Training Step: 8446  | total loss: \u001b[1m\u001b[32m0.19442\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 768 | loss: 0.19442 - acc: 0.9878 -- iter: 45/55\n","Training Step: 8447  | total loss: \u001b[1m\u001b[32m0.17522\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 768 | loss: 0.17522 - acc: 0.9890 -- iter: 50/55\n","Training Step: 8448  | total loss: \u001b[1m\u001b[32m0.15794\u001b[0m\u001b[0m | time: 0.059s\n","| Adam | epoch: 768 | loss: 0.15794 - acc: 0.9901 -- iter: 55/55\n","--\n","Training Step: 8449  | total loss: \u001b[1m\u001b[32m0.14301\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 769 | loss: 0.14301 - acc: 0.9911 -- iter: 05/55\n","Training Step: 8450  | total loss: \u001b[1m\u001b[32m0.12919\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 769 | loss: 0.12919 - acc: 0.9920 -- iter: 10/55\n","Training Step: 8451  | total loss: \u001b[1m\u001b[32m1.83478\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 769 | loss: 1.83478 - acc: 0.8928 -- iter: 15/55\n","Training Step: 8452  | total loss: \u001b[1m\u001b[32m1.65182\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 769 | loss: 1.65182 - acc: 0.9035 -- iter: 20/55\n","Training Step: 8453  | total loss: \u001b[1m\u001b[32m1.48745\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 769 | loss: 1.48745 - acc: 0.9132 -- iter: 25/55\n","Training Step: 8454  | total loss: \u001b[1m\u001b[32m1.33923\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 769 | loss: 1.33923 - acc: 0.9219 -- iter: 30/55\n","Training Step: 8455  | total loss: \u001b[1m\u001b[32m1.20568\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 769 | loss: 1.20568 - acc: 0.9297 -- iter: 35/55\n","Training Step: 8456  | total loss: \u001b[1m\u001b[32m1.20568\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 769 | loss: 1.20568 - acc: 0.9367 -- iter: 40/55\n","Training Step: 8457  | total loss: \u001b[1m\u001b[32m1.08578\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 769 | loss: 1.08578 - acc: 0.9430 -- iter: 45/55\n","Training Step: 8458  | total loss: \u001b[1m\u001b[32m0.97839\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 769 | loss: 0.97839 - acc: 0.9487 -- iter: 50/55\n","Training Step: 8459  | total loss: \u001b[1m\u001b[32m0.88184\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 769 | loss: 0.88184 - acc: 0.9539 -- iter: 55/55\n","--\n","Training Step: 8460  | total loss: \u001b[1m\u001b[32m0.79438\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 770 | loss: 0.79438 - acc: 0.9585 -- iter: 05/55\n","Training Step: 8461  | total loss: \u001b[1m\u001b[32m0.71566\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 770 | loss: 0.71566 - acc: 0.9626 -- iter: 10/55\n","Training Step: 8462  | total loss: \u001b[1m\u001b[32m0.64497\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 770 | loss: 0.64497 - acc: 0.9664 -- iter: 15/55\n","Training Step: 8463  | total loss: \u001b[1m\u001b[32m0.58057\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 770 | loss: 0.58057 - acc: 0.8697 -- iter: 20/55\n","Training Step: 8464  | total loss: \u001b[1m\u001b[32m2.08574\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 770 | loss: 2.08574 - acc: 0.8828 -- iter: 25/55\n","Training Step: 8465  | total loss: \u001b[1m\u001b[32m1.87900\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 770 | loss: 1.87900 - acc: 0.8945 -- iter: 30/55\n","Training Step: 8466  | total loss: \u001b[1m\u001b[32m1.69164\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 770 | loss: 1.69164 - acc: 0.9050 -- iter: 35/55\n","Training Step: 8467  | total loss: \u001b[1m\u001b[32m1.52264\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 770 | loss: 1.52264 - acc: 0.9145 -- iter: 40/55\n","Training Step: 8468  | total loss: \u001b[1m\u001b[32m1.23462\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 770 | loss: 1.23462 - acc: 0.9231 -- iter: 45/55\n","Training Step: 8469  | total loss: \u001b[1m\u001b[32m1.11191\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 770 | loss: 1.11191 - acc: 0.9308 -- iter: 50/55\n","Training Step: 8470  | total loss: \u001b[1m\u001b[32m1.00154\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 770 | loss: 1.00154 - acc: 0.9377 -- iter: 55/55\n","--\n","Training Step: 8471  | total loss: \u001b[1m\u001b[32m0.90180\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 771 | loss: 0.90180 - acc: 0.9439 -- iter: 05/55\n","Training Step: 8472  | total loss: \u001b[1m\u001b[32m0.81204\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 771 | loss: 0.81204 - acc: 0.9495 -- iter: 10/55\n","Training Step: 8473  | total loss: \u001b[1m\u001b[32m0.81204\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 771 | loss: 0.81204 - acc: 0.9546 -- iter: 15/55\n","Training Step: 8474  | total loss: \u001b[1m\u001b[32m0.73104\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 771 | loss: 0.73104 - acc: 0.9591 -- iter: 20/55\n","Training Step: 8475  | total loss: \u001b[1m\u001b[32m0.65858\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 771 | loss: 0.65858 - acc: 0.9591 -- iter: 25/55\n","Training Step: 8476  | total loss: \u001b[1m\u001b[32m0.53514\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 771 | loss: 0.53514 - acc: 0.9669 -- iter: 30/55\n","Training Step: 8477  | total loss: \u001b[1m\u001b[32m0.48280\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 771 | loss: 0.48280 - acc: 0.9702 -- iter: 35/55\n","Training Step: 8478  | total loss: \u001b[1m\u001b[32m0.48280\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 771 | loss: 0.48280 - acc: 0.9702 -- iter: 40/55\n","Training Step: 8479  | total loss: \u001b[1m\u001b[32m0.43455\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 771 | loss: 0.43455 - acc: 0.9759 -- iter: 45/55\n","Training Step: 8480  | total loss: \u001b[1m\u001b[32m0.39160\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 771 | loss: 0.39160 - acc: 0.9783 -- iter: 50/55\n","Training Step: 8481  | total loss: \u001b[1m\u001b[32m0.31887\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 771 | loss: 0.31887 - acc: 0.9804 -- iter: 55/55\n","--\n","Training Step: 8482  | total loss: \u001b[1m\u001b[32m0.28934\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 772 | loss: 0.28934 - acc: 0.9824 -- iter: 05/55\n","Training Step: 8483  | total loss: \u001b[1m\u001b[32m0.26103\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 772 | loss: 0.26103 - acc: 0.9842 -- iter: 10/55\n","Training Step: 8484  | total loss: \u001b[1m\u001b[32m0.23555\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 772 | loss: 0.23555 - acc: 0.9857 -- iter: 15/55\n","Training Step: 8485  | total loss: \u001b[1m\u001b[32m0.21238\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 772 | loss: 0.21238 - acc: 0.9872 -- iter: 20/55\n","Training Step: 8486  | total loss: \u001b[1m\u001b[32m0.19177\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 772 | loss: 0.19177 - acc: 0.9885 -- iter: 25/55\n","Training Step: 8487  | total loss: \u001b[1m\u001b[32m1.72580\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 772 | loss: 1.72580 - acc: 0.8896 -- iter: 30/55\n","Training Step: 8488  | total loss: \u001b[1m\u001b[32m1.55366\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 772 | loss: 1.55366 - acc: 0.9006 -- iter: 35/55\n","Training Step: 8489  | total loss: \u001b[1m\u001b[32m1.55366\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 772 | loss: 1.55366 - acc: 0.9006 -- iter: 40/55\n","Training Step: 8490  | total loss: \u001b[1m\u001b[32m1.25974\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 772 | loss: 1.25974 - acc: 0.9195 -- iter: 45/55\n","Training Step: 8491  | total loss: \u001b[1m\u001b[32m1.13599\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 772 | loss: 1.13599 - acc: 0.9276 -- iter: 50/55\n","Training Step: 8492  | total loss: \u001b[1m\u001b[32m1.02298\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 772 | loss: 1.02298 - acc: 0.9276 -- iter: 55/55\n","--\n","Training Step: 8493  | total loss: \u001b[1m\u001b[32m1.02298\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 773 | loss: 1.02298 - acc: 0.9348 -- iter: 05/55\n","Training Step: 8494  | total loss: \u001b[1m\u001b[32m0.92134\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 773 | loss: 0.92134 - acc: 0.9413 -- iter: 10/55\n","Training Step: 8495  | total loss: \u001b[1m\u001b[32m0.82977\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 773 | loss: 0.82977 - acc: 0.9472 -- iter: 15/55\n","Training Step: 8496  | total loss: \u001b[1m\u001b[32m0.74720\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 773 | loss: 0.74720 - acc: 0.9525 -- iter: 20/55\n","Training Step: 8497  | total loss: \u001b[1m\u001b[32m0.67289\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 773 | loss: 0.67289 - acc: 0.9572 -- iter: 25/55\n","Training Step: 8498  | total loss: \u001b[1m\u001b[32m0.60657\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 773 | loss: 0.60657 - acc: 0.9615 -- iter: 30/55\n","Training Step: 8499  | total loss: \u001b[1m\u001b[32m0.54703\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 773 | loss: 0.54703 - acc: 0.9654 -- iter: 35/55\n","Training Step: 8500  | total loss: \u001b[1m\u001b[32m0.49315\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 773 | loss: 0.49315 - acc: 0.9688 -- iter: 40/55\n","Training Step: 8501  | total loss: \u001b[1m\u001b[32m0.44446\u001b[0m\u001b[0m | time: 0.062s\n","| Adam | epoch: 773 | loss: 0.44446 - acc: 0.9719 -- iter: 45/55\n","Training Step: 8502  | total loss: \u001b[1m\u001b[32m0.40033\u001b[0m\u001b[0m | time: 0.070s\n","| Adam | epoch: 773 | loss: 0.40033 - acc: 0.9747 -- iter: 50/55\n","Training Step: 8503  | total loss: \u001b[1m\u001b[32m0.36059\u001b[0m\u001b[0m | time: 0.079s\n","| Adam | epoch: 773 | loss: 0.36059 - acc: 0.9773 -- iter: 55/55\n","--\n","Training Step: 8504  | total loss: \u001b[1m\u001b[32m0.32549\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 774 | loss: 0.32549 - acc: 0.9795 -- iter: 05/55\n","Training Step: 8505  | total loss: \u001b[1m\u001b[32m0.29329\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 774 | loss: 0.29329 - acc: 0.9816 -- iter: 10/55\n","Training Step: 8506  | total loss: \u001b[1m\u001b[32m0.26636\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 774 | loss: 0.26636 - acc: 0.9834 -- iter: 15/55\n","Training Step: 8507  | total loss: \u001b[1m\u001b[32m0.24125\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 774 | loss: 0.24125 - acc: 0.9851 -- iter: 20/55\n","Training Step: 8508  | total loss: \u001b[1m\u001b[32m0.21773\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 774 | loss: 0.21773 - acc: 0.9866 -- iter: 25/55\n","Training Step: 8509  | total loss: \u001b[1m\u001b[32m0.17782\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 774 | loss: 0.17782 - acc: 0.9879 -- iter: 30/55\n","Training Step: 8510  | total loss: \u001b[1m\u001b[32m0.17782\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 774 | loss: 0.17782 - acc: 0.9891 -- iter: 35/55\n","Training Step: 8511  | total loss: \u001b[1m\u001b[32m0.14571\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 774 | loss: 0.14571 - acc: 0.9902 -- iter: 40/55\n","Training Step: 8512  | total loss: \u001b[1m\u001b[32m0.14571\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 774 | loss: 0.14571 - acc: 0.9912 -- iter: 45/55\n","Training Step: 8513  | total loss: \u001b[1m\u001b[32m0.13136\u001b[0m\u001b[0m | time: 0.064s\n","| Adam | epoch: 774 | loss: 0.13136 - acc: 0.9921 -- iter: 50/55\n","Training Step: 8514  | total loss: \u001b[1m\u001b[32m0.11865\u001b[0m\u001b[0m | time: 0.070s\n","| Adam | epoch: 774 | loss: 0.11865 - acc: 0.9929 -- iter: 55/55\n","--\n","Training Step: 8515  | total loss: \u001b[1m\u001b[32m0.10763\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 775 | loss: 0.10763 - acc: 0.9936 -- iter: 05/55\n","Training Step: 8516  | total loss: \u001b[1m\u001b[32m0.09809\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 775 | loss: 0.09809 - acc: 0.9942 -- iter: 10/55\n","Training Step: 8517  | total loss: \u001b[1m\u001b[32m0.08861\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 775 | loss: 0.08861 - acc: 0.9948 -- iter: 15/55\n","Training Step: 8518  | total loss: \u001b[1m\u001b[32m0.08075\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 775 | loss: 0.08075 - acc: 0.9953 -- iter: 20/55\n","Training Step: 8519  | total loss: \u001b[1m\u001b[32m0.06783\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 775 | loss: 0.06783 - acc: 0.9958 -- iter: 25/55\n","Training Step: 8520  | total loss: \u001b[1m\u001b[32m0.06783\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 775 | loss: 0.06783 - acc: 0.9962 -- iter: 30/55\n","Training Step: 8521  | total loss: \u001b[1m\u001b[32m0.06179\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 775 | loss: 0.06179 - acc: 0.9966 -- iter: 35/55\n","Training Step: 8522  | total loss: \u001b[1m\u001b[32m0.05619\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 775 | loss: 0.05619 - acc: 0.9969 -- iter: 40/55\n","Training Step: 8523  | total loss: \u001b[1m\u001b[32m0.05101\u001b[0m\u001b[0m | time: 0.063s\n","| Adam | epoch: 775 | loss: 0.05101 - acc: 0.9972 -- iter: 45/55\n","Training Step: 8524  | total loss: \u001b[1m\u001b[32m0.78902\u001b[0m\u001b[0m | time: 0.069s\n","| Adam | epoch: 775 | loss: 0.78902 - acc: 0.9375 -- iter: 50/55\n","Training Step: 8525  | total loss: \u001b[1m\u001b[32m0.71055\u001b[0m\u001b[0m | time: 0.077s\n","| Adam | epoch: 775 | loss: 0.71055 - acc: 0.9438 -- iter: 55/55\n","--\n","Training Step: 8526  | total loss: \u001b[1m\u001b[32m0.57709\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 776 | loss: 0.57709 - acc: 0.9494 -- iter: 05/55\n","Training Step: 8527  | total loss: \u001b[1m\u001b[32m0.57709\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 776 | loss: 0.57709 - acc: 0.9544 -- iter: 10/55\n","Training Step: 8528  | total loss: \u001b[1m\u001b[32m0.52033\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 776 | loss: 0.52033 - acc: 0.9590 -- iter: 15/55\n","Training Step: 8529  | total loss: \u001b[1m\u001b[32m0.46974\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 776 | loss: 0.46974 - acc: 0.9631 -- iter: 20/55\n","Training Step: 8530  | total loss: \u001b[1m\u001b[32m0.42330\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 776 | loss: 0.42330 - acc: 0.9668 -- iter: 25/55\n","Training Step: 8531  | total loss: \u001b[1m\u001b[32m0.38151\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 776 | loss: 0.38151 - acc: 0.9701 -- iter: 30/55\n","Training Step: 8717  | total loss: \u001b[1m\u001b[32m0.00750\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 793 | loss: 0.00750 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8718  | total loss: \u001b[1m\u001b[32m0.00744\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 793 | loss: 0.00744 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8719  | total loss: \u001b[1m\u001b[32m0.00774\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 793 | loss: 0.00774 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8720  | total loss: \u001b[1m\u001b[32m0.00751\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 793 | loss: 0.00751 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8721  | total loss: \u001b[1m\u001b[32m0.00748\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 793 | loss: 0.00748 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8722  | total loss: \u001b[1m\u001b[32m0.00704\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 793 | loss: 0.00704 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8723  | total loss: \u001b[1m\u001b[32m0.00666\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 793 | loss: 0.00666 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8724  | total loss: \u001b[1m\u001b[32m0.00749\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 794 | loss: 0.00749 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8725  | total loss: \u001b[1m\u001b[32m0.00822\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 794 | loss: 0.00822 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8726  | total loss: \u001b[1m\u001b[32m0.00798\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 794 | loss: 0.00798 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8727  | total loss: \u001b[1m\u001b[32m0.00757\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 794 | loss: 0.00757 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8728  | total loss: \u001b[1m\u001b[32m0.00753\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 794 | loss: 0.00753 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8729  | total loss: \u001b[1m\u001b[32m0.00714\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 794 | loss: 0.00714 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8730  | total loss: \u001b[1m\u001b[32m0.00710\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 794 | loss: 0.00710 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8731  | total loss: \u001b[1m\u001b[32m0.00674\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 794 | loss: 0.00674 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8732  | total loss: \u001b[1m\u001b[32m0.00664\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 794 | loss: 0.00664 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8733  | total loss: \u001b[1m\u001b[32m0.00725\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 794 | loss: 0.00725 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8734  | total loss: \u001b[1m\u001b[32m0.00677\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 794 | loss: 0.00677 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8735  | total loss: \u001b[1m\u001b[32m0.00708\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 795 | loss: 0.00708 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8736  | total loss: \u001b[1m\u001b[32m0.00736\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 795 | loss: 0.00736 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8737  | total loss: \u001b[1m\u001b[32m0.00766\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 795 | loss: 0.00766 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8738  | total loss: \u001b[1m\u001b[32m0.00766\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 795 | loss: 0.00766 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8739  | total loss: \u001b[1m\u001b[32m0.00736\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 795 | loss: 0.00736 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8740  | total loss: \u001b[1m\u001b[32m0.00712\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 795 | loss: 0.00712 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8741  | total loss: \u001b[1m\u001b[32m0.00712\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 795 | loss: 0.00712 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8742  | total loss: \u001b[1m\u001b[32m0.00712\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 795 | loss: 0.00712 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8743  | total loss: \u001b[1m\u001b[32m0.00734\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 795 | loss: 0.00734 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8744  | total loss: \u001b[1m\u001b[32m0.00705\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 795 | loss: 0.00705 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8745  | total loss: \u001b[1m\u001b[32m0.00672\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 795 | loss: 0.00672 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8746  | total loss: \u001b[1m\u001b[32m0.00686\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 796 | loss: 0.00686 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8747  | total loss: \u001b[1m\u001b[32m0.00664\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 796 | loss: 0.00664 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8748  | total loss: \u001b[1m\u001b[32m0.00644\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 796 | loss: 0.00644 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8749  | total loss: \u001b[1m\u001b[32m0.00643\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 796 | loss: 0.00643 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8750  | total loss: \u001b[1m\u001b[32m0.00628\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 796 | loss: 0.00628 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8751  | total loss: \u001b[1m\u001b[32m0.00622\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 796 | loss: 0.00622 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8752  | total loss: \u001b[1m\u001b[32m0.00625\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 796 | loss: 0.00625 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8753  | total loss: \u001b[1m\u001b[32m0.00625\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 796 | loss: 0.00625 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8754  | total loss: \u001b[1m\u001b[32m0.00622\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 796 | loss: 0.00622 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8755  | total loss: \u001b[1m\u001b[32m0.00657\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 796 | loss: 0.00657 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8756  | total loss: \u001b[1m\u001b[32m0.00639\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 796 | loss: 0.00639 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8757  | total loss: \u001b[1m\u001b[32m0.00617\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 797 | loss: 0.00617 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8758  | total loss: \u001b[1m\u001b[32m0.00587\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 797 | loss: 0.00587 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8759  | total loss: \u001b[1m\u001b[32m0.00582\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 797 | loss: 0.00582 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8760  | total loss: \u001b[1m\u001b[32m0.00578\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 797 | loss: 0.00578 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8761  | total loss: \u001b[1m\u001b[32m0.00579\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 797 | loss: 0.00579 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8762  | total loss: \u001b[1m\u001b[32m0.00609\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 797 | loss: 0.00609 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8763  | total loss: \u001b[1m\u001b[32m0.00606\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 797 | loss: 0.00606 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8764  | total loss: \u001b[1m\u001b[32m0.00605\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 797 | loss: 0.00605 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8765  | total loss: \u001b[1m\u001b[32m0.00592\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 797 | loss: 0.00592 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8766  | total loss: \u001b[1m\u001b[32m0.00567\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 797 | loss: 0.00567 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8767  | total loss: \u001b[1m\u001b[32m0.00560\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 797 | loss: 0.00560 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8768  | total loss: \u001b[1m\u001b[32m0.00571\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 798 | loss: 0.00571 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8769  | total loss: \u001b[1m\u001b[32m0.00576\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 798 | loss: 0.00576 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8770  | total loss: \u001b[1m\u001b[32m0.00575\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 798 | loss: 0.00575 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8771  | total loss: \u001b[1m\u001b[32m0.00588\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 798 | loss: 0.00588 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8772  | total loss: \u001b[1m\u001b[32m0.00588\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 798 | loss: 0.00588 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8773  | total loss: \u001b[1m\u001b[32m0.00600\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 798 | loss: 0.00600 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8774  | total loss: \u001b[1m\u001b[32m0.00590\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 798 | loss: 0.00590 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8775  | total loss: \u001b[1m\u001b[32m0.00607\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 798 | loss: 0.00607 - acc: 0.9000 -- iter: 40/55\n","Training Step: 8776  | total loss: \u001b[1m\u001b[32m1.95935\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 798 | loss: 1.95935 - acc: 0.9000 -- iter: 45/55\n","Training Step: 8777  | total loss: \u001b[1m\u001b[32m1.76397\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 798 | loss: 1.76397 - acc: 0.9100 -- iter: 50/55\n","Training Step: 8778  | total loss: \u001b[1m\u001b[32m1.58812\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 798 | loss: 1.58812 - acc: 0.9190 -- iter: 55/55\n","--\n","Training Step: 8779  | total loss: \u001b[1m\u001b[32m1.42955\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 799 | loss: 1.42955 - acc: 0.9344 -- iter: 05/55\n","Training Step: 8780  | total loss: \u001b[1m\u001b[32m1.28689\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 799 | loss: 1.28689 - acc: 0.9344 -- iter: 10/55\n","Training Step: 8781  | total loss: \u001b[1m\u001b[32m1.15869\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 799 | loss: 1.15869 - acc: 0.9410 -- iter: 15/55\n","Training Step: 8782  | total loss: \u001b[1m\u001b[32m1.04304\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 799 | loss: 1.04304 - acc: 0.9469 -- iter: 20/55\n","Training Step: 8783  | total loss: \u001b[1m\u001b[32m0.93944\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 799 | loss: 0.93944 - acc: 0.9522 -- iter: 25/55\n","Training Step: 8784  | total loss: \u001b[1m\u001b[32m0.84716\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 799 | loss: 0.84716 - acc: 0.9570 -- iter: 30/55\n","Training Step: 8785  | total loss: \u001b[1m\u001b[32m0.76424\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 799 | loss: 0.76424 - acc: 0.9613 -- iter: 35/55\n","Training Step: 8786  | total loss: \u001b[1m\u001b[32m0.68838\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 799 | loss: 0.68838 - acc: 0.9651 -- iter: 40/55\n","Training Step: 8787  | total loss: \u001b[1m\u001b[32m0.62059\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 799 | loss: 0.62059 - acc: 0.9718 -- iter: 45/55\n","Training Step: 8788  | total loss: \u001b[1m\u001b[32m0.55945\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 799 | loss: 0.55945 - acc: 0.9746 -- iter: 50/55\n","Training Step: 8789  | total loss: \u001b[1m\u001b[32m0.50390\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 799 | loss: 0.50390 - acc: 0.9771 -- iter: 55/55\n","--\n","Training Step: 8790  | total loss: \u001b[1m\u001b[32m0.45404\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 800 | loss: 0.45404 - acc: 0.9794 -- iter: 05/55\n","Training Step: 8791  | total loss: \u001b[1m\u001b[32m0.40905\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 800 | loss: 0.40905 - acc: 0.9794 -- iter: 10/55\n","Training Step: 8792  | total loss: \u001b[1m\u001b[32m0.36931\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 800 | loss: 0.36931 - acc: 0.9815 -- iter: 15/55\n","Training Step: 8793  | total loss: \u001b[1m\u001b[32m0.33295\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 800 | loss: 0.33295 - acc: 0.9833 -- iter: 20/55\n","Training Step: 8794  | total loss: \u001b[1m\u001b[32m0.29995\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 800 | loss: 0.29995 - acc: 0.9850 -- iter: 25/55\n","Training Step: 8795  | total loss: \u001b[1m\u001b[32m0.27049\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 800 | loss: 0.27049 - acc: 0.9878 -- iter: 30/55\n","Training Step: 8796  | total loss: \u001b[1m\u001b[32m0.24435\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 800 | loss: 0.24435 - acc: 0.9878 -- iter: 35/55\n","Training Step: 8797  | total loss: \u001b[1m\u001b[32m0.22082\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 800 | loss: 0.22082 - acc: 0.9891 -- iter: 40/55\n","Training Step: 8798  | total loss: \u001b[1m\u001b[32m0.20088\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 800 | loss: 0.20088 - acc: 0.9902 -- iter: 45/55\n","Training Step: 8799  | total loss: \u001b[1m\u001b[32m0.18134\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 800 | loss: 0.18134 - acc: 0.9911 -- iter: 50/55\n","Training Step: 8800  | total loss: \u001b[1m\u001b[32m0.16368\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 800 | loss: 0.16368 - acc: 0.9920 -- iter: 55/55\n","--\n","Training Step: 8801  | total loss: \u001b[1m\u001b[32m0.14760\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 801 | loss: 0.14760 - acc: 0.9935 -- iter: 05/55\n","Training Step: 8802  | total loss: \u001b[1m\u001b[32m0.13350\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 801 | loss: 0.13350 - acc: 0.9942 -- iter: 10/55\n","Training Step: 8803  | total loss: \u001b[1m\u001b[32m0.12068\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 801 | loss: 0.12068 - acc: 0.9942 -- iter: 15/55\n","Training Step: 8804  | total loss: \u001b[1m\u001b[32m0.10975\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 801 | loss: 0.10975 - acc: 0.9948 -- iter: 20/55\n","Training Step: 8805  | total loss: \u001b[1m\u001b[32m0.09938\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 801 | loss: 0.09938 - acc: 0.9953 -- iter: 25/55\n","Training Step: 8806  | total loss: \u001b[1m\u001b[32m0.09100\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 801 | loss: 0.09100 - acc: 0.9958 -- iter: 30/55\n","Training Step: 8807  | total loss: \u001b[1m\u001b[32m0.08229\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 801 | loss: 0.08229 - acc: 0.9962 -- iter: 35/55\n","Training Step: 8808  | total loss: \u001b[1m\u001b[32m0.07458\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 801 | loss: 0.07458 - acc: 0.9966 -- iter: 40/55\n","Training Step: 8809  | total loss: \u001b[1m\u001b[32m0.06763\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 801 | loss: 0.06763 - acc: 0.9969 -- iter: 45/55\n","Training Step: 8810  | total loss: \u001b[1m\u001b[32m0.06105\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 801 | loss: 0.06105 - acc: 0.9972 -- iter: 50/55\n","Training Step: 8811  | total loss: \u001b[1m\u001b[32m0.05600\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 801 | loss: 0.05600 - acc: 0.9977 -- iter: 55/55\n","--\n","Training Step: 8812  | total loss: \u001b[1m\u001b[32m0.05079\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 802 | loss: 0.05079 - acc: 0.9980 -- iter: 05/55\n","Training Step: 8813  | total loss: \u001b[1m\u001b[32m0.04589\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 802 | loss: 0.04589 - acc: 0.9982 -- iter: 10/55\n","Training Step: 8814  | total loss: \u001b[1m\u001b[32m0.04182\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 802 | loss: 0.04182 - acc: 0.9984 -- iter: 15/55\n","Training Step: 8815  | total loss: \u001b[1m\u001b[32m0.03825\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 802 | loss: 0.03825 - acc: 0.9985 -- iter: 20/55\n","Training Step: 8816  | total loss: \u001b[1m\u001b[32m0.03485\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 802 | loss: 0.03485 - acc: 0.9987 -- iter: 25/55\n","Training Step: 8817  | total loss: \u001b[1m\u001b[32m0.03207\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 802 | loss: 0.03207 - acc: 0.9988 -- iter: 30/55\n","Training Step: 8818  | total loss: \u001b[1m\u001b[32m0.02950\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 802 | loss: 0.02950 - acc: 0.9989 -- iter: 35/55\n","Training Step: 8819  | total loss: \u001b[1m\u001b[32m0.02796\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 802 | loss: 0.02796 - acc: 0.9990 -- iter: 40/55\n","Training Step: 8820  | total loss: \u001b[1m\u001b[32m0.02540\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 802 | loss: 0.02540 - acc: 0.9991 -- iter: 45/55\n","Training Step: 8821  | total loss: \u001b[1m\u001b[32m0.02309\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 802 | loss: 0.02309 - acc: 0.9992 -- iter: 50/55\n","Training Step: 8822  | total loss: \u001b[1m\u001b[32m0.02126\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 802 | loss: 0.02126 - acc: 0.9993 -- iter: 55/55\n","--\n","Training Step: 8823  | total loss: \u001b[1m\u001b[32m0.01947\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 803 | loss: 0.01947 - acc: 0.9994 -- iter: 05/55\n","Training Step: 8824  | total loss: \u001b[1m\u001b[32m0.01846\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 803 | loss: 0.01846 - acc: 0.9994 -- iter: 10/55\n","Training Step: 8825  | total loss: \u001b[1m\u001b[32m0.01698\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 803 | loss: 0.01698 - acc: 0.9995 -- iter: 15/55\n","Training Step: 8826  | total loss: \u001b[1m\u001b[32m0.01618\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 803 | loss: 0.01618 - acc: 0.9995 -- iter: 20/55\n","Training Step: 8827  | total loss: \u001b[1m\u001b[32m0.01513\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 803 | loss: 0.01513 - acc: 0.9996 -- iter: 25/55\n","Training Step: 8828  | total loss: \u001b[1m\u001b[32m0.01428\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 803 | loss: 0.01428 - acc: 0.9996 -- iter: 30/55\n","Training Step: 8829  | total loss: \u001b[1m\u001b[32m0.01318\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 803 | loss: 0.01318 - acc: 0.9997 -- iter: 35/55\n","Training Step: 8830  | total loss: \u001b[1m\u001b[32m0.01260\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 803 | loss: 0.01260 - acc: 0.9997 -- iter: 40/55\n","Training Step: 8831  | total loss: \u001b[1m\u001b[32m0.01163\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 803 | loss: 0.01163 - acc: 0.9997 -- iter: 45/55\n","Training Step: 8832  | total loss: \u001b[1m\u001b[32m0.01096\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 803 | loss: 0.01096 - acc: 0.9998 -- iter: 50/55\n","Training Step: 8833  | total loss: \u001b[1m\u001b[32m0.01035\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 803 | loss: 0.01035 - acc: 0.9998 -- iter: 55/55\n","--\n","Training Step: 8834  | total loss: \u001b[1m\u001b[32m0.01010\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 804 | loss: 0.01010 - acc: 0.9998 -- iter: 05/55\n","Training Step: 8835  | total loss: \u001b[1m\u001b[32m0.00938\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 804 | loss: 0.00938 - acc: 0.9998 -- iter: 10/55\n","Training Step: 8836  | total loss: \u001b[1m\u001b[32m0.00916\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 804 | loss: 0.00916 - acc: 0.9998 -- iter: 15/55\n","Training Step: 8837  | total loss: \u001b[1m\u001b[32m0.00918\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 804 | loss: 0.00918 - acc: 0.9998 -- iter: 20/55\n","Training Step: 8838  | total loss: \u001b[1m\u001b[32m0.00844\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 804 | loss: 0.00844 - acc: 0.9999 -- iter: 25/55\n","Training Step: 8839  | total loss: \u001b[1m\u001b[32m0.00838\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 804 | loss: 0.00838 - acc: 0.9999 -- iter: 30/55\n","Training Step: 8840  | total loss: \u001b[1m\u001b[32m0.00833\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 804 | loss: 0.00833 - acc: 0.9999 -- iter: 35/55\n","Training Step: 8841  | total loss: \u001b[1m\u001b[32m0.00762\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 804 | loss: 0.00762 - acc: 0.9999 -- iter: 40/55\n","Training Step: 8842  | total loss: \u001b[1m\u001b[32m0.00732\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 804 | loss: 0.00732 - acc: 0.9999 -- iter: 45/55\n","Training Step: 8843  | total loss: \u001b[1m\u001b[32m0.00708\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 804 | loss: 0.00708 - acc: 0.9999 -- iter: 50/55\n","Training Step: 8844  | total loss: \u001b[1m\u001b[32m0.00689\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 804 | loss: 0.00689 - acc: 0.9999 -- iter: 55/55\n","--\n","Training Step: 8845  | total loss: \u001b[1m\u001b[32m0.00655\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 805 | loss: 0.00655 - acc: 0.9999 -- iter: 05/55\n","Training Step: 8846  | total loss: \u001b[1m\u001b[32m0.00648\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 805 | loss: 0.00648 - acc: 0.9999 -- iter: 10/55\n","Training Step: 8847  | total loss: \u001b[1m\u001b[32m0.00621\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 805 | loss: 0.00621 - acc: 0.9999 -- iter: 15/55\n","Training Step: 8848  | total loss: \u001b[1m\u001b[32m0.00630\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 805 | loss: 0.00630 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8849  | total loss: \u001b[1m\u001b[32m0.00580\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 805 | loss: 0.00580 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8850  | total loss: \u001b[1m\u001b[32m0.00532\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 805 | loss: 0.00532 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8851  | total loss: \u001b[1m\u001b[32m0.00539\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 805 | loss: 0.00539 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8852  | total loss: \u001b[1m\u001b[32m0.00553\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 805 | loss: 0.00553 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8853  | total loss: \u001b[1m\u001b[32m0.00539\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 805 | loss: 0.00539 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8854  | total loss: \u001b[1m\u001b[32m0.00580\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 805 | loss: 0.00580 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8855  | total loss: \u001b[1m\u001b[32m0.00604\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 805 | loss: 0.00604 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8856  | total loss: \u001b[1m\u001b[32m0.00626\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 806 | loss: 0.00626 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8857  | total loss: \u001b[1m\u001b[32m0.00591\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 806 | loss: 0.00591 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8858  | total loss: \u001b[1m\u001b[32m0.00580\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 806 | loss: 0.00580 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8859  | total loss: \u001b[1m\u001b[32m0.00563\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 806 | loss: 0.00563 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8860  | total loss: \u001b[1m\u001b[32m0.00546\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 806 | loss: 0.00546 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8861  | total loss: \u001b[1m\u001b[32m0.00515\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 806 | loss: 0.00515 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8862  | total loss: \u001b[1m\u001b[32m0.00483\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 806 | loss: 0.00483 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8863  | total loss: \u001b[1m\u001b[32m0.00480\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 806 | loss: 0.00480 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8864  | total loss: \u001b[1m\u001b[32m0.00480\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 806 | loss: 0.00480 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8865  | total loss: \u001b[1m\u001b[32m0.00501\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 806 | loss: 0.00501 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8866  | total loss: \u001b[1m\u001b[32m0.00503\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 806 | loss: 0.00503 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8867  | total loss: \u001b[1m\u001b[32m0.00521\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 807 | loss: 0.00521 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8868  | total loss: \u001b[1m\u001b[32m0.00528\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 807 | loss: 0.00528 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8869  | total loss: \u001b[1m\u001b[32m0.00528\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 807 | loss: 0.00528 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8870  | total loss: \u001b[1m\u001b[32m0.00550\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 807 | loss: 0.00550 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8871  | total loss: \u001b[1m\u001b[32m0.00540\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 807 | loss: 0.00540 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8872  | total loss: \u001b[1m\u001b[32m0.00531\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 807 | loss: 0.00531 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8873  | total loss: \u001b[1m\u001b[32m0.00531\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 807 | loss: 0.00531 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8874  | total loss: \u001b[1m\u001b[32m0.00550\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 807 | loss: 0.00550 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8875  | total loss: \u001b[1m\u001b[32m0.00523\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 807 | loss: 0.00523 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8876  | total loss: \u001b[1m\u001b[32m0.00525\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 807 | loss: 0.00525 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8877  | total loss: \u001b[1m\u001b[32m0.00535\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 807 | loss: 0.00535 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8878  | total loss: \u001b[1m\u001b[32m0.00505\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 808 | loss: 0.00505 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8879  | total loss: \u001b[1m\u001b[32m0.00493\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 808 | loss: 0.00493 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8880  | total loss: \u001b[1m\u001b[32m0.00482\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 808 | loss: 0.00482 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8881  | total loss: \u001b[1m\u001b[32m0.00488\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 808 | loss: 0.00488 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8882  | total loss: \u001b[1m\u001b[32m0.00503\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 808 | loss: 0.00503 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8883  | total loss: \u001b[1m\u001b[32m1.95785\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 808 | loss: 1.95785 - acc: 0.9000 -- iter: 30/55\n","Training Step: 8884  | total loss: \u001b[1m\u001b[32m1.76213\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 808 | loss: 1.76213 - acc: 0.9100 -- iter: 35/55\n","Training Step: 8885  | total loss: \u001b[1m\u001b[32m1.58655\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 808 | loss: 1.58655 - acc: 0.9190 -- iter: 40/55\n","Training Step: 8886  | total loss: \u001b[1m\u001b[32m1.42834\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 808 | loss: 1.42834 - acc: 0.9271 -- iter: 45/55\n","Training Step: 8887  | total loss: \u001b[1m\u001b[32m1.28576\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 808 | loss: 1.28576 - acc: 0.9344 -- iter: 50/55\n","Training Step: 8888  | total loss: \u001b[1m\u001b[32m1.15775\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 808 | loss: 1.15775 - acc: 0.9410 -- iter: 55/55\n","--\n","Training Step: 8889  | total loss: \u001b[1m\u001b[32m1.04225\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 809 | loss: 1.04225 - acc: 0.9469 -- iter: 05/55\n","Training Step: 8890  | total loss: \u001b[1m\u001b[32m0.93909\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 809 | loss: 0.93909 - acc: 0.9522 -- iter: 10/55\n","Training Step: 8891  | total loss: \u001b[1m\u001b[32m0.84572\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 809 | loss: 0.84572 - acc: 0.9570 -- iter: 15/55\n","Training Step: 8892  | total loss: \u001b[1m\u001b[32m0.76169\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 809 | loss: 0.76169 - acc: 0.9613 -- iter: 20/55\n","Training Step: 8893  | total loss: \u001b[1m\u001b[32m0.68567\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 809 | loss: 0.68567 - acc: 0.9651 -- iter: 25/55\n","Training Step: 8894  | total loss: \u001b[1m\u001b[32m0.61754\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 809 | loss: 0.61754 - acc: 0.9686 -- iter: 30/55\n","Training Step: 8895  | total loss: \u001b[1m\u001b[32m0.55696\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 809 | loss: 0.55696 - acc: 0.9718 -- iter: 35/55\n","Training Step: 8896  | total loss: \u001b[1m\u001b[32m0.50190\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 809 | loss: 0.50190 - acc: 0.9746 -- iter: 40/55\n","Training Step: 8897  | total loss: \u001b[1m\u001b[32m0.50190\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 809 | loss: 0.50190 - acc: 0.9771 -- iter: 45/55\n","Training Step: 8898  | total loss: \u001b[1m\u001b[32m0.45271\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 809 | loss: 0.45271 - acc: 0.9794 -- iter: 50/55\n","Training Step: 8899  | total loss: \u001b[1m\u001b[32m0.40777\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 809 | loss: 0.40777 - acc: 0.9815 -- iter: 55/55\n","--\n","Training Step: 8900  | total loss: \u001b[1m\u001b[32m0.36741\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 810 | loss: 0.36741 - acc: 0.9833 -- iter: 05/55\n","Training Step: 8901  | total loss: \u001b[1m\u001b[32m0.33125\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 810 | loss: 0.33125 - acc: 0.9850 -- iter: 10/55\n","Training Step: 8902  | total loss: \u001b[1m\u001b[32m0.29844\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 810 | loss: 0.29844 - acc: 0.9865 -- iter: 15/55\n","Training Step: 8903  | total loss: \u001b[1m\u001b[32m0.26884\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 810 | loss: 0.26884 - acc: 0.9878 -- iter: 20/55\n","Training Step: 8904  | total loss: \u001b[1m\u001b[32m0.24246\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 810 | loss: 0.24246 - acc: 0.9891 -- iter: 25/55\n","Training Step: 8905  | total loss: \u001b[1m\u001b[32m0.21872\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 810 | loss: 0.21872 - acc: 0.9902 -- iter: 30/55\n","Training Step: 8906  | total loss: \u001b[1m\u001b[32m0.17809\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 810 | loss: 0.17809 - acc: 0.9911 -- iter: 35/55\n","Training Step: 8907  | total loss: \u001b[1m\u001b[32m0.16119\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 810 | loss: 0.16119 - acc: 0.9920 -- iter: 40/55\n","Training Step: 8908  | total loss: \u001b[1m\u001b[32m0.16119\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 810 | loss: 0.16119 - acc: 0.9928 -- iter: 45/55\n","Training Step: 8909  | total loss: \u001b[1m\u001b[32m0.14587\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 810 | loss: 0.14587 - acc: 0.9935 -- iter: 50/55\n","Training Step: 8910  | total loss: \u001b[1m\u001b[32m0.11893\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 810 | loss: 0.11893 - acc: 0.9942 -- iter: 55/55\n","--\n","Training Step: 8911  | total loss: \u001b[1m\u001b[32m0.10749\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 811 | loss: 0.10749 - acc: 0.9948 -- iter: 05/55\n","Training Step: 8912  | total loss: \u001b[1m\u001b[32m0.09707\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 811 | loss: 0.09707 - acc: 0.9953 -- iter: 10/55\n","Training Step: 8913  | total loss: \u001b[1m\u001b[32m0.08791\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 811 | loss: 0.08791 - acc: 0.9953 -- iter: 15/55\n","Training Step: 8914  | total loss: \u001b[1m\u001b[32m0.08031\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 811 | loss: 0.08031 - acc: 0.9958 -- iter: 20/55\n","Training Step: 8915  | total loss: \u001b[1m\u001b[32m0.07267\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 811 | loss: 0.07267 - acc: 0.9962 -- iter: 25/55\n","Training Step: 8916  | total loss: \u001b[1m\u001b[32m0.06580\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 811 | loss: 0.06580 - acc: 0.9966 -- iter: 30/55\n","Training Step: 8917  | total loss: \u001b[1m\u001b[32m0.05942\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 811 | loss: 0.05942 - acc: 0.9969 -- iter: 35/55\n","Training Step: 8918  | total loss: \u001b[1m\u001b[32m0.05403\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 811 | loss: 0.05403 - acc: 0.9972 -- iter: 40/55\n","Training Step: 8919  | total loss: \u001b[1m\u001b[32m0.04928\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 811 | loss: 0.04928 - acc: 0.9975 -- iter: 45/55\n","Training Step: 8920  | total loss: \u001b[1m\u001b[32m0.04482\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 811 | loss: 0.04482 - acc: 0.9977 -- iter: 50/55\n","Training Step: 8921  | total loss: \u001b[1m\u001b[32m0.04077\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 811 | loss: 0.04077 - acc: 0.9980 -- iter: 55/55\n","--\n","Training Step: 8922  | total loss: \u001b[1m\u001b[32m0.03724\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 812 | loss: 0.03724 - acc: 0.9982 -- iter: 05/55\n","Training Step: 8923  | total loss: \u001b[1m\u001b[32m0.03381\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 812 | loss: 0.03381 - acc: 0.9984 -- iter: 10/55\n","Training Step: 8924  | total loss: \u001b[1m\u001b[32m0.03158\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 812 | loss: 0.03158 - acc: 0.9985 -- iter: 15/55\n","Training Step: 8925  | total loss: \u001b[1m\u001b[32m0.02875\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 812 | loss: 0.02875 - acc: 0.9988 -- iter: 20/55\n","Training Step: 8926  | total loss: \u001b[1m\u001b[32m0.02633\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 812 | loss: 0.02633 - acc: 0.9989 -- iter: 25/55\n","Training Step: 8927  | total loss: \u001b[1m\u001b[32m0.02394\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 812 | loss: 0.02394 - acc: 0.9990 -- iter: 30/55\n","Training Step: 8928  | total loss: \u001b[1m\u001b[32m0.02394\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 812 | loss: 0.02394 - acc: 0.9990 -- iter: 35/55\n","Training Step: 8929  | total loss: \u001b[1m\u001b[32m0.02179\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 812 | loss: 0.02179 - acc: 0.9992 -- iter: 40/55\n","Training Step: 8930  | total loss: \u001b[1m\u001b[32m0.01903\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 812 | loss: 0.01903 - acc: 0.9993 -- iter: 45/55\n","Training Step: 8931  | total loss: \u001b[1m\u001b[32m0.01750\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 812 | loss: 0.01750 - acc: 0.9994 -- iter: 50/55\n","Training Step: 8932  | total loss: \u001b[1m\u001b[32m0.01636\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 812 | loss: 0.01636 - acc: 0.9994 -- iter: 55/55\n","--\n","Training Step: 8933  | total loss: \u001b[1m\u001b[32m0.01564\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 813 | loss: 0.01564 - acc: 0.9995 -- iter: 05/55\n","Training Step: 8934  | total loss: \u001b[1m\u001b[32m0.01487\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 813 | loss: 0.01487 - acc: 0.9995 -- iter: 10/55\n","Training Step: 8935  | total loss: \u001b[1m\u001b[32m0.01384\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 813 | loss: 0.01384 - acc: 0.9995 -- iter: 15/55\n","Training Step: 8936  | total loss: \u001b[1m\u001b[32m0.01271\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 813 | loss: 0.01271 - acc: 0.9996 -- iter: 20/55\n","Training Step: 8937  | total loss: \u001b[1m\u001b[32m0.01164\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 813 | loss: 0.01164 - acc: 0.9996 -- iter: 25/55\n","Training Step: 8938  | total loss: \u001b[1m\u001b[32m0.01095\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 813 | loss: 0.01095 - acc: 0.9997 -- iter: 30/55\n","Training Step: 8939  | total loss: \u001b[1m\u001b[32m0.01024\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 813 | loss: 0.01024 - acc: 0.9997 -- iter: 35/55\n","Training Step: 8940  | total loss: \u001b[1m\u001b[32m0.00960\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 813 | loss: 0.00960 - acc: 0.9997 -- iter: 40/55\n","Training Step: 8941  | total loss: \u001b[1m\u001b[32m0.00900\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 813 | loss: 0.00900 - acc: 0.9998 -- iter: 45/55\n","Training Step: 8942  | total loss: \u001b[1m\u001b[32m0.00880\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 813 | loss: 0.00880 - acc: 0.9998 -- iter: 50/55\n","Training Step: 8943  | total loss: \u001b[1m\u001b[32m0.00838\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 813 | loss: 0.00838 - acc: 0.9998 -- iter: 55/55\n","--\n","Training Step: 8944  | total loss: \u001b[1m\u001b[32m0.00791\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 814 | loss: 0.00791 - acc: 0.9998 -- iter: 05/55\n","Training Step: 8945  | total loss: \u001b[1m\u001b[32m0.00732\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 814 | loss: 0.00732 - acc: 0.9999 -- iter: 10/55\n","Training Step: 8946  | total loss: \u001b[1m\u001b[32m0.00709\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 814 | loss: 0.00709 - acc: 0.9999 -- iter: 15/55\n","Training Step: 8947  | total loss: \u001b[1m\u001b[32m0.00658\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 814 | loss: 0.00658 - acc: 0.9999 -- iter: 20/55\n","Training Step: 8948  | total loss: \u001b[1m\u001b[32m0.00658\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 814 | loss: 0.00658 - acc: 0.9999 -- iter: 25/55\n","Training Step: 8949  | total loss: \u001b[1m\u001b[32m0.00652\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 814 | loss: 0.00652 - acc: 0.9999 -- iter: 30/55\n","Training Step: 8950  | total loss: \u001b[1m\u001b[32m0.00641\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 814 | loss: 0.00641 - acc: 0.9999 -- iter: 35/55\n","Training Step: 8951  | total loss: \u001b[1m\u001b[32m0.00597\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 814 | loss: 0.00597 - acc: 0.9999 -- iter: 40/55\n","Training Step: 8952  | total loss: \u001b[1m\u001b[32m0.00595\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 814 | loss: 0.00595 - acc: 0.9999 -- iter: 45/55\n","Training Step: 8953  | total loss: \u001b[1m\u001b[32m0.00593\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 814 | loss: 0.00593 - acc: 0.9999 -- iter: 50/55\n","Training Step: 8954  | total loss: \u001b[1m\u001b[32m0.00591\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 814 | loss: 0.00591 - acc: 0.9999 -- iter: 55/55\n","--\n","Training Step: 8955  | total loss: \u001b[1m\u001b[32m0.00645\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 815 | loss: 0.00645 - acc: 0.9999 -- iter: 05/55\n","Training Step: 8956  | total loss: \u001b[1m\u001b[32m0.00628\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 815 | loss: 0.00628 - acc: 0.9999 -- iter: 10/55\n","Training Step: 8957  | total loss: \u001b[1m\u001b[32m0.00599\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 815 | loss: 0.00599 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8958  | total loss: \u001b[1m\u001b[32m0.00571\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 815 | loss: 0.00571 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8959  | total loss: \u001b[1m\u001b[32m0.00575\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 815 | loss: 0.00575 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8960  | total loss: \u001b[1m\u001b[32m0.00585\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 815 | loss: 0.00585 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8961  | total loss: \u001b[1m\u001b[32m0.00587\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 815 | loss: 0.00587 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8962  | total loss: \u001b[1m\u001b[32m0.00570\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 815 | loss: 0.00570 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8963  | total loss: \u001b[1m\u001b[32m0.00542\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 815 | loss: 0.00542 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8964  | total loss: \u001b[1m\u001b[32m0.00523\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 815 | loss: 0.00523 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8965  | total loss: \u001b[1m\u001b[32m0.00558\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 815 | loss: 0.00558 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8966  | total loss: \u001b[1m\u001b[32m0.00517\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 816 | loss: 0.00517 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8967  | total loss: \u001b[1m\u001b[32m0.00509\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 816 | loss: 0.00509 - acc: 1.0000 -- iter: 10/55\n","Training Step: 8968  | total loss: \u001b[1m\u001b[32m0.00538\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 816 | loss: 0.00538 - acc: 1.0000 -- iter: 15/55\n","Training Step: 8969  | total loss: \u001b[1m\u001b[32m0.00531\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 816 | loss: 0.00531 - acc: 1.0000 -- iter: 20/55\n","Training Step: 8970  | total loss: \u001b[1m\u001b[32m0.00527\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 816 | loss: 0.00527 - acc: 1.0000 -- iter: 25/55\n","Training Step: 8971  | total loss: \u001b[1m\u001b[32m0.00535\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 816 | loss: 0.00535 - acc: 1.0000 -- iter: 30/55\n","Training Step: 8972  | total loss: \u001b[1m\u001b[32m0.00512\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 816 | loss: 0.00512 - acc: 1.0000 -- iter: 35/55\n","Training Step: 8973  | total loss: \u001b[1m\u001b[32m0.00513\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 816 | loss: 0.00513 - acc: 1.0000 -- iter: 40/55\n","Training Step: 8974  | total loss: \u001b[1m\u001b[32m0.00496\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 816 | loss: 0.00496 - acc: 1.0000 -- iter: 45/55\n","Training Step: 8975  | total loss: \u001b[1m\u001b[32m0.00506\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 816 | loss: 0.00506 - acc: 1.0000 -- iter: 50/55\n","Training Step: 8976  | total loss: \u001b[1m\u001b[32m0.00515\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 816 | loss: 0.00515 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 8977  | total loss: \u001b[1m\u001b[32m0.00509\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 817 | loss: 0.00509 - acc: 1.0000 -- iter: 05/55\n","Training Step: 8978  | total loss: \u001b[1m\u001b[32m0.00493\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 817 | loss: 0.00493 - acc: 1.0000 -- iter: 10/55\n","Training Step: 9283  | total loss: \u001b[1m\u001b[32m0.00278\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 844 | loss: 0.00278 - acc: 1.0000 -- iter: 50/55\n","Training Step: 9284  | total loss: \u001b[1m\u001b[32m0.00283\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 844 | loss: 0.00283 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 9285  | total loss: \u001b[1m\u001b[32m0.00269\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 845 | loss: 0.00269 - acc: 1.0000 -- iter: 05/55\n","Training Step: 9286  | total loss: \u001b[1m\u001b[32m0.00265\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 845 | loss: 0.00265 - acc: 1.0000 -- iter: 10/55\n","Training Step: 9287  | total loss: \u001b[1m\u001b[32m0.00278\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 845 | loss: 0.00278 - acc: 1.0000 -- iter: 15/55\n","Training Step: 9288  | total loss: \u001b[1m\u001b[32m0.00278\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 845 | loss: 0.00278 - acc: 1.0000 -- iter: 20/55\n","Training Step: 9289  | total loss: \u001b[1m\u001b[32m0.00293\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 845 | loss: 0.00293 - acc: 1.0000 -- iter: 25/55\n","Training Step: 9290  | total loss: \u001b[1m\u001b[32m0.00293\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 845 | loss: 0.00293 - acc: 1.0000 -- iter: 30/55\n","Training Step: 9291  | total loss: \u001b[1m\u001b[32m0.00317\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 845 | loss: 0.00317 - acc: 1.0000 -- iter: 35/55\n","Training Step: 9292  | total loss: \u001b[1m\u001b[32m0.00335\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 845 | loss: 0.00335 - acc: 1.0000 -- iter: 40/55\n","Training Step: 9293  | total loss: \u001b[1m\u001b[32m0.00313\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 845 | loss: 0.00313 - acc: 1.0000 -- iter: 45/55\n","Training Step: 9294  | total loss: \u001b[1m\u001b[32m0.00309\u001b[0m\u001b[0m | time: 0.060s\n","| Adam | epoch: 845 | loss: 0.00309 - acc: 1.0000 -- iter: 50/55\n","Training Step: 9295  | total loss: \u001b[1m\u001b[32m0.00302\u001b[0m\u001b[0m | time: 0.067s\n","| Adam | epoch: 845 | loss: 0.00302 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 9296  | total loss: \u001b[1m\u001b[32m0.00288\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 846 | loss: 0.00288 - acc: 1.0000 -- iter: 05/55\n","Training Step: 9297  | total loss: \u001b[1m\u001b[32m0.00293\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 846 | loss: 0.00293 - acc: 1.0000 -- iter: 10/55\n","Training Step: 9298  | total loss: \u001b[1m\u001b[32m0.00317\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 846 | loss: 0.00317 - acc: 1.0000 -- iter: 15/55\n","Training Step: 9299  | total loss: \u001b[1m\u001b[32m0.00337\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 846 | loss: 0.00337 - acc: 1.0000 -- iter: 20/55\n","Training Step: 9300  | total loss: \u001b[1m\u001b[32m0.00328\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 846 | loss: 0.00328 - acc: 1.0000 -- iter: 25/55\n","Training Step: 9301  | total loss: \u001b[1m\u001b[32m0.00321\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 846 | loss: 0.00321 - acc: 1.0000 -- iter: 30/55\n","Training Step: 9302  | total loss: \u001b[1m\u001b[32m0.00321\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 846 | loss: 0.00321 - acc: 1.0000 -- iter: 35/55\n","Training Step: 9303  | total loss: \u001b[1m\u001b[32m0.00317\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 846 | loss: 0.00317 - acc: 1.0000 -- iter: 40/55\n","Training Step: 9304  | total loss: \u001b[1m\u001b[32m0.00310\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 846 | loss: 0.00310 - acc: 1.0000 -- iter: 45/55\n","Training Step: 9305  | total loss: \u001b[1m\u001b[32m0.00302\u001b[0m\u001b[0m | time: 0.068s\n","| Adam | epoch: 846 | loss: 0.00302 - acc: 1.0000 -- iter: 50/55\n","Training Step: 9306  | total loss: \u001b[1m\u001b[32m0.00300\u001b[0m\u001b[0m | time: 0.073s\n","| Adam | epoch: 846 | loss: 0.00300 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 9307  | total loss: \u001b[1m\u001b[32m0.00300\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 847 | loss: 0.00300 - acc: 1.0000 -- iter: 05/55\n","Training Step: 9308  | total loss: \u001b[1m\u001b[32m0.00301\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 847 | loss: 0.00301 - acc: 1.0000 -- iter: 10/55\n","Training Step: 9309  | total loss: \u001b[1m\u001b[32m0.00288\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 847 | loss: 0.00288 - acc: 1.0000 -- iter: 15/55\n","Training Step: 9310  | total loss: \u001b[1m\u001b[32m0.00300\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 847 | loss: 0.00300 - acc: 1.0000 -- iter: 20/55\n","Training Step: 9311  | total loss: \u001b[1m\u001b[32m0.00300\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 847 | loss: 0.00300 - acc: 1.0000 -- iter: 25/55\n","Training Step: 9312  | total loss: \u001b[1m\u001b[32m0.00313\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 847 | loss: 0.00313 - acc: 1.0000 -- iter: 30/55\n","Training Step: 9313  | total loss: \u001b[1m\u001b[32m0.00324\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 847 | loss: 0.00324 - acc: 1.0000 -- iter: 35/55\n","Training Step: 9314  | total loss: \u001b[1m\u001b[32m0.00324\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 847 | loss: 0.00324 - acc: 1.0000 -- iter: 40/55\n","Training Step: 9315  | total loss: \u001b[1m\u001b[32m0.00311\u001b[0m\u001b[0m | time: 0.062s\n","| Adam | epoch: 847 | loss: 0.00311 - acc: 1.0000 -- iter: 45/55\n","Training Step: 9316  | total loss: \u001b[1m\u001b[32m0.00309\u001b[0m\u001b[0m | time: 0.068s\n","| Adam | epoch: 847 | loss: 0.00309 - acc: 1.0000 -- iter: 50/55\n","Training Step: 9317  | total loss: \u001b[1m\u001b[32m0.00300\u001b[0m\u001b[0m | time: 0.073s\n","| Adam | epoch: 847 | loss: 0.00300 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 9318  | total loss: \u001b[1m\u001b[32m0.00311\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 848 | loss: 0.00311 - acc: 1.0000 -- iter: 05/55\n","Training Step: 9319  | total loss: \u001b[1m\u001b[32m0.00310\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 848 | loss: 0.00310 - acc: 1.0000 -- iter: 10/55\n","Training Step: 9320  | total loss: \u001b[1m\u001b[32m0.00327\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 848 | loss: 0.00327 - acc: 1.0000 -- iter: 15/55\n","Training Step: 9321  | total loss: \u001b[1m\u001b[32m0.00319\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 848 | loss: 0.00319 - acc: 1.0000 -- iter: 20/55\n","Training Step: 9322  | total loss: \u001b[1m\u001b[32m0.00311\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 848 | loss: 0.00311 - acc: 1.0000 -- iter: 25/55\n","Training Step: 9323  | total loss: \u001b[1m\u001b[32m0.00320\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 848 | loss: 0.00320 - acc: 1.0000 -- iter: 30/55\n","Training Step: 9324  | total loss: \u001b[1m\u001b[32m0.00304\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 848 | loss: 0.00304 - acc: 1.0000 -- iter: 35/55\n","Training Step: 9325  | total loss: \u001b[1m\u001b[32m0.00290\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 848 | loss: 0.00290 - acc: 1.0000 -- iter: 40/55\n","Training Step: 9326  | total loss: \u001b[1m\u001b[32m0.00284\u001b[0m\u001b[0m | time: 0.067s\n","| Adam | epoch: 848 | loss: 0.00284 - acc: 1.0000 -- iter: 45/55\n","Training Step: 9327  | total loss: \u001b[1m\u001b[32m0.00293\u001b[0m\u001b[0m | time: 0.075s\n","| Adam | epoch: 848 | loss: 0.00293 - acc: 1.0000 -- iter: 50/55\n","Training Step: 9328  | total loss: \u001b[1m\u001b[32m0.00277\u001b[0m\u001b[0m | time: 0.083s\n","| Adam | epoch: 848 | loss: 0.00277 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 9329  | total loss: \u001b[1m\u001b[32m0.00283\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 849 | loss: 0.00283 - acc: 1.0000 -- iter: 05/55\n","Training Step: 9330  | total loss: \u001b[1m\u001b[32m0.00283\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 849 | loss: 0.00283 - acc: 1.0000 -- iter: 10/55\n","Training Step: 9331  | total loss: \u001b[1m\u001b[32m0.00281\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 849 | loss: 0.00281 - acc: 1.0000 -- iter: 15/55\n","Training Step: 9332  | total loss: \u001b[1m\u001b[32m0.00272\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 849 | loss: 0.00272 - acc: 1.0000 -- iter: 20/55\n","Training Step: 9333  | total loss: \u001b[1m\u001b[32m0.00277\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 849 | loss: 0.00277 - acc: 1.0000 -- iter: 25/55\n","Training Step: 9334  | total loss: \u001b[1m\u001b[32m0.00286\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 849 | loss: 0.00286 - acc: 1.0000 -- iter: 30/55\n","Training Step: 9335  | total loss: \u001b[1m\u001b[32m0.00285\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 849 | loss: 0.00285 - acc: 1.0000 -- iter: 35/55\n","Training Step: 9336  | total loss: \u001b[1m\u001b[32m0.00285\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 849 | loss: 0.00285 - acc: 1.0000 -- iter: 40/55\n","Training Step: 9337  | total loss: \u001b[1m\u001b[32m0.00280\u001b[0m\u001b[0m | time: 0.062s\n","| Adam | epoch: 849 | loss: 0.00280 - acc: 1.0000 -- iter: 45/55\n","Training Step: 9338  | total loss: \u001b[1m\u001b[32m0.00280\u001b[0m\u001b[0m | time: 0.069s\n","| Adam | epoch: 849 | loss: 0.00280 - acc: 1.0000 -- iter: 50/55\n","Training Step: 9339  | total loss: \u001b[1m\u001b[32m0.00269\u001b[0m\u001b[0m | time: 0.075s\n","| Adam | epoch: 849 | loss: 0.00269 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 9340  | total loss: \u001b[1m\u001b[32m0.00269\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 850 | loss: 0.00269 - acc: 1.0000 -- iter: 05/55\n","Training Step: 9341  | total loss: \u001b[1m\u001b[32m0.00282\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 850 | loss: 0.00282 - acc: 1.0000 -- iter: 10/55\n","Training Step: 9342  | total loss: \u001b[1m\u001b[32m0.00302\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 850 | loss: 0.00302 - acc: 1.0000 -- iter: 15/55\n","Training Step: 9343  | total loss: \u001b[1m\u001b[32m0.00301\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 850 | loss: 0.00301 - acc: 1.0000 -- iter: 20/55\n","Training Step: 9344  | total loss: \u001b[1m\u001b[32m0.00283\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 850 | loss: 0.00283 - acc: 1.0000 -- iter: 25/55\n","Training Step: 9345  | total loss: \u001b[1m\u001b[32m0.00292\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 850 | loss: 0.00292 - acc: 1.0000 -- iter: 30/55\n","Training Step: 9346  | total loss: \u001b[1m\u001b[32m0.00284\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 850 | loss: 0.00284 - acc: 1.0000 -- iter: 35/55\n","Training Step: 9347  | total loss: \u001b[1m\u001b[32m0.00281\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 850 | loss: 0.00281 - acc: 1.0000 -- iter: 40/55\n","Training Step: 9348  | total loss: \u001b[1m\u001b[32m0.00289\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 850 | loss: 0.00289 - acc: 1.0000 -- iter: 45/55\n","Training Step: 9349  | total loss: \u001b[1m\u001b[32m0.00296\u001b[0m\u001b[0m | time: 0.063s\n","| Adam | epoch: 850 | loss: 0.00296 - acc: 1.0000 -- iter: 50/55\n","Training Step: 9350  | total loss: \u001b[1m\u001b[32m0.00295\u001b[0m\u001b[0m | time: 0.069s\n","| Adam | epoch: 850 | loss: 0.00295 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 9351  | total loss: \u001b[1m\u001b[32m0.00279\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 851 | loss: 0.00279 - acc: 1.0000 -- iter: 05/55\n","Training Step: 9352  | total loss: \u001b[1m\u001b[32m2.30510\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 851 | loss: 2.30510 - acc: 0.9000 -- iter: 10/55\n","Training Step: 9353  | total loss: \u001b[1m\u001b[32m2.07482\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 851 | loss: 2.07482 - acc: 0.9100 -- iter: 15/55\n","Training Step: 9354  | total loss: \u001b[1m\u001b[32m1.86752\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 851 | loss: 1.86752 - acc: 0.9190 -- iter: 20/55\n","Training Step: 9355  | total loss: \u001b[1m\u001b[32m1.68119\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 851 | loss: 1.68119 - acc: 0.9271 -- iter: 25/55\n","Training Step: 9356  | total loss: \u001b[1m\u001b[32m1.51356\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 851 | loss: 1.51356 - acc: 0.9344 -- iter: 30/55\n","Training Step: 9357  | total loss: \u001b[1m\u001b[32m1.36251\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 851 | loss: 1.36251 - acc: 0.9410 -- iter: 35/55\n","Training Step: 9358  | total loss: \u001b[1m\u001b[32m1.22660\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 851 | loss: 1.22660 - acc: 0.9469 -- iter: 40/55\n","Training Step: 9359  | total loss: \u001b[1m\u001b[32m1.10409\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 851 | loss: 1.10409 - acc: 0.9522 -- iter: 45/55\n","Training Step: 9360  | total loss: \u001b[1m\u001b[32m0.89484\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 851 | loss: 0.89484 - acc: 0.9570 -- iter: 50/55\n","Training Step: 9361  | total loss: \u001b[1m\u001b[32m0.89484\u001b[0m\u001b[0m | time: 0.068s\n","| Adam | epoch: 851 | loss: 0.89484 - acc: 0.9613 -- iter: 55/55\n","--\n","Training Step: 9362  | total loss: \u001b[1m\u001b[32m0.72520\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 852 | loss: 0.72520 - acc: 0.9651 -- iter: 05/55\n","Training Step: 9363  | total loss: \u001b[1m\u001b[32m0.72520\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 852 | loss: 0.72520 - acc: 0.9686 -- iter: 10/55\n","Training Step: 9364  | total loss: \u001b[1m\u001b[32m2.25407\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 852 | loss: 2.25407 - acc: 0.8918 -- iter: 15/55\n","Training Step: 9365  | total loss: \u001b[1m\u001b[32m2.02913\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 852 | loss: 2.02913 - acc: 0.9026 -- iter: 20/55\n","Training Step: 9366  | total loss: \u001b[1m\u001b[32m1.82649\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 852 | loss: 1.82649 - acc: 0.9123 -- iter: 25/55\n","Training Step: 9367  | total loss: \u001b[1m\u001b[32m1.64437\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 852 | loss: 1.64437 - acc: 0.9211 -- iter: 30/55\n","Training Step: 9368  | total loss: \u001b[1m\u001b[32m1.33243\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 852 | loss: 1.33243 - acc: 0.9290 -- iter: 35/55\n","Training Step: 9369  | total loss: \u001b[1m\u001b[32m1.33243\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 852 | loss: 1.33243 - acc: 0.9361 -- iter: 40/55\n","Training Step: 9370  | total loss: \u001b[1m\u001b[32m1.07968\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 852 | loss: 1.07968 - acc: 0.9425 -- iter: 45/55\n","Training Step: 9371  | total loss: \u001b[1m\u001b[32m1.07968\u001b[0m\u001b[0m | time: 0.068s\n","| Adam | epoch: 852 | loss: 1.07968 - acc: 0.9482 -- iter: 50/55\n","Training Step: 9372  | total loss: \u001b[1m\u001b[32m0.97199\u001b[0m\u001b[0m | time: 0.076s\n","| Adam | epoch: 852 | loss: 0.97199 - acc: 0.9534 -- iter: 55/55\n","--\n","Training Step: 9373  | total loss: \u001b[1m\u001b[32m0.87506\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 853 | loss: 0.87506 - acc: 0.9581 -- iter: 05/55\n","Training Step: 9374  | total loss: \u001b[1m\u001b[32m0.78768\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 853 | loss: 0.78768 - acc: 0.9660 -- iter: 10/55\n","Training Step: 9375  | total loss: \u001b[1m\u001b[32m0.70941\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 853 | loss: 0.70941 - acc: 0.9660 -- iter: 15/55\n","Training Step: 9376  | total loss: \u001b[1m\u001b[32m2.32038\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 853 | loss: 2.32038 - acc: 0.8894 -- iter: 20/55\n","Training Step: 9377  | total loss: \u001b[1m\u001b[32m2.08867\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 853 | loss: 2.08867 - acc: 0.9005 -- iter: 25/55\n","Training Step: 9378  | total loss: \u001b[1m\u001b[32m1.87994\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 853 | loss: 1.87994 - acc: 0.9104 -- iter: 30/55\n","Training Step: 9379  | total loss: \u001b[1m\u001b[32m1.69253\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 853 | loss: 1.69253 - acc: 0.9194 -- iter: 35/55\n","Training Step: 9380  | total loss: \u001b[1m\u001b[32m1.52354\u001b[0m\u001b[0m | time: 0.062s\n","| Adam | epoch: 853 | loss: 1.52354 - acc: 0.9275 -- iter: 40/55\n","Training Step: 9381  | total loss: \u001b[1m\u001b[32m1.37160\u001b[0m\u001b[0m | time: 0.068s\n","| Adam | epoch: 853 | loss: 1.37160 - acc: 0.9347 -- iter: 45/55\n","Training Step: 9382  | total loss: \u001b[1m\u001b[32m1.23457\u001b[0m\u001b[0m | time: 0.074s\n","| Adam | epoch: 853 | loss: 1.23457 - acc: 0.9412 -- iter: 50/55\n","Training Step: 9383  | total loss: \u001b[1m\u001b[32m1.11145\u001b[0m\u001b[0m | time: 0.082s\n","| Adam | epoch: 853 | loss: 1.11145 - acc: 0.9471 -- iter: 55/55\n","--\n","Training Step: 9384  | total loss: \u001b[1m\u001b[32m1.00060\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 854 | loss: 1.00060 - acc: 0.9524 -- iter: 05/55\n","Training Step: 9385  | total loss: \u001b[1m\u001b[32m0.90083\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 854 | loss: 0.90083 - acc: 0.9572 -- iter: 10/55\n","Training Step: 9386  | total loss: \u001b[1m\u001b[32m0.81105\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 854 | loss: 0.81105 - acc: 0.9614 -- iter: 15/55\n","Training Step: 9387  | total loss: \u001b[1m\u001b[32m0.73031\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 854 | loss: 0.73031 - acc: 0.9653 -- iter: 20/55\n","Training Step: 9388  | total loss: \u001b[1m\u001b[32m2.13785\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 854 | loss: 2.13785 - acc: 0.8688 -- iter: 25/55\n","Training Step: 9389  | total loss: \u001b[1m\u001b[32m1.92447\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 854 | loss: 1.92447 - acc: 0.8819 -- iter: 30/55\n","Training Step: 9390  | total loss: \u001b[1m\u001b[32m1.73231\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 854 | loss: 1.73231 - acc: 0.8937 -- iter: 35/55\n","Training Step: 9391  | total loss: \u001b[1m\u001b[32m1.55932\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 854 | loss: 1.55932 - acc: 0.9043 -- iter: 40/55\n","Training Step: 9392  | total loss: \u001b[1m\u001b[32m1.40383\u001b[0m\u001b[0m | time: 0.066s\n","| Adam | epoch: 854 | loss: 1.40383 - acc: 0.9139 -- iter: 45/55\n","Training Step: 9393  | total loss: \u001b[1m\u001b[32m1.26373\u001b[0m\u001b[0m | time: 0.073s\n","| Adam | epoch: 854 | loss: 1.26373 - acc: 0.9225 -- iter: 50/55\n","Training Step: 9394  | total loss: \u001b[1m\u001b[32m1.13747\u001b[0m\u001b[0m | time: 0.081s\n","| Adam | epoch: 854 | loss: 1.13747 - acc: 0.9303 -- iter: 55/55\n","--\n","Training Step: 9395  | total loss: \u001b[1m\u001b[32m1.02407\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 855 | loss: 1.02407 - acc: 0.9372 -- iter: 05/55\n","Training Step: 9396  | total loss: \u001b[1m\u001b[32m0.92212\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 855 | loss: 0.92212 - acc: 0.9435 -- iter: 10/55\n","Training Step: 9397  | total loss: \u001b[1m\u001b[32m0.74787\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 855 | loss: 0.74787 - acc: 0.9492 -- iter: 15/55\n","Training Step: 9398  | total loss: \u001b[1m\u001b[32m0.74787\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 855 | loss: 0.74787 - acc: 0.9542 -- iter: 20/55\n","Training Step: 9399  | total loss: \u001b[1m\u001b[32m0.67355\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 855 | loss: 0.67355 - acc: 0.9588 -- iter: 25/55\n","Training Step: 9400  | total loss: \u001b[1m\u001b[32m2.61048\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 855 | loss: 2.61048 - acc: 0.8629 -- iter: 30/55\n","Training Step: 9401  | total loss: \u001b[1m\u001b[32m2.34979\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 855 | loss: 2.34979 - acc: 0.8766 -- iter: 35/55\n","Training Step: 9402  | total loss: \u001b[1m\u001b[32m2.11541\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 855 | loss: 2.11541 - acc: 0.8890 -- iter: 40/55\n","Training Step: 9403  | total loss: \u001b[1m\u001b[32m1.90416\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 855 | loss: 1.90416 - acc: 0.9001 -- iter: 45/55\n","Training Step: 9404  | total loss: \u001b[1m\u001b[32m1.71417\u001b[0m\u001b[0m | time: 0.065s\n","| Adam | epoch: 855 | loss: 1.71417 - acc: 0.9191 -- iter: 50/55\n","Training Step: 9405  | total loss: \u001b[1m\u001b[32m1.54307\u001b[0m\u001b[0m | time: 0.072s\n","| Adam | epoch: 855 | loss: 1.54307 - acc: 0.9272 -- iter: 55/55\n","--\n","Training Step: 9406  | total loss: \u001b[1m\u001b[32m1.38940\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 856 | loss: 1.38940 - acc: 0.9344 -- iter: 05/55\n","Training Step: 9407  | total loss: \u001b[1m\u001b[32m1.25109\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 856 | loss: 1.25109 - acc: 0.9410 -- iter: 10/55\n","Training Step: 9408  | total loss: \u001b[1m\u001b[32m1.12613\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 856 | loss: 1.12613 - acc: 0.9469 -- iter: 15/55\n","Training Step: 9409  | total loss: \u001b[1m\u001b[32m1.01367\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 856 | loss: 1.01367 - acc: 0.9522 -- iter: 20/55\n","Training Step: 9410  | total loss: \u001b[1m\u001b[32m0.91272\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 856 | loss: 0.91272 - acc: 0.9570 -- iter: 25/55\n","Training Step: 9411  | total loss: \u001b[1m\u001b[32m0.82183\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 856 | loss: 0.82183 - acc: 0.9570 -- iter: 30/55\n","Training Step: 9412  | total loss: \u001b[1m\u001b[32m2.73083\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 856 | loss: 2.73083 - acc: 0.8752 -- iter: 35/55\n","Training Step: 9413  | total loss: \u001b[1m\u001b[32m2.21310\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 856 | loss: 2.21310 - acc: 0.8752 -- iter: 40/55\n","Training Step: 9414  | total loss: \u001b[1m\u001b[32m2.21310\u001b[0m\u001b[0m | time: 0.064s\n","| Adam | epoch: 856 | loss: 2.21310 - acc: 0.8989 -- iter: 45/55\n","Training Step: 9415  | total loss: \u001b[1m\u001b[32m1.99216\u001b[0m\u001b[0m | time: 0.072s\n","| Adam | epoch: 856 | loss: 1.99216 - acc: 0.8989 -- iter: 50/55\n","Training Step: 9416  | total loss: \u001b[1m\u001b[32m1.79326\u001b[0m\u001b[0m | time: 0.079s\n","| Adam | epoch: 856 | loss: 1.79326 - acc: 0.9090 -- iter: 55/55\n","--\n","Training Step: 9417  | total loss: \u001b[1m\u001b[32m1.61421\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 857 | loss: 1.61421 - acc: 0.9181 -- iter: 05/55\n","Training Step: 9418  | total loss: \u001b[1m\u001b[32m1.45334\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 857 | loss: 1.45334 - acc: 0.9263 -- iter: 10/55\n","Training Step: 9419  | total loss: \u001b[1m\u001b[32m1.30829\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 857 | loss: 1.30829 - acc: 0.9337 -- iter: 15/55\n","Training Step: 9420  | total loss: \u001b[1m\u001b[32m1.17811\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 857 | loss: 1.17811 - acc: 0.9403 -- iter: 20/55\n","Training Step: 9421  | total loss: \u001b[1m\u001b[32m1.06095\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 857 | loss: 1.06095 - acc: 0.9463 -- iter: 25/55\n","Training Step: 9422  | total loss: \u001b[1m\u001b[32m0.95502\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 857 | loss: 0.95502 - acc: 0.9565 -- iter: 30/55\n","Training Step: 9423  | total loss: \u001b[1m\u001b[32m0.85992\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 857 | loss: 0.85992 - acc: 0.9608 -- iter: 35/55\n","Training Step: 9424  | total loss: \u001b[1m\u001b[32m0.77429\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 857 | loss: 0.77429 - acc: 0.9647 -- iter: 40/55\n","Training Step: 9425  | total loss: \u001b[1m\u001b[32m0.69760\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 857 | loss: 0.69760 - acc: 0.9683 -- iter: 45/55\n","Training Step: 9426  | total loss: \u001b[1m\u001b[32m0.62813\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 857 | loss: 0.62813 - acc: 0.9714 -- iter: 50/55\n","Training Step: 9427  | total loss: \u001b[1m\u001b[32m0.56541\u001b[0m\u001b[0m | time: 0.062s\n","| Adam | epoch: 857 | loss: 0.56541 - acc: 0.9743 -- iter: 55/55\n","--\n","Training Step: 9428  | total loss: \u001b[1m\u001b[32m0.50919\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 858 | loss: 0.50919 - acc: 0.9769 -- iter: 05/55\n","Training Step: 9429  | total loss: \u001b[1m\u001b[32m0.41355\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 858 | loss: 0.41355 - acc: 0.9792 -- iter: 10/55\n","Training Step: 9430  | total loss: \u001b[1m\u001b[32m0.41355\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 858 | loss: 0.41355 - acc: 0.9813 -- iter: 15/55\n","Training Step: 9431  | total loss: \u001b[1m\u001b[32m0.37261\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 858 | loss: 0.37261 - acc: 0.9831 -- iter: 20/55\n","Training Step: 9432  | total loss: \u001b[1m\u001b[32m0.33567\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 858 | loss: 0.33567 - acc: 0.9848 -- iter: 25/55\n","Training Step: 9433  | total loss: \u001b[1m\u001b[32m0.30242\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 858 | loss: 0.30242 - acc: 0.9848 -- iter: 30/55\n","Training Step: 9434  | total loss: \u001b[1m\u001b[32m0.24588\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 858 | loss: 0.24588 - acc: 0.9863 -- iter: 35/55\n","Training Step: 9435  | total loss: \u001b[1m\u001b[32m2.00351\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 858 | loss: 2.00351 - acc: 0.9877 -- iter: 40/55\n","Training Step: 9436  | total loss: \u001b[1m\u001b[32m2.00351\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 858 | loss: 2.00351 - acc: 0.8889 -- iter: 45/55\n","Training Step: 9437  | total loss: \u001b[1m\u001b[32m1.80366\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 858 | loss: 1.80366 - acc: 0.9000 -- iter: 50/55\n","Training Step: 9438  | total loss: \u001b[1m\u001b[32m1.62392\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 858 | loss: 1.62392 - acc: 0.9100 -- iter: 55/55\n","--\n","Training Step: 9439  | total loss: \u001b[1m\u001b[32m1.46175\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 859 | loss: 1.46175 - acc: 0.9190 -- iter: 05/55\n","Training Step: 9440  | total loss: \u001b[1m\u001b[32m1.31599\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 859 | loss: 1.31599 - acc: 0.9271 -- iter: 10/55\n","Training Step: 9441  | total loss: \u001b[1m\u001b[32m1.06743\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 859 | loss: 1.06743 - acc: 0.9344 -- iter: 15/55\n","Training Step: 9442  | total loss: \u001b[1m\u001b[32m0.96093\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 859 | loss: 0.96093 - acc: 0.9410 -- iter: 20/55\n","Training Step: 9443  | total loss: \u001b[1m\u001b[32m0.86531\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 859 | loss: 0.86531 - acc: 0.9469 -- iter: 25/55\n","Training Step: 9444  | total loss: \u001b[1m\u001b[32m0.77925\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 859 | loss: 0.77925 - acc: 0.9570 -- iter: 30/55\n","Training Step: 9445  | total loss: \u001b[1m\u001b[32m0.70191\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 859 | loss: 0.70191 - acc: 0.9613 -- iter: 35/55\n","Training Step: 9446  | total loss: \u001b[1m\u001b[32m0.63204\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 859 | loss: 0.63204 - acc: 0.9651 -- iter: 40/55\n","Training Step: 9447  | total loss: \u001b[1m\u001b[32m0.63204\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 859 | loss: 0.63204 - acc: 0.8686 -- iter: 45/55\n","Training Step: 9448  | total loss: \u001b[1m\u001b[32m2.57261\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 859 | loss: 2.57261 - acc: 0.8818 -- iter: 50/55\n","Training Step: 9449  | total loss: \u001b[1m\u001b[32m2.08484\u001b[0m\u001b[0m | time: 0.063s\n","| Adam | epoch: 859 | loss: 2.08484 - acc: 0.8936 -- iter: 55/55\n","--\n","Training Step: 9450  | total loss: \u001b[1m\u001b[32m2.08484\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 860 | loss: 2.08484 - acc: 0.8936 -- iter: 05/55\n","Training Step: 9451  | total loss: \u001b[1m\u001b[32m1.87668\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 860 | loss: 1.87668 - acc: 0.9042 -- iter: 10/55\n","Training Step: 9452  | total loss: \u001b[1m\u001b[32m1.52073\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 860 | loss: 1.52073 - acc: 0.9138 -- iter: 15/55\n","Training Step: 9453  | total loss: \u001b[1m\u001b[32m1.36908\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 860 | loss: 1.36908 - acc: 0.9302 -- iter: 20/55\n","Training Step: 9454  | total loss: \u001b[1m\u001b[32m1.36908\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 860 | loss: 1.36908 - acc: 0.9302 -- iter: 25/55\n","Training Step: 9455  | total loss: \u001b[1m\u001b[32m1.23257\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 860 | loss: 1.23257 - acc: 0.9372 -- iter: 30/55\n","Training Step: 9456  | total loss: \u001b[1m\u001b[32m1.10956\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 860 | loss: 1.10956 - acc: 0.9435 -- iter: 35/55\n","Training Step: 9457  | total loss: \u001b[1m\u001b[32m0.99885\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 860 | loss: 0.99885 - acc: 0.9491 -- iter: 40/55\n","Training Step: 9458  | total loss: \u001b[1m\u001b[32m0.89963\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 860 | loss: 0.89963 - acc: 0.9588 -- iter: 45/55\n","Training Step: 9459  | total loss: \u001b[1m\u001b[32m0.81040\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 860 | loss: 0.81040 - acc: 0.9629 -- iter: 50/55\n","Training Step: 9460  | total loss: \u001b[1m\u001b[32m0.72986\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 860 | loss: 0.72986 - acc: 0.9666 -- iter: 55/55\n","--\n","Training Step: 9461  | total loss: \u001b[1m\u001b[32m0.65711\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 861 | loss: 0.65711 - acc: 0.9666 -- iter: 05/55\n","Training Step: 9462  | total loss: \u001b[1m\u001b[32m0.59151\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 861 | loss: 0.59151 - acc: 0.9699 -- iter: 10/55\n","Training Step: 9463  | total loss: \u001b[1m\u001b[32m0.53307\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 861 | loss: 0.53307 - acc: 0.9730 -- iter: 15/55\n","Training Step: 9464  | total loss: \u001b[1m\u001b[32m0.48029\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 861 | loss: 0.48029 - acc: 0.9757 -- iter: 20/55\n","Training Step: 9465  | total loss: \u001b[1m\u001b[32m0.43284\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 861 | loss: 0.43284 - acc: 0.9781 -- iter: 25/55\n","Training Step: 9466  | total loss: \u001b[1m\u001b[32m0.38965\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 861 | loss: 0.38965 - acc: 0.9803 -- iter: 30/55\n","Training Step: 9467  | total loss: \u001b[1m\u001b[32m0.35129\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 861 | loss: 0.35129 - acc: 0.9823 -- iter: 35/55\n","Training Step: 9468  | total loss: \u001b[1m\u001b[32m0.31683\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 861 | loss: 0.31683 - acc: 0.9840 -- iter: 40/55\n","Training Step: 9469  | total loss: \u001b[1m\u001b[32m0.28582\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 861 | loss: 0.28582 - acc: 0.9856 -- iter: 45/55\n","Training Step: 9470  | total loss: \u001b[1m\u001b[32m0.25794\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 861 | loss: 0.25794 - acc: 0.9871 -- iter: 50/55\n","Training Step: 9471  | total loss: \u001b[1m\u001b[32m0.23252\u001b[0m\u001b[0m | time: 0.060s\n","| Adam | epoch: 861 | loss: 0.23252 - acc: 0.9884 -- iter: 55/55\n","--\n","Training Step: 9472  | total loss: \u001b[1m\u001b[32m0.20980\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 862 | loss: 0.20980 - acc: 0.9895 -- iter: 05/55\n","Training Step: 9473  | total loss: \u001b[1m\u001b[32m0.18904\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 862 | loss: 0.18904 - acc: 0.9906 -- iter: 10/55\n","Training Step: 9474  | total loss: \u001b[1m\u001b[32m0.17032\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 862 | loss: 0.17032 - acc: 0.9915 -- iter: 15/55\n","Training Step: 9475  | total loss: \u001b[1m\u001b[32m0.15363\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 862 | loss: 0.15363 - acc: 0.9931 -- iter: 20/55\n","Training Step: 9476  | total loss: \u001b[1m\u001b[32m0.13907\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 862 | loss: 0.13907 - acc: 0.9938 -- iter: 25/55\n","Training Step: 9477  | total loss: \u001b[1m\u001b[32m0.12548\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 862 | loss: 0.12548 - acc: 0.9944 -- iter: 30/55\n","Training Step: 9478  | total loss: \u001b[1m\u001b[32m0.11377\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 862 | loss: 0.11377 - acc: 0.9950 -- iter: 35/55\n","Training Step: 9479  | total loss: \u001b[1m\u001b[32m0.10312\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 862 | loss: 0.10312 - acc: 0.9950 -- iter: 40/55\n","Training Step: 9480  | total loss: \u001b[1m\u001b[32m0.08423\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 862 | loss: 0.08423 - acc: 0.9955 -- iter: 45/55\n","Training Step: 9481  | total loss: \u001b[1m\u001b[32m0.08423\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 862 | loss: 0.08423 - acc: 0.9959 -- iter: 50/55\n","Training Step: 9482  | total loss: \u001b[1m\u001b[32m0.06896\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 862 | loss: 0.06896 - acc: 0.9963 -- iter: 55/55\n","--\n","Training Step: 9483  | total loss: \u001b[1m\u001b[32m1.35130\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 863 | loss: 1.35130 - acc: 0.9967 -- iter: 05/55\n","Training Step: 9484  | total loss: \u001b[1m\u001b[32m1.21675\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 863 | loss: 1.21675 - acc: 0.9170 -- iter: 10/55\n","Training Step: 9485  | total loss: \u001b[1m\u001b[32m1.09590\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 863 | loss: 1.09590 - acc: 0.9253 -- iter: 15/55\n","Training Step: 9486  | total loss: \u001b[1m\u001b[32m0.98678\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 863 | loss: 0.98678 - acc: 0.9328 -- iter: 20/55\n","Training Step: 9487  | total loss: \u001b[1m\u001b[32m0.88876\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 863 | loss: 0.88876 - acc: 0.9395 -- iter: 25/55\n","Training Step: 9488  | total loss: \u001b[1m\u001b[32m0.88876\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 863 | loss: 0.88876 - acc: 0.9510 -- iter: 30/55\n","Training Step: 9489  | total loss: \u001b[1m\u001b[32m0.80001\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 863 | loss: 0.80001 - acc: 0.9510 -- iter: 35/55\n","Training Step: 9490  | total loss: \u001b[1m\u001b[32m0.72070\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 863 | loss: 0.72070 - acc: 0.9559 -- iter: 40/55\n","Training Step: 9491  | total loss: \u001b[1m\u001b[32m0.58435\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 863 | loss: 0.58435 - acc: 0.9603 -- iter: 45/55\n","Training Step: 9492  | total loss: \u001b[1m\u001b[32m0.52620\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 863 | loss: 0.52620 - acc: 0.9643 -- iter: 50/55\n","Training Step: 9493  | total loss: \u001b[1m\u001b[32m0.47414\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 863 | loss: 0.47414 - acc: 0.9679 -- iter: 55/55\n","--\n","Training Step: 9494  | total loss: \u001b[1m\u001b[32m0.47414\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 864 | loss: 0.47414 - acc: 0.9711 -- iter: 05/55\n","Training Step: 9495  | total loss: \u001b[1m\u001b[32m0.42735\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 864 | loss: 0.42735 - acc: 0.9740 -- iter: 10/55\n","Training Step: 9496  | total loss: \u001b[1m\u001b[32m0.38512\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 864 | loss: 0.38512 - acc: 0.9766 -- iter: 15/55\n","Training Step: 9497  | total loss: \u001b[1m\u001b[32m0.31254\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 864 | loss: 0.31254 - acc: 0.9789 -- iter: 20/55\n","Training Step: 9498  | total loss: \u001b[1m\u001b[32m0.28229\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 864 | loss: 0.28229 - acc: 0.9829 -- iter: 25/55\n","Training Step: 9499  | total loss: \u001b[1m\u001b[32m0.25431\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 864 | loss: 0.25431 - acc: 0.9846 -- iter: 30/55\n","Training Step: 9500  | total loss: \u001b[1m\u001b[32m0.22928\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 864 | loss: 0.22928 - acc: 0.9862 -- iter: 35/55\n","Training Step: 9501  | total loss: \u001b[1m\u001b[32m0.20689\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 864 | loss: 0.20689 - acc: 0.9875 -- iter: 40/55\n","Training Step: 9502  | total loss: \u001b[1m\u001b[32m0.18663\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 864 | loss: 0.18663 - acc: 0.9888 -- iter: 45/55\n","Training Step: 9503  | total loss: \u001b[1m\u001b[32m0.16880\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 864 | loss: 0.16880 - acc: 0.9899 -- iter: 50/55\n","Training Step: 9504  | total loss: \u001b[1m\u001b[32m0.15276\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 864 | loss: 0.15276 - acc: 0.9899 -- iter: 55/55\n","--\n","Training Step: 9505  | total loss: \u001b[1m\u001b[32m0.13830\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 865 | loss: 0.13830 - acc: 0.9909 -- iter: 05/55\n","Training Step: 9506  | total loss: \u001b[1m\u001b[32m0.12549\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 865 | loss: 0.12549 - acc: 0.9918 -- iter: 10/55\n","Training Step: 9507  | total loss: \u001b[1m\u001b[32m0.11321\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 865 | loss: 0.11321 - acc: 0.9926 -- iter: 15/55\n","Training Step: 9508  | total loss: \u001b[1m\u001b[32m0.10254\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 865 | loss: 0.10254 - acc: 0.9934 -- iter: 20/55\n","Training Step: 9509  | total loss: \u001b[1m\u001b[32m0.10254\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 865 | loss: 0.10254 - acc: 0.9940 -- iter: 25/55\n","Training Step: 9510  | total loss: \u001b[1m\u001b[32m0.09253\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 865 | loss: 0.09253 - acc: 0.9952 -- iter: 30/55\n","Training Step: 9511  | total loss: \u001b[1m\u001b[32m0.08391\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 865 | loss: 0.08391 - acc: 0.9957 -- iter: 35/55\n","Training Step: 9512  | total loss: \u001b[1m\u001b[32m0.06877\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 865 | loss: 0.06877 - acc: 0.9957 -- iter: 40/55\n","Training Step: 9513  | total loss: \u001b[1m\u001b[32m0.06210\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 865 | loss: 0.06210 - acc: 0.9961 -- iter: 45/55\n","Training Step: 9514  | total loss: \u001b[1m\u001b[32m0.05651\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 865 | loss: 0.05651 - acc: 0.9965 -- iter: 50/55\n","Training Step: 9515  | total loss: \u001b[1m\u001b[32m0.05216\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 865 | loss: 0.05216 - acc: 0.9968 -- iter: 55/55\n","--\n","Training Step: 9516  | total loss: \u001b[1m\u001b[32m0.04825\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 866 | loss: 0.04825 - acc: 0.9972 -- iter: 05/55\n","Training Step: 9517  | total loss: \u001b[1m\u001b[32m0.04364\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 866 | loss: 0.04364 - acc: 0.9974 -- iter: 10/55\n","Training Step: 9518  | total loss: \u001b[1m\u001b[32m0.04364\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 866 | loss: 0.04364 - acc: 0.9977 -- iter: 15/55\n","Training Step: 9519  | total loss: \u001b[1m\u001b[32m0.03996\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 866 | loss: 0.03996 - acc: 0.9979 -- iter: 20/55\n","Training Step: 9520  | total loss: \u001b[1m\u001b[32m0.03656\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 866 | loss: 0.03656 - acc: 0.9981 -- iter: 25/55\n","Training Step: 9521  | total loss: \u001b[1m\u001b[32m0.03362\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 866 | loss: 0.03362 - acc: 0.9983 -- iter: 30/55\n","Training Step: 9522  | total loss: \u001b[1m\u001b[32m0.03054\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 866 | loss: 0.03054 - acc: 0.9986 -- iter: 35/55\n","Training Step: 9523  | total loss: \u001b[1m\u001b[32m0.02777\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 866 | loss: 0.02777 - acc: 0.9988 -- iter: 40/55\n","Training Step: 9524  | total loss: \u001b[1m\u001b[32m0.02517\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 866 | loss: 0.02517 - acc: 0.9989 -- iter: 45/55\n","Training Step: 9525  | total loss: \u001b[1m\u001b[32m0.02330\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 866 | loss: 0.02330 - acc: 0.9990 -- iter: 50/55\n","Training Step: 9526  | total loss: \u001b[1m\u001b[32m0.02193\u001b[0m\u001b[0m | time: 0.066s\n","| Adam | epoch: 866 | loss: 0.02193 - acc: 0.9991 -- iter: 55/55\n","--\n","Training Step: 9527  | total loss: \u001b[1m\u001b[32m0.02016\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 867 | loss: 0.02016 - acc: 0.9992 -- iter: 05/55\n","Training Step: 9528  | total loss: \u001b[1m\u001b[32m0.01760\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 867 | loss: 0.01760 - acc: 0.9993 -- iter: 10/55\n","Training Step: 9529  | total loss: \u001b[1m\u001b[32m0.01640\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 867 | loss: 0.01640 - acc: 0.9993 -- iter: 15/55\n","Training Step: 9530  | total loss: \u001b[1m\u001b[32m0.01534\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 867 | loss: 0.01534 - acc: 0.9994 -- iter: 20/55\n","Training Step: 9531  | total loss: \u001b[1m\u001b[32m0.01427\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 867 | loss: 0.01427 - acc: 0.9995 -- iter: 25/55\n","Training Step: 9532  | total loss: \u001b[1m\u001b[32m0.01330\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 867 | loss: 0.01330 - acc: 0.9995 -- iter: 30/55\n","Training Step: 9533  | total loss: \u001b[1m\u001b[32m0.01231\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 867 | loss: 0.01231 - acc: 0.9996 -- iter: 35/55\n","Training Step: 9534  | total loss: \u001b[1m\u001b[32m0.01167\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 867 | loss: 0.01167 - acc: 0.9996 -- iter: 40/55\n","Training Step: 9535  | total loss: \u001b[1m\u001b[32m0.01086\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 867 | loss: 0.01086 - acc: 0.9997 -- iter: 45/55\n","Training Step: 9536  | total loss: \u001b[1m\u001b[32m0.00997\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 867 | loss: 0.00997 - acc: 0.9997 -- iter: 50/55\n","Training Step: 9537  | total loss: \u001b[1m\u001b[32m0.00926\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 867 | loss: 0.00926 - acc: 0.9997 -- iter: 55/55\n","--\n","Training Step: 9538  | total loss: \u001b[1m\u001b[32m0.00892\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 868 | loss: 0.00892 - acc: 0.9997 -- iter: 05/55\n","Training Step: 9539  | total loss: \u001b[1m\u001b[32m0.00851\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 868 | loss: 0.00851 - acc: 0.9998 -- iter: 10/55\n","Training Step: 9540  | total loss: \u001b[1m\u001b[32m0.00814\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 868 | loss: 0.00814 - acc: 0.9998 -- iter: 15/55\n","Training Step: 9541  | total loss: \u001b[1m\u001b[32m0.00813\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 868 | loss: 0.00813 - acc: 0.9998 -- iter: 20/55\n","Training Step: 9542  | total loss: \u001b[1m\u001b[32m0.00796\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 868 | loss: 0.00796 - acc: 0.9998 -- iter: 25/55\n","Training Step: 9543  | total loss: \u001b[1m\u001b[32m0.00786\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 868 | loss: 0.00786 - acc: 0.9999 -- iter: 30/55\n","Training Step: 9544  | total loss: \u001b[1m\u001b[32m0.00760\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 868 | loss: 0.00760 - acc: 0.9999 -- iter: 35/55\n","Training Step: 9646  | total loss: \u001b[1m\u001b[32m0.29165\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 877 | loss: 0.29165 - acc: 0.9853 -- iter: 50/55\n","Training Step: 9647  | total loss: \u001b[1m\u001b[32m0.26436\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 877 | loss: 0.26436 - acc: 0.9868 -- iter: 55/55\n","--\n","Training Step: 9648  | total loss: \u001b[1m\u001b[32m0.23829\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 878 | loss: 0.23829 - acc: 0.9881 -- iter: 05/55\n","Training Step: 9649  | total loss: \u001b[1m\u001b[32m0.19378\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 878 | loss: 0.19378 - acc: 0.9893 -- iter: 10/55\n","Training Step: 9650  | total loss: \u001b[1m\u001b[32m0.17498\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 878 | loss: 0.17498 - acc: 0.9904 -- iter: 15/55\n","Training Step: 9651  | total loss: \u001b[1m\u001b[32m0.17498\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 878 | loss: 0.17498 - acc: 0.9913 -- iter: 20/55\n","Training Step: 9652  | total loss: \u001b[1m\u001b[32m0.15792\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 878 | loss: 0.15792 - acc: 0.9922 -- iter: 25/55\n","Training Step: 9653  | total loss: \u001b[1m\u001b[32m0.14265\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 878 | loss: 0.14265 - acc: 0.9930 -- iter: 30/55\n","Training Step: 9654  | total loss: \u001b[1m\u001b[32m0.11629\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 878 | loss: 0.11629 - acc: 0.9937 -- iter: 35/55\n","Training Step: 9655  | total loss: \u001b[1m\u001b[32m0.10520\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 878 | loss: 0.10520 - acc: 0.9943 -- iter: 40/55\n","Training Step: 9656  | total loss: \u001b[1m\u001b[32m0.09492\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 878 | loss: 0.09492 - acc: 0.9949 -- iter: 45/55\n","Training Step: 9657  | total loss: \u001b[1m\u001b[32m0.08571\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 878 | loss: 0.08571 - acc: 0.9954 -- iter: 50/55\n","Training Step: 9658  | total loss: \u001b[1m\u001b[32m0.07750\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 878 | loss: 0.07750 - acc: 0.9958 -- iter: 55/55\n","--\n","Training Step: 9659  | total loss: \u001b[1m\u001b[32m0.07750\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 879 | loss: 0.07750 - acc: 0.9963 -- iter: 05/55\n","Training Step: 9660  | total loss: \u001b[1m\u001b[32m0.07028\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 879 | loss: 0.07028 - acc: 0.9966 -- iter: 10/55\n","Training Step: 9661  | total loss: \u001b[1m\u001b[32m0.06378\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 879 | loss: 0.06378 - acc: 0.9970 -- iter: 15/55\n","Training Step: 9662  | total loss: \u001b[1m\u001b[32m0.05308\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 879 | loss: 0.05308 - acc: 0.9973 -- iter: 20/55\n","Training Step: 9663  | total loss: \u001b[1m\u001b[32m0.04848\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 879 | loss: 0.04848 - acc: 0.9973 -- iter: 25/55\n","Training Step: 9664  | total loss: \u001b[1m\u001b[32m0.04423\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 879 | loss: 0.04423 - acc: 0.9975 -- iter: 30/55\n","Training Step: 9665  | total loss: \u001b[1m\u001b[32m0.04034\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 879 | loss: 0.04034 - acc: 0.9978 -- iter: 35/55\n","Training Step: 9666  | total loss: \u001b[1m\u001b[32m0.03672\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 879 | loss: 0.03672 - acc: 0.9980 -- iter: 40/55\n","Training Step: 9667  | total loss: \u001b[1m\u001b[32m0.03672\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 879 | loss: 0.03672 - acc: 0.9984 -- iter: 45/55\n","Training Step: 9668  | total loss: \u001b[1m\u001b[32m0.03346\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 879 | loss: 0.03346 - acc: 0.9986 -- iter: 50/55\n","Training Step: 9669  | total loss: \u001b[1m\u001b[32m0.03053\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 879 | loss: 0.03053 - acc: 0.9987 -- iter: 55/55\n","--\n","Training Step: 9670  | total loss: \u001b[1m\u001b[32m0.02769\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 880 | loss: 0.02769 - acc: 0.9988 -- iter: 05/55\n","Training Step: 9671  | total loss: \u001b[1m\u001b[32m0.02557\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 880 | loss: 0.02557 - acc: 0.9989 -- iter: 10/55\n","Training Step: 9672  | total loss: \u001b[1m\u001b[32m0.02228\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 880 | loss: 0.02228 - acc: 0.9991 -- iter: 15/55\n","Training Step: 9673  | total loss: \u001b[1m\u001b[32m0.02045\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 880 | loss: 0.02045 - acc: 0.9991 -- iter: 20/55\n","Training Step: 9674  | total loss: \u001b[1m\u001b[32m0.01858\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 880 | loss: 0.01858 - acc: 0.9992 -- iter: 25/55\n","Training Step: 9675  | total loss: \u001b[1m\u001b[32m0.01703\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 880 | loss: 0.01703 - acc: 0.9993 -- iter: 30/55\n","Training Step: 9676  | total loss: \u001b[1m\u001b[32m0.01578\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 880 | loss: 0.01578 - acc: 0.9994 -- iter: 35/55\n","Training Step: 9677  | total loss: \u001b[1m\u001b[32m0.01468\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 880 | loss: 0.01468 - acc: 0.9994 -- iter: 40/55\n","Training Step: 9678  | total loss: \u001b[1m\u001b[32m0.01338\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 880 | loss: 0.01338 - acc: 0.9995 -- iter: 45/55\n","Training Step: 9679  | total loss: \u001b[1m\u001b[32m0.01237\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 880 | loss: 0.01237 - acc: 0.9995 -- iter: 50/55\n","Training Step: 9680  | total loss: \u001b[1m\u001b[32m0.01162\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 880 | loss: 0.01162 - acc: 0.9995 -- iter: 55/55\n","--\n","Training Step: 9681  | total loss: \u001b[1m\u001b[32m0.01100\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 881 | loss: 0.01100 - acc: 0.9996 -- iter: 05/55\n","Training Step: 9682  | total loss: \u001b[1m\u001b[32m0.01053\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 881 | loss: 0.01053 - acc: 0.9997 -- iter: 10/55\n","Training Step: 9683  | total loss: \u001b[1m\u001b[32m0.00954\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 881 | loss: 0.00954 - acc: 0.9997 -- iter: 15/55\n","Training Step: 9684  | total loss: \u001b[1m\u001b[32m0.00865\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 881 | loss: 0.00865 - acc: 0.9997 -- iter: 20/55\n","Training Step: 9685  | total loss: \u001b[1m\u001b[32m0.00834\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 881 | loss: 0.00834 - acc: 0.9998 -- iter: 25/55\n","Training Step: 9686  | total loss: \u001b[1m\u001b[32m0.00834\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 881 | loss: 0.00834 - acc: 0.9998 -- iter: 30/55\n","Training Step: 9687  | total loss: \u001b[1m\u001b[32m0.00793\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 881 | loss: 0.00793 - acc: 0.9998 -- iter: 35/55\n","Training Step: 9688  | total loss: \u001b[1m\u001b[32m0.00766\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 881 | loss: 0.00766 - acc: 0.9998 -- iter: 40/55\n","Training Step: 9689  | total loss: \u001b[1m\u001b[32m0.00721\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 881 | loss: 0.00721 - acc: 0.9998 -- iter: 45/55\n","Training Step: 9690  | total loss: \u001b[1m\u001b[32m0.00689\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 881 | loss: 0.00689 - acc: 0.9999 -- iter: 50/55\n","Training Step: 9691  | total loss: \u001b[1m\u001b[32m0.00674\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 881 | loss: 0.00674 - acc: 0.9999 -- iter: 55/55\n","--\n","Training Step: 9692  | total loss: \u001b[1m\u001b[32m0.00629\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 882 | loss: 0.00629 - acc: 0.9999 -- iter: 05/55\n","Training Step: 9693  | total loss: \u001b[1m\u001b[32m0.00644\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 882 | loss: 0.00644 - acc: 0.9999 -- iter: 10/55\n","Training Step: 9694  | total loss: \u001b[1m\u001b[32m0.00601\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 882 | loss: 0.00601 - acc: 0.9999 -- iter: 15/55\n","Training Step: 9695  | total loss: \u001b[1m\u001b[32m0.00573\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 882 | loss: 0.00573 - acc: 0.9999 -- iter: 20/55\n","Training Step: 9696  | total loss: \u001b[1m\u001b[32m0.00564\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 882 | loss: 0.00564 - acc: 0.9999 -- iter: 25/55\n","Training Step: 9697  | total loss: \u001b[1m\u001b[32m0.00556\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 882 | loss: 0.00556 - acc: 0.9999 -- iter: 30/55\n","Training Step: 9698  | total loss: \u001b[1m\u001b[32m0.00531\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 882 | loss: 0.00531 - acc: 0.9999 -- iter: 35/55\n","Training Step: 9699  | total loss: \u001b[1m\u001b[32m0.00522\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 882 | loss: 0.00522 - acc: 0.9999 -- iter: 40/55\n","Training Step: 9700  | total loss: \u001b[1m\u001b[32m0.00517\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 882 | loss: 0.00517 - acc: 1.0000 -- iter: 45/55\n","Training Step: 9701  | total loss: \u001b[1m\u001b[32m0.00501\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 882 | loss: 0.00501 - acc: 1.0000 -- iter: 50/55\n","Training Step: 9702  | total loss: \u001b[1m\u001b[32m0.00495\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 882 | loss: 0.00495 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 9703  | total loss: \u001b[1m\u001b[32m0.00482\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 883 | loss: 0.00482 - acc: 1.0000 -- iter: 05/55\n","Training Step: 9704  | total loss: \u001b[1m\u001b[32m0.00468\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 883 | loss: 0.00468 - acc: 1.0000 -- iter: 10/55\n","Training Step: 9705  | total loss: \u001b[1m\u001b[32m0.00443\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 883 | loss: 0.00443 - acc: 1.0000 -- iter: 15/55\n","Training Step: 9706  | total loss: \u001b[1m\u001b[32m0.00436\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 883 | loss: 0.00436 - acc: 1.0000 -- iter: 20/55\n","Training Step: 9707  | total loss: \u001b[1m\u001b[32m0.00423\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 883 | loss: 0.00423 - acc: 1.0000 -- iter: 25/55\n","Training Step: 9708  | total loss: \u001b[1m\u001b[32m0.00435\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 883 | loss: 0.00435 - acc: 1.0000 -- iter: 30/55\n","Training Step: 9709  | total loss: \u001b[1m\u001b[32m0.00445\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 883 | loss: 0.00445 - acc: 1.0000 -- iter: 35/55\n","Training Step: 9710  | total loss: \u001b[1m\u001b[32m0.00473\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 883 | loss: 0.00473 - acc: 1.0000 -- iter: 40/55\n","Training Step: 9711  | total loss: \u001b[1m\u001b[32m0.00454\u001b[0m\u001b[0m | time: 0.063s\n","| Adam | epoch: 883 | loss: 0.00454 - acc: 1.0000 -- iter: 45/55\n","Training Step: 9712  | total loss: \u001b[1m\u001b[32m0.00413\u001b[0m\u001b[0m | time: 0.068s\n","| Adam | epoch: 883 | loss: 0.00413 - acc: 1.0000 -- iter: 50/55\n","Training Step: 9713  | total loss: \u001b[1m\u001b[32m0.00399\u001b[0m\u001b[0m | time: 0.072s\n","| Adam | epoch: 883 | loss: 0.00399 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 9714  | total loss: \u001b[1m\u001b[32m0.00394\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 884 | loss: 0.00394 - acc: 1.0000 -- iter: 05/55\n","Training Step: 9715  | total loss: \u001b[1m\u001b[32m0.00409\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 884 | loss: 0.00409 - acc: 1.0000 -- iter: 10/55\n","Training Step: 9716  | total loss: \u001b[1m\u001b[32m0.00409\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 884 | loss: 0.00409 - acc: 1.0000 -- iter: 15/55\n","Training Step: 9717  | total loss: \u001b[1m\u001b[32m0.00418\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 884 | loss: 0.00418 - acc: 1.0000 -- iter: 20/55\n","Training Step: 9718  | total loss: \u001b[1m\u001b[32m0.00417\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 884 | loss: 0.00417 - acc: 1.0000 -- iter: 25/55\n","Training Step: 9719  | total loss: \u001b[1m\u001b[32m0.00445\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 884 | loss: 0.00445 - acc: 1.0000 -- iter: 30/55\n","Training Step: 9720  | total loss: \u001b[1m\u001b[32m0.00445\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 884 | loss: 0.00445 - acc: 1.0000 -- iter: 35/55\n","Training Step: 9721  | total loss: \u001b[1m\u001b[32m0.00446\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 884 | loss: 0.00446 - acc: 1.0000 -- iter: 40/55\n","Training Step: 9722  | total loss: \u001b[1m\u001b[32m0.00444\u001b[0m\u001b[0m | time: 0.059s\n","| Adam | epoch: 884 | loss: 0.00444 - acc: 1.0000 -- iter: 45/55\n","Training Step: 9723  | total loss: \u001b[1m\u001b[32m0.00417\u001b[0m\u001b[0m | time: 0.064s\n","| Adam | epoch: 884 | loss: 0.00417 - acc: 1.0000 -- iter: 50/55\n","Training Step: 9724  | total loss: \u001b[1m\u001b[32m0.00390\u001b[0m\u001b[0m | time: 0.071s\n","| Adam | epoch: 884 | loss: 0.00390 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 9725  | total loss: \u001b[1m\u001b[32m0.00395\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 885 | loss: 0.00395 - acc: 1.0000 -- iter: 05/55\n","Training Step: 9726  | total loss: \u001b[1m\u001b[32m0.00369\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 885 | loss: 0.00369 - acc: 1.0000 -- iter: 10/55\n","Training Step: 9727  | total loss: \u001b[1m\u001b[32m0.00416\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 885 | loss: 0.00416 - acc: 1.0000 -- iter: 15/55\n","Training Step: 9728  | total loss: \u001b[1m\u001b[32m0.00394\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 885 | loss: 0.00394 - acc: 1.0000 -- iter: 20/55\n","Training Step: 9729  | total loss: \u001b[1m\u001b[32m0.00400\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 885 | loss: 0.00400 - acc: 1.0000 -- iter: 25/55\n","Training Step: 9730  | total loss: \u001b[1m\u001b[32m0.00416\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 885 | loss: 0.00416 - acc: 1.0000 -- iter: 30/55\n","Training Step: 9731  | total loss: \u001b[1m\u001b[32m0.00408\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 885 | loss: 0.00408 - acc: 1.0000 -- iter: 35/55\n","Training Step: 9732  | total loss: \u001b[1m\u001b[32m0.00401\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 885 | loss: 0.00401 - acc: 1.0000 -- iter: 40/55\n","Training Step: 9733  | total loss: \u001b[1m\u001b[32m0.00382\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 885 | loss: 0.00382 - acc: 1.0000 -- iter: 45/55\n","Training Step: 9734  | total loss: \u001b[1m\u001b[32m0.00392\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 885 | loss: 0.00392 - acc: 1.0000 -- iter: 50/55\n","Training Step: 9735  | total loss: \u001b[1m\u001b[32m1.84577\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 885 | loss: 1.84577 - acc: 0.9200 -- iter: 55/55\n","--\n","Training Step: 9736  | total loss: \u001b[1m\u001b[32m1.66143\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 886 | loss: 1.66143 - acc: 0.9280 -- iter: 05/55\n","Training Step: 9737  | total loss: \u001b[1m\u001b[32m1.49592\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 886 | loss: 1.49592 - acc: 0.9352 -- iter: 10/55\n","Training Step: 9738  | total loss: \u001b[1m\u001b[32m1.34659\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 886 | loss: 1.34659 - acc: 0.9417 -- iter: 15/55\n","Training Step: 9739  | total loss: \u001b[1m\u001b[32m1.21217\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 886 | loss: 1.21217 - acc: 0.9475 -- iter: 20/55\n","Training Step: 9740  | total loss: \u001b[1m\u001b[32m1.09143\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 886 | loss: 1.09143 - acc: 0.9528 -- iter: 25/55\n","Training Step: 9741  | total loss: \u001b[1m\u001b[32m0.98275\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 886 | loss: 0.98275 - acc: 0.9575 -- iter: 30/55\n","Training Step: 9742  | total loss: \u001b[1m\u001b[32m0.88488\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 886 | loss: 0.88488 - acc: 0.9617 -- iter: 35/55\n","Training Step: 9743  | total loss: \u001b[1m\u001b[32m0.79657\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 886 | loss: 0.79657 - acc: 0.9656 -- iter: 40/55\n","Training Step: 9744  | total loss: \u001b[1m\u001b[32m0.71709\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 886 | loss: 0.71709 - acc: 0.9690 -- iter: 45/55\n","Training Step: 9745  | total loss: \u001b[1m\u001b[32m0.64557\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 886 | loss: 0.64557 - acc: 0.9721 -- iter: 50/55\n","Training Step: 9746  | total loss: \u001b[1m\u001b[32m0.58139\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 886 | loss: 0.58139 - acc: 0.9749 -- iter: 55/55\n","--\n","Training Step: 9747  | total loss: \u001b[1m\u001b[32m0.58139\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 887 | loss: 0.58139 - acc: 0.9749 -- iter: 05/55\n","Training Step: 9748  | total loss: \u001b[1m\u001b[32m0.52398\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 887 | loss: 0.52398 - acc: 0.9774 -- iter: 10/55\n","Training Step: 9749  | total loss: \u001b[1m\u001b[32m0.47213\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 887 | loss: 0.47213 - acc: 0.9797 -- iter: 15/55\n","Training Step: 9750  | total loss: \u001b[1m\u001b[32m0.42525\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 887 | loss: 0.42525 - acc: 0.9817 -- iter: 20/55\n","Training Step: 9751  | total loss: \u001b[1m\u001b[32m0.34533\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 887 | loss: 0.34533 - acc: 0.9852 -- iter: 25/55\n","Training Step: 9752  | total loss: \u001b[1m\u001b[32m0.31111\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 887 | loss: 0.31111 - acc: 0.9867 -- iter: 30/55\n","Training Step: 9753  | total loss: \u001b[1m\u001b[32m0.28033\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 887 | loss: 0.28033 - acc: 0.9880 -- iter: 35/55\n","Training Step: 9754  | total loss: \u001b[1m\u001b[32m0.25264\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 887 | loss: 0.25264 - acc: 0.9892 -- iter: 40/55\n","Training Step: 9755  | total loss: \u001b[1m\u001b[32m0.22792\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 887 | loss: 0.22792 - acc: 0.9903 -- iter: 45/55\n","Training Step: 9756  | total loss: \u001b[1m\u001b[32m0.20568\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 887 | loss: 0.20568 - acc: 0.9912 -- iter: 50/55\n","Training Step: 9757  | total loss: \u001b[1m\u001b[32m0.18526\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 887 | loss: 0.18526 - acc: 0.9921 -- iter: 55/55\n","--\n","Training Step: 9758  | total loss: \u001b[1m\u001b[32m0.16700\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 888 | loss: 0.16700 - acc: 0.9929 -- iter: 05/55\n","Training Step: 9759  | total loss: \u001b[1m\u001b[32m2.22183\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 888 | loss: 2.22183 - acc: 0.8936 -- iter: 10/55\n","Training Step: 9760  | total loss: \u001b[1m\u001b[32m1.99982\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 888 | loss: 1.99982 - acc: 0.9043 -- iter: 15/55\n","Training Step: 9761  | total loss: \u001b[1m\u001b[32m1.80039\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 888 | loss: 1.80039 - acc: 0.9138 -- iter: 20/55\n","Training Step: 9762  | total loss: \u001b[1m\u001b[32m1.62077\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 888 | loss: 1.62077 - acc: 0.9224 -- iter: 25/55\n","Training Step: 9763  | total loss: \u001b[1m\u001b[32m1.45918\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 888 | loss: 1.45918 - acc: 0.9302 -- iter: 30/55\n","Training Step: 9764  | total loss: \u001b[1m\u001b[32m1.31359\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 888 | loss: 1.31359 - acc: 0.9372 -- iter: 35/55\n","Training Step: 9765  | total loss: \u001b[1m\u001b[32m1.18267\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 888 | loss: 1.18267 - acc: 0.9372 -- iter: 40/55\n","Training Step: 9766  | total loss: \u001b[1m\u001b[32m1.06487\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 888 | loss: 1.06487 - acc: 0.9435 -- iter: 45/55\n","Training Step: 9767  | total loss: \u001b[1m\u001b[32m0.95854\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 888 | loss: 0.95854 - acc: 0.9491 -- iter: 50/55\n","Training Step: 9768  | total loss: \u001b[1m\u001b[32m0.86284\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 888 | loss: 0.86284 - acc: 0.9588 -- iter: 55/55\n","--\n","Training Step: 9769  | total loss: \u001b[1m\u001b[32m0.77691\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 889 | loss: 0.77691 - acc: 0.9629 -- iter: 05/55\n","Training Step: 9770  | total loss: \u001b[1m\u001b[32m0.69963\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 889 | loss: 0.69963 - acc: 0.9666 -- iter: 10/55\n","Training Step: 9771  | total loss: \u001b[1m\u001b[32m2.18758\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 889 | loss: 2.18758 - acc: 0.8700 -- iter: 15/55\n","Training Step: 9772  | total loss: \u001b[1m\u001b[32m2.18758\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 889 | loss: 2.18758 - acc: 0.8830 -- iter: 20/55\n","Training Step: 9773  | total loss: \u001b[1m\u001b[32m1.96898\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 889 | loss: 1.96898 - acc: 0.8947 -- iter: 25/55\n","Training Step: 9774  | total loss: \u001b[1m\u001b[32m1.77242\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 889 | loss: 1.77242 - acc: 0.8947 -- iter: 30/55\n","Training Step: 9775  | total loss: \u001b[1m\u001b[32m1.59552\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 889 | loss: 1.59552 - acc: 0.9052 -- iter: 35/55\n","Training Step: 9776  | total loss: \u001b[1m\u001b[32m1.43638\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 889 | loss: 1.43638 - acc: 0.9147 -- iter: 40/55\n","Training Step: 9777  | total loss: \u001b[1m\u001b[32m1.29305\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 889 | loss: 1.29305 - acc: 0.9232 -- iter: 45/55\n","Training Step: 9778  | total loss: \u001b[1m\u001b[32m1.16402\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 889 | loss: 1.16402 - acc: 0.9309 -- iter: 50/55\n","Training Step: 9779  | total loss: \u001b[1m\u001b[32m1.04815\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 889 | loss: 1.04815 - acc: 0.9378 -- iter: 55/55\n","--\n","Training Step: 9780  | total loss: \u001b[1m\u001b[32m0.94394\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 890 | loss: 0.94394 - acc: 0.9440 -- iter: 05/55\n","Training Step: 9781  | total loss: \u001b[1m\u001b[32m0.85016\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 890 | loss: 0.85016 - acc: 0.9496 -- iter: 10/55\n","Training Step: 9782  | total loss: \u001b[1m\u001b[32m0.76555\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 890 | loss: 0.76555 - acc: 0.9547 -- iter: 15/55\n","Training Step: 9783  | total loss: \u001b[1m\u001b[32m0.74821\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 890 | loss: 0.74821 - acc: 0.8453 -- iter: 20/55\n","Training Step: 9784  | total loss: \u001b[1m\u001b[32m2.44949\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 890 | loss: 2.44949 - acc: 0.8407 -- iter: 25/55\n","Training Step: 9785  | total loss: \u001b[1m\u001b[32m2.24644\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 890 | loss: 2.24644 - acc: 0.8567 -- iter: 30/55\n","Training Step: 9786  | total loss: \u001b[1m\u001b[32m2.02210\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 890 | loss: 2.02210 - acc: 0.8710 -- iter: 35/55\n","Training Step: 9787  | total loss: \u001b[1m\u001b[32m1.82046\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 890 | loss: 1.82046 - acc: 0.8839 -- iter: 40/55\n","Training Step: 9788  | total loss: \u001b[1m\u001b[32m1.63895\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 890 | loss: 1.63895 - acc: 0.8955 -- iter: 45/55\n","Training Step: 9789  | total loss: \u001b[1m\u001b[32m1.47540\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 890 | loss: 1.47540 - acc: 0.9060 -- iter: 50/55\n","Training Step: 9790  | total loss: \u001b[1m\u001b[32m1.32821\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 890 | loss: 1.32821 - acc: 0.9154 -- iter: 55/55\n","--\n","Training Step: 9791  | total loss: \u001b[1m\u001b[32m1.07670\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 891 | loss: 1.07670 - acc: 0.9238 -- iter: 05/55\n","Training Step: 9792  | total loss: \u001b[1m\u001b[32m0.96933\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 891 | loss: 0.96933 - acc: 0.9314 -- iter: 10/55\n","Training Step: 9793  | total loss: \u001b[1m\u001b[32m0.87283\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 891 | loss: 0.87283 - acc: 0.9383 -- iter: 15/55\n","Training Step: 9794  | total loss: \u001b[1m\u001b[32m0.78597\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 891 | loss: 0.78597 - acc: 0.9445 -- iter: 20/55\n","Training Step: 9795  | total loss: \u001b[1m\u001b[32m0.78597\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 891 | loss: 0.78597 - acc: 0.8700 -- iter: 25/55\n","Training Step: 9796  | total loss: \u001b[1m\u001b[32m1.72716\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 891 | loss: 1.72716 - acc: 0.8830 -- iter: 30/55\n","Training Step: 9797  | total loss: \u001b[1m\u001b[32m1.39951\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 891 | loss: 1.39951 - acc: 0.8947 -- iter: 35/55\n","Training Step: 9798  | total loss: \u001b[1m\u001b[32m1.39951\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 891 | loss: 1.39951 - acc: 0.9052 -- iter: 40/55\n","Training Step: 9799  | total loss: \u001b[1m\u001b[32m1.26042\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 891 | loss: 1.26042 - acc: 0.9147 -- iter: 45/55\n","Training Step: 9800  | total loss: \u001b[1m\u001b[32m1.13453\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 891 | loss: 1.13453 - acc: 0.9147 -- iter: 50/55\n","Training Step: 9801  | total loss: \u001b[1m\u001b[32m1.02186\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 891 | loss: 1.02186 - acc: 0.9233 -- iter: 55/55\n","--\n","Training Step: 9802  | total loss: \u001b[1m\u001b[32m0.92023\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 892 | loss: 0.92023 - acc: 0.9309 -- iter: 05/55\n","Training Step: 9803  | total loss: \u001b[1m\u001b[32m0.82877\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 892 | loss: 0.82877 - acc: 0.9378 -- iter: 10/55\n","Training Step: 9804  | total loss: \u001b[1m\u001b[32m0.74651\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 892 | loss: 0.74651 - acc: 0.9440 -- iter: 15/55\n","Training Step: 9805  | total loss: \u001b[1m\u001b[32m0.67247\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 892 | loss: 0.67247 - acc: 0.9547 -- iter: 20/55\n","Training Step: 9806  | total loss: \u001b[1m\u001b[32m0.60554\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 892 | loss: 0.60554 - acc: 0.9592 -- iter: 25/55\n","Training Step: 9807  | total loss: \u001b[1m\u001b[32m0.54616\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 892 | loss: 0.54616 - acc: 0.8633 -- iter: 30/55\n","Training Step: 9808  | total loss: \u001b[1m\u001b[32m2.72253\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 892 | loss: 2.72253 - acc: 0.8770 -- iter: 35/55\n","Training Step: 9809  | total loss: \u001b[1m\u001b[32m2.45065\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 892 | loss: 2.45065 - acc: 0.8770 -- iter: 40/55\n","Training Step: 9810  | total loss: \u001b[1m\u001b[32m2.20629\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 892 | loss: 2.20629 - acc: 0.8893 -- iter: 45/55\n","Training Step: 9811  | total loss: \u001b[1m\u001b[32m1.78890\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 892 | loss: 1.78890 - acc: 0.9003 -- iter: 50/55\n","Training Step: 9812  | total loss: \u001b[1m\u001b[32m1.61061\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 892 | loss: 1.61061 - acc: 0.9103 -- iter: 55/55\n","--\n","Training Step: 9813  | total loss: \u001b[1m\u001b[32m1.61061\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 893 | loss: 1.61061 - acc: 0.9273 -- iter: 05/55\n","Training Step: 9814  | total loss: \u001b[1m\u001b[32m1.44973\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 893 | loss: 1.44973 - acc: 0.9346 -- iter: 10/55\n","Training Step: 9815  | total loss: \u001b[1m\u001b[32m1.30494\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 893 | loss: 1.30494 - acc: 0.9412 -- iter: 15/55\n","Training Step: 9816  | total loss: \u001b[1m\u001b[32m1.17518\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 893 | loss: 1.17518 - acc: 0.9470 -- iter: 20/55\n","Training Step: 9817  | total loss: \u001b[1m\u001b[32m1.05839\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 893 | loss: 1.05839 - acc: 0.9523 -- iter: 25/55\n","Training Step: 9818  | total loss: \u001b[1m\u001b[32m0.95279\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 893 | loss: 0.95279 - acc: 0.9571 -- iter: 30/55\n","Training Step: 9819  | total loss: \u001b[1m\u001b[32m2.21161\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 893 | loss: 2.21161 - acc: 0.8814 -- iter: 35/55\n","Training Step: 9820  | total loss: \u001b[1m\u001b[32m1.99105\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 893 | loss: 1.99105 - acc: 0.8933 -- iter: 40/55\n","Training Step: 9821  | total loss: \u001b[1m\u001b[32m1.79209\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 893 | loss: 1.79209 - acc: 0.9039 -- iter: 45/55\n","Training Step: 9822  | total loss: \u001b[1m\u001b[32m1.61341\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 893 | loss: 1.61341 - acc: 0.9135 -- iter: 50/55\n","Training Step: 9823  | total loss: \u001b[1m\u001b[32m1.61341\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 893 | loss: 1.61341 - acc: 0.9222 -- iter: 55/55\n","--\n","Training Step: 9824  | total loss: \u001b[1m\u001b[32m1.45269\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 894 | loss: 1.45269 - acc: 0.9300 -- iter: 05/55\n","Training Step: 9825  | total loss: \u001b[1m\u001b[32m1.17774\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 894 | loss: 1.17774 - acc: 0.9370 -- iter: 10/55\n","Training Step: 9826  | total loss: \u001b[1m\u001b[32m1.06115\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 894 | loss: 1.06115 - acc: 0.9433 -- iter: 15/55\n","Training Step: 9827  | total loss: \u001b[1m\u001b[32m1.06115\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 894 | loss: 1.06115 - acc: 0.9489 -- iter: 20/55\n","Training Step: 9828  | total loss: \u001b[1m\u001b[32m0.95569\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 894 | loss: 0.95569 - acc: 0.9489 -- iter: 25/55\n","Training Step: 9829  | total loss: \u001b[1m\u001b[32m0.86082\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 894 | loss: 0.86082 - acc: 0.9540 -- iter: 30/55\n","Training Step: 9830  | total loss: \u001b[1m\u001b[32m0.77531\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 894 | loss: 0.77531 - acc: 0.9586 -- iter: 35/55\n","Training Step: 9831  | total loss: \u001b[1m\u001b[32m0.69849\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 894 | loss: 0.69849 - acc: 0.9628 -- iter: 40/55\n","Training Step: 9832  | total loss: \u001b[1m\u001b[32m2.46813\u001b[0m\u001b[0m | time: 0.059s\n","| Adam | epoch: 894 | loss: 2.46813 - acc: 0.8665 -- iter: 45/55\n","Training Step: 9833  | total loss: \u001b[1m\u001b[32m2.22209\u001b[0m\u001b[0m | time: 0.065s\n","| Adam | epoch: 894 | loss: 2.22209 - acc: 0.8799 -- iter: 50/55\n","Training Step: 9834  | total loss: \u001b[1m\u001b[32m2.00044\u001b[0m\u001b[0m | time: 0.071s\n","| Adam | epoch: 894 | loss: 2.00044 - acc: 0.8919 -- iter: 55/55\n","--\n","Training Step: 9835  | total loss: \u001b[1m\u001b[32m1.80129\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 895 | loss: 1.80129 - acc: 0.9027 -- iter: 05/55\n","Training Step: 9836  | total loss: \u001b[1m\u001b[32m1.62184\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 895 | loss: 1.62184 - acc: 0.9124 -- iter: 10/55\n","Training Step: 9837  | total loss: \u001b[1m\u001b[32m1.46098\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 895 | loss: 1.46098 - acc: 0.9212 -- iter: 15/55\n","Training Step: 9838  | total loss: \u001b[1m\u001b[32m1.31527\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 895 | loss: 1.31527 - acc: 0.9291 -- iter: 20/55\n","Training Step: 9839  | total loss: \u001b[1m\u001b[32m1.06745\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 895 | loss: 1.06745 - acc: 0.9361 -- iter: 25/55\n","Training Step: 9840  | total loss: \u001b[1m\u001b[32m0.96223\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 895 | loss: 0.96223 - acc: 0.9425 -- iter: 30/55\n","Training Step: 9841  | total loss: \u001b[1m\u001b[32m0.86705\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 895 | loss: 0.86705 - acc: 0.9535 -- iter: 35/55\n","Training Step: 9842  | total loss: \u001b[1m\u001b[32m0.78054\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 895 | loss: 0.78054 - acc: 0.9581 -- iter: 40/55\n","Training Step: 9843  | total loss: \u001b[1m\u001b[32m0.70339\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 895 | loss: 0.70339 - acc: 0.9623 -- iter: 45/55\n","Training Step: 9844  | total loss: \u001b[1m\u001b[32m0.63317\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 895 | loss: 0.63317 - acc: 0.9661 -- iter: 50/55\n","Training Step: 9845  | total loss: \u001b[1m\u001b[32m0.57123\u001b[0m\u001b[0m | time: 0.067s\n","| Adam | epoch: 895 | loss: 0.57123 - acc: 0.9695 -- iter: 55/55\n","--\n","Training Step: 9846  | total loss: \u001b[1m\u001b[32m0.57123\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 896 | loss: 0.57123 - acc: 0.9695 -- iter: 05/55\n","Training Step: 9847  | total loss: \u001b[1m\u001b[32m0.46396\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 896 | loss: 0.46396 - acc: 0.9753 -- iter: 10/55\n","Training Step: 9848  | total loss: \u001b[1m\u001b[32m0.41837\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 896 | loss: 0.41837 - acc: 0.9777 -- iter: 15/55\n","Training Step: 9849  | total loss: \u001b[1m\u001b[32m0.37842\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 896 | loss: 0.37842 - acc: 0.9800 -- iter: 20/55\n","Training Step: 9850  | total loss: \u001b[1m\u001b[32m0.34150\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 896 | loss: 0.34150 - acc: 0.9820 -- iter: 25/55\n","Training Step: 9851  | total loss: \u001b[1m\u001b[32m0.30793\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 896 | loss: 0.30793 - acc: 0.9838 -- iter: 30/55\n","Training Step: 9852  | total loss: \u001b[1m\u001b[32m0.27771\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 896 | loss: 0.27771 - acc: 0.9854 -- iter: 35/55\n","Training Step: 9853  | total loss: \u001b[1m\u001b[32m0.25062\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 896 | loss: 0.25062 - acc: 0.9869 -- iter: 40/55\n","Training Step: 9854  | total loss: \u001b[1m\u001b[32m0.22601\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 896 | loss: 0.22601 - acc: 0.9882 -- iter: 45/55\n","Training Step: 9855  | total loss: \u001b[1m\u001b[32m0.22601\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 896 | loss: 0.22601 - acc: 0.9882 -- iter: 50/55\n","Training Step: 9856  | total loss: \u001b[1m\u001b[32m0.20440\u001b[0m\u001b[0m | time: 0.059s\n","| Adam | epoch: 896 | loss: 0.20440 - acc: 0.9894 -- iter: 55/55\n","--\n","Training Step: 9857  | total loss: \u001b[1m\u001b[32m0.18513\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 897 | loss: 0.18513 - acc: 0.9904 -- iter: 05/55\n","Training Step: 9858  | total loss: \u001b[1m\u001b[32m0.16744\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 897 | loss: 0.16744 - acc: 0.9922 -- iter: 10/55\n","Training Step: 9859  | total loss: \u001b[1m\u001b[32m0.15153\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 897 | loss: 0.15153 - acc: 0.9930 -- iter: 15/55\n","Training Step: 9860  | total loss: \u001b[1m\u001b[32m0.13664\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 897 | loss: 0.13664 - acc: 0.9937 -- iter: 20/55\n","Training Step: 9861  | total loss: \u001b[1m\u001b[32m0.12317\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 897 | loss: 0.12317 - acc: 0.9943 -- iter: 25/55\n","Training Step: 9862  | total loss: \u001b[1m\u001b[32m0.11128\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 897 | loss: 0.11128 - acc: 0.9949 -- iter: 30/55\n","Training Step: 9863  | total loss: \u001b[1m\u001b[32m0.09187\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 897 | loss: 0.09187 - acc: 0.9954 -- iter: 35/55\n","Training Step: 9864  | total loss: \u001b[1m\u001b[32m0.08338\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 897 | loss: 0.08338 - acc: 0.9959 -- iter: 40/55\n","Training Step: 9865  | total loss: \u001b[1m\u001b[32m0.08338\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 897 | loss: 0.08338 - acc: 0.9959 -- iter: 45/55\n","Training Step: 9866  | total loss: \u001b[1m\u001b[32m0.07544\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 897 | loss: 0.07544 - acc: 0.9963 -- iter: 50/55\n","Training Step: 9867  | total loss: \u001b[1m\u001b[32m0.06896\u001b[0m\u001b[0m | time: 0.063s\n","| Adam | epoch: 897 | loss: 0.06896 - acc: 0.9967 -- iter: 55/55\n","--\n","Training Step: 9868  | total loss: \u001b[1m\u001b[32m0.06360\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 898 | loss: 0.06360 - acc: 0.9970 -- iter: 05/55\n","Training Step: 9869  | total loss: \u001b[1m\u001b[32m0.05794\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 898 | loss: 0.05794 - acc: 0.9973 -- iter: 10/55\n","Training Step: 9870  | total loss: \u001b[1m\u001b[32m0.05325\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 898 | loss: 0.05325 - acc: 0.9976 -- iter: 15/55\n","Training Step: 9871  | total loss: \u001b[1m\u001b[32m0.04824\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 898 | loss: 0.04824 - acc: 0.9978 -- iter: 20/55\n","Training Step: 9872  | total loss: \u001b[1m\u001b[32m0.04405\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 898 | loss: 0.04405 - acc: 0.9980 -- iter: 25/55\n","Training Step: 9873  | total loss: \u001b[1m\u001b[32m0.04051\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 898 | loss: 0.04051 - acc: 0.9982 -- iter: 30/55\n","Training Step: 9874  | total loss: \u001b[1m\u001b[32m0.03731\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 898 | loss: 0.03731 - acc: 0.9984 -- iter: 35/55\n","Training Step: 9875  | total loss: \u001b[1m\u001b[32m0.03485\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 898 | loss: 0.03485 - acc: 0.9987 -- iter: 40/55\n","Training Step: 9876  | total loss: \u001b[1m\u001b[32m0.03233\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 898 | loss: 0.03233 - acc: 0.9988 -- iter: 45/55\n","Training Step: 9877  | total loss: \u001b[1m\u001b[32m0.03006\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 898 | loss: 0.03006 - acc: 0.9990 -- iter: 50/55\n","Training Step: 9878  | total loss: \u001b[1m\u001b[32m0.02752\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 898 | loss: 0.02752 - acc: 0.9991 -- iter: 55/55\n","--\n","Training Step: 9879  | total loss: \u001b[1m\u001b[32m0.02499\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 899 | loss: 0.02499 - acc: 0.9992 -- iter: 05/55\n","Training Step: 9880  | total loss: \u001b[1m\u001b[32m0.02310\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 899 | loss: 0.02310 - acc: 0.9992 -- iter: 10/55\n","Training Step: 9881  | total loss: \u001b[1m\u001b[32m0.01956\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 899 | loss: 0.01956 - acc: 0.9993 -- iter: 15/55\n","Training Step: 9882  | total loss: \u001b[1m\u001b[32m0.01847\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 899 | loss: 0.01847 - acc: 0.9994 -- iter: 20/55\n","Training Step: 9883  | total loss: \u001b[1m\u001b[32m0.01763\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 899 | loss: 0.01763 - acc: 0.9994 -- iter: 25/55\n","Training Step: 9884  | total loss: \u001b[1m\u001b[32m0.01714\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 899 | loss: 0.01714 - acc: 0.9995 -- iter: 30/55\n","Training Step: 9885  | total loss: \u001b[1m\u001b[32m0.01634\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 899 | loss: 0.01634 - acc: 0.9995 -- iter: 35/55\n","Training Step: 9886  | total loss: \u001b[1m\u001b[32m0.01501\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 899 | loss: 0.01501 - acc: 0.9996 -- iter: 40/55\n","Training Step: 9887  | total loss: \u001b[1m\u001b[32m0.01407\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 899 | loss: 0.01407 - acc: 0.9996 -- iter: 45/55\n","Training Step: 9888  | total loss: \u001b[1m\u001b[32m0.01407\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 899 | loss: 0.01407 - acc: 0.9997 -- iter: 50/55\n","Training Step: 9889  | total loss: \u001b[1m\u001b[32m0.01321\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 899 | loss: 0.01321 - acc: 0.9997 -- iter: 55/55\n","--\n","Training Step: 9890  | total loss: \u001b[1m\u001b[32m0.01195\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 900 | loss: 0.01195 - acc: 0.9997 -- iter: 05/55\n","Training Step: 9891  | total loss: \u001b[1m\u001b[32m0.66568\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 900 | loss: 0.66568 - acc: 0.9598 -- iter: 10/55\n","Training Step: 9892  | total loss: \u001b[1m\u001b[32m0.60014\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 900 | loss: 0.60014 - acc: 0.9638 -- iter: 15/55\n","Training Step: 9893  | total loss: \u001b[1m\u001b[32m0.54060\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 900 | loss: 0.54060 - acc: 0.9638 -- iter: 20/55\n","Training Step: 9894  | total loss: \u001b[1m\u001b[32m0.48756\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 900 | loss: 0.48756 - acc: 0.9674 -- iter: 25/55\n","Training Step: 9895  | total loss: \u001b[1m\u001b[32m0.43931\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 900 | loss: 0.43931 - acc: 0.9707 -- iter: 30/55\n","Training Step: 9896  | total loss: \u001b[1m\u001b[32m0.39585\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 900 | loss: 0.39585 - acc: 0.9736 -- iter: 35/55\n","Training Step: 9897  | total loss: \u001b[1m\u001b[32m0.35703\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 900 | loss: 0.35703 - acc: 0.9762 -- iter: 40/55\n","Training Step: 9898  | total loss: \u001b[1m\u001b[32m0.32183\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 900 | loss: 0.32183 - acc: 0.9786 -- iter: 45/55\n","Training Step: 9899  | total loss: \u001b[1m\u001b[32m0.29033\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 900 | loss: 0.29033 - acc: 0.9827 -- iter: 50/55\n","Training Step: 9900  | total loss: \u001b[1m\u001b[32m0.26198\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 900 | loss: 0.26198 - acc: 0.9844 -- iter: 55/55\n","--\n","Training Step: 9901  | total loss: \u001b[1m\u001b[32m0.23756\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 901 | loss: 0.23756 - acc: 0.9860 -- iter: 05/55\n","Training Step: 9902  | total loss: \u001b[1m\u001b[32m0.21439\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 901 | loss: 0.21439 - acc: 0.9874 -- iter: 10/55\n","Training Step: 9903  | total loss: \u001b[1m\u001b[32m2.49553\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 901 | loss: 2.49553 - acc: 0.8886 -- iter: 15/55\n","Training Step: 9904  | total loss: \u001b[1m\u001b[32m2.24677\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 901 | loss: 2.24677 - acc: 0.8998 -- iter: 20/55\n","Training Step: 9905  | total loss: \u001b[1m\u001b[32m2.02411\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 901 | loss: 2.02411 - acc: 0.9098 -- iter: 25/55\n","Training Step: 9906  | total loss: \u001b[1m\u001b[32m1.82225\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 901 | loss: 1.82225 - acc: 0.9188 -- iter: 30/55\n","Training Step: 9907  | total loss: \u001b[1m\u001b[32m1.64134\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 901 | loss: 1.64134 - acc: 0.9269 -- iter: 35/55\n","Training Step: 9986  | total loss: \u001b[1m\u001b[32m0.50516\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 908 | loss: 0.50516 - acc: 0.9683 -- iter: 45/55\n","Training Step: 9987  | total loss: \u001b[1m\u001b[32m0.45499\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 908 | loss: 0.45499 - acc: 0.9715 -- iter: 50/55\n","Training Step: 9988  | total loss: \u001b[1m\u001b[32m1.88748\u001b[0m\u001b[0m | time: 0.060s\n","| Adam | epoch: 908 | loss: 1.88748 - acc: 0.8743 -- iter: 55/55\n","--\n","Training Step: 9989  | total loss: \u001b[1m\u001b[32m1.69976\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 909 | loss: 1.69976 - acc: 0.8982 -- iter: 05/55\n","Training Step: 9990  | total loss: \u001b[1m\u001b[32m1.53117\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 909 | loss: 1.53117 - acc: 0.9084 -- iter: 10/55\n","Training Step: 9991  | total loss: \u001b[1m\u001b[32m1.37875\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 909 | loss: 1.37875 - acc: 0.9175 -- iter: 15/55\n","Training Step: 9992  | total loss: \u001b[1m\u001b[32m1.37875\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 909 | loss: 1.37875 - acc: 0.9175 -- iter: 20/55\n","Training Step: 9993  | total loss: \u001b[1m\u001b[32m1.11757\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 909 | loss: 1.11757 - acc: 0.9332 -- iter: 25/55\n","Training Step: 9994  | total loss: \u001b[1m\u001b[32m1.00695\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 909 | loss: 1.00695 - acc: 0.9399 -- iter: 30/55\n","Training Step: 9995  | total loss: \u001b[1m\u001b[32m1.00695\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 909 | loss: 1.00695 - acc: 0.9459 -- iter: 35/55\n","Training Step: 9996  | total loss: \u001b[1m\u001b[32m0.90819\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 909 | loss: 0.90819 - acc: 0.9513 -- iter: 40/55\n","Training Step: 9997  | total loss: \u001b[1m\u001b[32m0.73818\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 909 | loss: 0.73818 - acc: 0.9562 -- iter: 45/55\n","Training Step: 9998  | total loss: \u001b[1m\u001b[32m0.66459\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 909 | loss: 0.66459 - acc: 0.9606 -- iter: 50/55\n","Training Step: 9999  | total loss: \u001b[1m\u001b[32m0.59938\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 909 | loss: 0.59938 - acc: 0.9645 -- iter: 55/55\n","--\n","Training Step: 10000  | total loss: \u001b[1m\u001b[32m0.54052\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 910 | loss: 0.54052 - acc: 0.9681 -- iter: 05/55\n","Training Step: 10001  | total loss: \u001b[1m\u001b[32m0.48699\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 910 | loss: 0.48699 - acc: 0.9712 -- iter: 10/55\n","Training Step: 10002  | total loss: \u001b[1m\u001b[32m0.48699\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 910 | loss: 0.48699 - acc: 0.9741 -- iter: 15/55\n","Training Step: 10003  | total loss: \u001b[1m\u001b[32m0.43901\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 910 | loss: 0.43901 - acc: 0.9767 -- iter: 20/55\n","Training Step: 10004  | total loss: \u001b[1m\u001b[32m0.39667\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 910 | loss: 0.39667 - acc: 0.9790 -- iter: 25/55\n","Training Step: 10005  | total loss: \u001b[1m\u001b[32m0.35728\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 910 | loss: 0.35728 - acc: 0.9811 -- iter: 30/55\n","Training Step: 10006  | total loss: \u001b[1m\u001b[32m0.32304\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 910 | loss: 0.32304 - acc: 0.9830 -- iter: 35/55\n","Training Step: 10007  | total loss: \u001b[1m\u001b[32m0.29230\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 910 | loss: 0.29230 - acc: 0.9847 -- iter: 40/55\n","Training Step: 10008  | total loss: \u001b[1m\u001b[32m0.26361\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 910 | loss: 0.26361 - acc: 0.9862 -- iter: 45/55\n","Training Step: 10009  | total loss: \u001b[1m\u001b[32m0.23778\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 910 | loss: 0.23778 - acc: 0.9876 -- iter: 50/55\n","Training Step: 10010  | total loss: \u001b[1m\u001b[32m0.21535\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 910 | loss: 0.21535 - acc: 0.9889 -- iter: 55/55\n","--\n","Training Step: 10011  | total loss: \u001b[1m\u001b[32m0.19407\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 911 | loss: 0.19407 - acc: 0.8900 -- iter: 05/55\n","Training Step: 10012  | total loss: \u001b[1m\u001b[32m2.27435\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 911 | loss: 2.27435 - acc: 0.9010 -- iter: 10/55\n","Training Step: 10013  | total loss: \u001b[1m\u001b[32m2.04828\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 911 | loss: 2.04828 - acc: 0.9109 -- iter: 15/55\n","Training Step: 10014  | total loss: \u001b[1m\u001b[32m1.84409\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 911 | loss: 1.84409 - acc: 0.9198 -- iter: 20/55\n","Training Step: 10015  | total loss: \u001b[1m\u001b[32m1.66061\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 911 | loss: 1.66061 - acc: 0.9278 -- iter: 25/55\n","Training Step: 10016  | total loss: \u001b[1m\u001b[32m1.49662\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 911 | loss: 1.49662 - acc: 0.9350 -- iter: 30/55\n","Training Step: 10017  | total loss: \u001b[1m\u001b[32m1.34747\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 911 | loss: 1.34747 - acc: 0.9415 -- iter: 35/55\n","Training Step: 10018  | total loss: \u001b[1m\u001b[32m1.21365\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 911 | loss: 1.21365 - acc: 0.9474 -- iter: 40/55\n","Training Step: 10019  | total loss: \u001b[1m\u001b[32m1.09337\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 911 | loss: 1.09337 - acc: 0.9526 -- iter: 45/55\n","Training Step: 10020  | total loss: \u001b[1m\u001b[32m0.98465\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 911 | loss: 0.98465 - acc: 0.9574 -- iter: 50/55\n","Training Step: 10021  | total loss: \u001b[1m\u001b[32m0.88679\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 911 | loss: 0.88679 - acc: 0.9616 -- iter: 55/55\n","--\n","Training Step: 10022  | total loss: \u001b[1m\u001b[32m0.79915\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 912 | loss: 0.79915 - acc: 0.9655 -- iter: 05/55\n","Training Step: 10023  | total loss: \u001b[1m\u001b[32m0.71965\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 912 | loss: 0.71965 - acc: 0.9089 -- iter: 10/55\n","Training Step: 10024  | total loss: \u001b[1m\u001b[32m2.02956\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 912 | loss: 2.02956 - acc: 0.9180 -- iter: 15/55\n","Training Step: 10025  | total loss: \u001b[1m\u001b[32m1.82786\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 912 | loss: 1.82786 - acc: 0.9262 -- iter: 20/55\n","Training Step: 10026  | total loss: \u001b[1m\u001b[32m1.64609\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 912 | loss: 1.64609 - acc: 0.9336 -- iter: 25/55\n","Training Step: 10027  | total loss: \u001b[1m\u001b[32m1.48235\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 912 | loss: 1.48235 - acc: 0.9402 -- iter: 30/55\n","Training Step: 10028  | total loss: \u001b[1m\u001b[32m1.33470\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 912 | loss: 1.33470 - acc: 0.9462 -- iter: 35/55\n","Training Step: 10029  | total loss: \u001b[1m\u001b[32m1.20197\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 912 | loss: 1.20197 - acc: 0.9516 -- iter: 40/55\n","Training Step: 10030  | total loss: \u001b[1m\u001b[32m1.08289\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 912 | loss: 1.08289 - acc: 0.9516 -- iter: 45/55\n","Training Step: 10031  | total loss: \u001b[1m\u001b[32m0.97505\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 912 | loss: 0.97505 - acc: 0.9564 -- iter: 50/55\n","Training Step: 10032  | total loss: \u001b[1m\u001b[32m0.87900\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 912 | loss: 0.87900 - acc: 0.9608 -- iter: 55/55\n","--\n","Training Step: 10033  | total loss: \u001b[1m\u001b[32m0.71433\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 913 | loss: 0.71433 - acc: 0.9647 -- iter: 05/55\n","Training Step: 10034  | total loss: \u001b[1m\u001b[32m0.64343\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 913 | loss: 0.64343 - acc: 0.9682 -- iter: 10/55\n","Training Step: 10035  | total loss: \u001b[1m\u001b[32m0.57964\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 913 | loss: 0.57964 - acc: 0.9714 -- iter: 15/55\n","Training Step: 10036  | total loss: \u001b[1m\u001b[32m0.52222\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 913 | loss: 0.52222 - acc: 0.9743 -- iter: 20/55\n","Training Step: 10037  | total loss: \u001b[1m\u001b[32m0.47049\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 913 | loss: 0.47049 - acc: 0.9769 -- iter: 25/55\n","Training Step: 10038  | total loss: \u001b[1m\u001b[32m0.42477\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 913 | loss: 0.42477 - acc: 0.9792 -- iter: 30/55\n","Training Step: 10039  | total loss: \u001b[1m\u001b[32m0.38321\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 913 | loss: 0.38321 - acc: 0.9812 -- iter: 35/55\n","Training Step: 10040  | total loss: \u001b[1m\u001b[32m0.34556\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 913 | loss: 0.34556 - acc: 0.9831 -- iter: 40/55\n","Training Step: 10041  | total loss: \u001b[1m\u001b[32m0.31177\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 913 | loss: 0.31177 - acc: 0.9848 -- iter: 45/55\n","Training Step: 10042  | total loss: \u001b[1m\u001b[32m0.28171\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 913 | loss: 0.28171 - acc: 0.9863 -- iter: 50/55\n","Training Step: 10043  | total loss: \u001b[1m\u001b[32m0.25446\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 913 | loss: 0.25446 - acc: 0.9877 -- iter: 55/55\n","--\n","Training Step: 10044  | total loss: \u001b[1m\u001b[32m0.22993\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 914 | loss: 0.22993 - acc: 0.9889 -- iter: 05/55\n","Training Step: 10045  | total loss: \u001b[1m\u001b[32m0.20785\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 914 | loss: 0.20785 - acc: 0.9900 -- iter: 10/55\n","Training Step: 10046  | total loss: \u001b[1m\u001b[32m0.18790\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 914 | loss: 0.18790 - acc: 0.9910 -- iter: 15/55\n","Training Step: 10047  | total loss: \u001b[1m\u001b[32m1.61531\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 914 | loss: 1.61531 - acc: 0.9919 -- iter: 20/55\n","Training Step: 10048  | total loss: \u001b[1m\u001b[32m1.45401\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 914 | loss: 1.45401 - acc: 0.9127 -- iter: 25/55\n","Training Step: 10049  | total loss: \u001b[1m\u001b[32m1.45401\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 914 | loss: 1.45401 - acc: 0.9215 -- iter: 30/55\n","Training Step: 10050  | total loss: \u001b[1m\u001b[32m1.30944\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 914 | loss: 1.30944 - acc: 0.9364 -- iter: 35/55\n","Training Step: 10051  | total loss: \u001b[1m\u001b[32m1.17940\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 914 | loss: 1.17940 - acc: 0.9427 -- iter: 40/55\n","Training Step: 10052  | total loss: \u001b[1m\u001b[32m1.06317\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 914 | loss: 1.06317 - acc: 0.9485 -- iter: 45/55\n","Training Step: 10053  | total loss: \u001b[1m\u001b[32m0.95744\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 914 | loss: 0.95744 - acc: 0.9536 -- iter: 50/55\n","Training Step: 10054  | total loss: \u001b[1m\u001b[32m0.86196\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 914 | loss: 0.86196 - acc: 0.9583 -- iter: 55/55\n","--\n","Training Step: 10055  | total loss: \u001b[1m\u001b[32m0.77660\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 915 | loss: 0.77660 - acc: 0.9624 -- iter: 05/55\n","Training Step: 10056  | total loss: \u001b[1m\u001b[32m0.69967\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 915 | loss: 0.69967 - acc: 0.9662 -- iter: 10/55\n","Training Step: 10057  | total loss: \u001b[1m\u001b[32m0.63042\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 915 | loss: 0.63042 - acc: 0.9662 -- iter: 15/55\n","Training Step: 10058  | total loss: \u001b[1m\u001b[32m0.56774\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 915 | loss: 0.56774 - acc: 0.9696 -- iter: 20/55\n","Training Step: 10059  | total loss: \u001b[1m\u001b[32m0.51277\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 915 | loss: 0.51277 - acc: 0.9726 -- iter: 25/55\n","Training Step: 10060  | total loss: \u001b[1m\u001b[32m1.06442\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 915 | loss: 1.06442 - acc: 0.9154 -- iter: 30/55\n","Training Step: 10061  | total loss: \u001b[1m\u001b[32m0.95878\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 915 | loss: 0.95878 - acc: 0.9238 -- iter: 35/55\n","Training Step: 10062  | total loss: \u001b[1m\u001b[32m0.86318\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 915 | loss: 0.86318 - acc: 0.9314 -- iter: 40/55\n","Training Step: 10063  | total loss: \u001b[1m\u001b[32m0.77782\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 915 | loss: 0.77782 - acc: 0.9383 -- iter: 45/55\n","Training Step: 10064  | total loss: \u001b[1m\u001b[32m0.70119\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 915 | loss: 0.70119 - acc: 0.9500 -- iter: 50/55\n","Training Step: 10065  | total loss: \u001b[1m\u001b[32m0.63112\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 915 | loss: 0.63112 - acc: 0.9550 -- iter: 55/55\n","--\n","Training Step: 10066  | total loss: \u001b[1m\u001b[32m0.56914\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 916 | loss: 0.56914 - acc: 0.9595 -- iter: 05/55\n","Training Step: 10067  | total loss: \u001b[1m\u001b[32m0.51330\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 916 | loss: 0.51330 - acc: 0.9636 -- iter: 10/55\n","Training Step: 10068  | total loss: \u001b[1m\u001b[32m0.46355\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 916 | loss: 0.46355 - acc: 0.9672 -- iter: 15/55\n","Training Step: 10069  | total loss: \u001b[1m\u001b[32m0.41879\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 916 | loss: 0.41879 - acc: 0.9672 -- iter: 20/55\n","Training Step: 10070  | total loss: \u001b[1m\u001b[32m0.37771\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 916 | loss: 0.37771 - acc: 0.9705 -- iter: 25/55\n","Training Step: 10071  | total loss: \u001b[1m\u001b[32m0.34196\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 916 | loss: 0.34196 - acc: 0.9734 -- iter: 30/55\n","Training Step: 10072  | total loss: \u001b[1m\u001b[32m2.13920\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 916 | loss: 2.13920 - acc: 0.8961 -- iter: 35/55\n","Training Step: 10073  | total loss: \u001b[1m\u001b[32m1.92596\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 916 | loss: 1.92596 - acc: 0.9065 -- iter: 40/55\n","Training Step: 10074  | total loss: \u001b[1m\u001b[32m1.73514\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 916 | loss: 1.73514 - acc: 0.9158 -- iter: 45/55\n","Training Step: 10075  | total loss: \u001b[1m\u001b[32m1.56268\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 916 | loss: 1.56268 - acc: 0.9243 -- iter: 50/55\n","Training Step: 10076  | total loss: \u001b[1m\u001b[32m1.40735\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 916 | loss: 1.40735 - acc: 0.9318 -- iter: 55/55\n","--\n","Training Step: 10077  | total loss: \u001b[1m\u001b[32m1.26725\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 917 | loss: 1.26725 - acc: 0.9386 -- iter: 05/55\n","Training Step: 10078  | total loss: \u001b[1m\u001b[32m1.14102\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 917 | loss: 1.14102 - acc: 0.9503 -- iter: 10/55\n","Training Step: 10079  | total loss: \u001b[1m\u001b[32m1.02774\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 917 | loss: 1.02774 - acc: 0.9553 -- iter: 15/55\n","Training Step: 10080  | total loss: \u001b[1m\u001b[32m0.92602\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 917 | loss: 0.92602 - acc: 0.9597 -- iter: 20/55\n","Training Step: 10081  | total loss: \u001b[1m\u001b[32m0.83449\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 917 | loss: 0.83449 - acc: 0.9638 -- iter: 25/55\n","Training Step: 10082  | total loss: \u001b[1m\u001b[32m0.75204\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 917 | loss: 0.75204 - acc: 0.9674 -- iter: 30/55\n","Training Step: 10083  | total loss: \u001b[1m\u001b[32m0.67799\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 917 | loss: 0.67799 - acc: 0.8707 -- iter: 35/55\n","Training Step: 10084  | total loss: \u001b[1m\u001b[32m2.54514\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 917 | loss: 2.54514 - acc: 0.8836 -- iter: 40/55\n","Training Step: 10085  | total loss: \u001b[1m\u001b[32m2.29121\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 917 | loss: 2.29121 - acc: 0.8952 -- iter: 45/55\n","Training Step: 10086  | total loss: \u001b[1m\u001b[32m2.06261\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 917 | loss: 2.06261 - acc: 0.9057 -- iter: 50/55\n","Training Step: 10087  | total loss: \u001b[1m\u001b[32m1.85732\u001b[0m\u001b[0m | time: 0.060s\n","| Adam | epoch: 917 | loss: 1.85732 - acc: 0.9151 -- iter: 55/55\n","--\n","Training Step: 10088  | total loss: \u001b[1m\u001b[32m1.67271\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 918 | loss: 1.67271 - acc: 0.9236 -- iter: 05/55\n","Training Step: 10089  | total loss: \u001b[1m\u001b[32m1.50606\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 918 | loss: 1.50606 - acc: 0.9313 -- iter: 10/55\n","Training Step: 10090  | total loss: \u001b[1m\u001b[32m1.35584\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 918 | loss: 1.35584 - acc: 0.9381 -- iter: 15/55\n","Training Step: 10091  | total loss: \u001b[1m\u001b[32m1.22100\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 918 | loss: 1.22100 - acc: 0.9443 -- iter: 20/55\n","Training Step: 10092  | total loss: \u001b[1m\u001b[32m1.10077\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 918 | loss: 1.10077 - acc: 0.9443 -- iter: 25/55\n","Training Step: 10093  | total loss: \u001b[1m\u001b[32m0.99255\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 918 | loss: 0.99255 - acc: 0.9499 -- iter: 30/55\n","Training Step: 10094  | total loss: \u001b[1m\u001b[32m0.80616\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 918 | loss: 0.80616 - acc: 0.9549 -- iter: 35/55\n","Training Step: 10095  | total loss: \u001b[1m\u001b[32m1.96856\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 918 | loss: 1.96856 - acc: 0.9594 -- iter: 40/55\n","Training Step: 10096  | total loss: \u001b[1m\u001b[32m1.77304\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 918 | loss: 1.77304 - acc: 0.8835 -- iter: 45/55\n","Training Step: 10097  | total loss: \u001b[1m\u001b[32m1.59729\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 918 | loss: 1.59729 - acc: 0.8951 -- iter: 50/55\n","Training Step: 10098  | total loss: \u001b[1m\u001b[32m1.59729\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 918 | loss: 1.59729 - acc: 0.9150 -- iter: 55/55\n","--\n","Training Step: 10099  | total loss: \u001b[1m\u001b[32m1.43896\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 919 | loss: 1.43896 - acc: 0.9235 -- iter: 05/55\n","Training Step: 10100  | total loss: \u001b[1m\u001b[32m1.29543\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 919 | loss: 1.29543 - acc: 0.9312 -- iter: 10/55\n","Training Step: 10101  | total loss: \u001b[1m\u001b[32m1.16737\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 919 | loss: 1.16737 - acc: 0.9381 -- iter: 15/55\n","Training Step: 10102  | total loss: \u001b[1m\u001b[32m1.05103\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 919 | loss: 1.05103 - acc: 0.9443 -- iter: 20/55\n","Training Step: 10103  | total loss: \u001b[1m\u001b[32m0.85229\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 919 | loss: 0.85229 - acc: 0.9443 -- iter: 25/55\n","Training Step: 10104  | total loss: \u001b[1m\u001b[32m0.76743\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 919 | loss: 0.76743 - acc: 0.9498 -- iter: 30/55\n","Training Step: 10105  | total loss: \u001b[1m\u001b[32m0.76743\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 919 | loss: 0.76743 - acc: 0.9549 -- iter: 35/55\n","Training Step: 10106  | total loss: \u001b[1m\u001b[32m0.69221\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 919 | loss: 0.69221 - acc: 0.9594 -- iter: 40/55\n","Training Step: 10107  | total loss: \u001b[1m\u001b[32m0.62397\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 919 | loss: 0.62397 - acc: 0.9634 -- iter: 45/55\n","Training Step: 10108  | total loss: \u001b[1m\u001b[32m2.30069\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 919 | loss: 2.30069 - acc: 0.8671 -- iter: 50/55\n","Training Step: 10109  | total loss: \u001b[1m\u001b[32m2.07292\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 919 | loss: 2.07292 - acc: 0.8804 -- iter: 55/55\n","--\n","Training Step: 10110  | total loss: \u001b[1m\u001b[32m1.86658\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 920 | loss: 1.86658 - acc: 0.8923 -- iter: 05/55\n","Training Step: 10111  | total loss: \u001b[1m\u001b[32m1.68195\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 920 | loss: 1.68195 - acc: 0.9031 -- iter: 10/55\n","Training Step: 10112  | total loss: \u001b[1m\u001b[32m1.51396\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 920 | loss: 1.51396 - acc: 0.9128 -- iter: 15/55\n","Training Step: 10113  | total loss: \u001b[1m\u001b[32m1.23012\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 920 | loss: 1.23012 - acc: 0.9215 -- iter: 20/55\n","Training Step: 10114  | total loss: \u001b[1m\u001b[32m1.11032\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 920 | loss: 1.11032 - acc: 0.9294 -- iter: 25/55\n","Training Step: 10115  | total loss: \u001b[1m\u001b[32m1.11032\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 920 | loss: 1.11032 - acc: 0.9364 -- iter: 30/55\n","Training Step: 10116  | total loss: \u001b[1m\u001b[32m0.90033\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 920 | loss: 0.90033 - acc: 0.9428 -- iter: 35/55\n","Training Step: 10117  | total loss: \u001b[1m\u001b[32m0.90033\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 920 | loss: 0.90033 - acc: 0.9485 -- iter: 40/55\n","Training Step: 10118  | total loss: \u001b[1m\u001b[32m0.81081\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 920 | loss: 0.81081 - acc: 0.9537 -- iter: 45/55\n","Training Step: 10119  | total loss: \u001b[1m\u001b[32m0.73093\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 920 | loss: 0.73093 - acc: 0.9583 -- iter: 50/55\n","Training Step: 10120  | total loss: \u001b[1m\u001b[32m2.61686\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 920 | loss: 2.61686 - acc: 0.8625 -- iter: 55/55\n","--\n","Training Step: 10121  | total loss: \u001b[1m\u001b[32m2.36242\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 921 | loss: 2.36242 - acc: 0.8762 -- iter: 05/55\n","Training Step: 10122  | total loss: \u001b[1m\u001b[32m2.12644\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 921 | loss: 2.12644 - acc: 0.8886 -- iter: 10/55\n","Training Step: 10123  | total loss: \u001b[1m\u001b[32m1.91476\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 921 | loss: 1.91476 - acc: 0.8997 -- iter: 15/55\n","Training Step: 10124  | total loss: \u001b[1m\u001b[32m1.72421\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 921 | loss: 1.72421 - acc: 0.9098 -- iter: 20/55\n","Training Step: 10125  | total loss: \u001b[1m\u001b[32m1.55238\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 921 | loss: 1.55238 - acc: 0.9188 -- iter: 25/55\n","Training Step: 10126  | total loss: \u001b[1m\u001b[32m1.25932\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 921 | loss: 1.25932 - acc: 0.9269 -- iter: 30/55\n","Training Step: 10127  | total loss: \u001b[1m\u001b[32m1.13447\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 921 | loss: 1.13447 - acc: 0.9342 -- iter: 35/55\n","Training Step: 10128  | total loss: \u001b[1m\u001b[32m1.02213\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 921 | loss: 1.02213 - acc: 0.9467 -- iter: 40/55\n","Training Step: 10129  | total loss: \u001b[1m\u001b[32m0.92015\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 921 | loss: 0.92015 - acc: 0.9467 -- iter: 45/55\n","Training Step: 10130  | total loss: \u001b[1m\u001b[32m0.82911\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 921 | loss: 0.82911 - acc: 0.9520 -- iter: 50/55\n","Training Step: 10131  | total loss: \u001b[1m\u001b[32m1.93669\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 921 | loss: 1.93669 - acc: 0.9568 -- iter: 55/55\n","--\n","Training Step: 10132  | total loss: \u001b[1m\u001b[32m1.93669\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 922 | loss: 1.93669 - acc: 0.8812 -- iter: 05/55\n","Training Step: 10133  | total loss: \u001b[1m\u001b[32m1.74426\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 922 | loss: 1.74426 - acc: 0.8930 -- iter: 10/55\n","Training Step: 10134  | total loss: \u001b[1m\u001b[32m1.41416\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 922 | loss: 1.41416 - acc: 0.9037 -- iter: 15/55\n","Training Step: 10135  | total loss: \u001b[1m\u001b[32m1.27406\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 922 | loss: 1.27406 - acc: 0.9134 -- iter: 20/55\n","Training Step: 10136  | total loss: \u001b[1m\u001b[32m1.14769\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 922 | loss: 1.14769 - acc: 0.9220 -- iter: 25/55\n","Training Step: 10137  | total loss: \u001b[1m\u001b[32m1.03388\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 922 | loss: 1.03388 - acc: 0.9298 -- iter: 30/55\n","Training Step: 10138  | total loss: \u001b[1m\u001b[32m0.93254\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 922 | loss: 0.93254 - acc: 0.9368 -- iter: 35/55\n","Training Step: 10139  | total loss: \u001b[1m\u001b[32m0.83984\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 922 | loss: 0.83984 - acc: 0.9432 -- iter: 40/55\n","Training Step: 10140  | total loss: \u001b[1m\u001b[32m0.75641\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 922 | loss: 0.75641 - acc: 0.9488 -- iter: 45/55\n","Training Step: 10141  | total loss: \u001b[1m\u001b[32m0.68164\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 922 | loss: 0.68164 - acc: 0.9540 -- iter: 50/55\n","Training Step: 10142  | total loss: \u001b[1m\u001b[32m0.61450\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 922 | loss: 0.61450 - acc: 0.9586 -- iter: 55/55\n","--\n","Training Step: 10143  | total loss: \u001b[1m\u001b[32m2.68688\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 923 | loss: 2.68688 - acc: 0.9627 -- iter: 05/55\n","Training Step: 10144  | total loss: \u001b[1m\u001b[32m2.41900\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 923 | loss: 2.41900 - acc: 0.8664 -- iter: 10/55\n","Training Step: 10145  | total loss: \u001b[1m\u001b[32m2.17859\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 923 | loss: 2.17859 - acc: 0.8798 -- iter: 15/55\n","Training Step: 10146  | total loss: \u001b[1m\u001b[32m1.96199\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 923 | loss: 1.96199 - acc: 0.8918 -- iter: 20/55\n","Training Step: 10147  | total loss: \u001b[1m\u001b[32m1.76643\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 923 | loss: 1.76643 - acc: 0.9026 -- iter: 25/55\n","Training Step: 10148  | total loss: \u001b[1m\u001b[32m1.59046\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 923 | loss: 1.59046 - acc: 0.9124 -- iter: 30/55\n","Training Step: 10149  | total loss: \u001b[1m\u001b[32m1.43265\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 923 | loss: 1.43265 - acc: 0.9290 -- iter: 35/55\n","Training Step: 10150  | total loss: \u001b[1m\u001b[32m1.29186\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 923 | loss: 1.29186 - acc: 0.9361 -- iter: 40/55\n","Training Step: 10151  | total loss: \u001b[1m\u001b[32m1.16274\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 923 | loss: 1.16274 - acc: 0.9425 -- iter: 45/55\n","Training Step: 10152  | total loss: \u001b[1m\u001b[32m1.04654\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 923 | loss: 1.04654 - acc: 0.9483 -- iter: 50/55\n","Training Step: 10153  | total loss: \u001b[1m\u001b[32m0.94237\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 923 | loss: 0.94237 - acc: 0.9534 -- iter: 55/55\n","--\n","Training Step: 10154  | total loss: \u001b[1m\u001b[32m0.84936\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 924 | loss: 0.84936 - acc: 0.9581 -- iter: 05/55\n","Training Step: 10155  | total loss: \u001b[1m\u001b[32m0.76599\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 924 | loss: 0.76599 - acc: 0.9623 -- iter: 10/55\n","Training Step: 10156  | total loss: \u001b[1m\u001b[32m0.69077\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 924 | loss: 0.69077 - acc: 0.9623 -- iter: 15/55\n","Training Step: 10157  | total loss: \u001b[1m\u001b[32m0.62282\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 924 | loss: 0.62282 - acc: 0.9660 -- iter: 20/55\n","Training Step: 10158  | total loss: \u001b[1m\u001b[32m0.62282\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 924 | loss: 0.62282 - acc: 0.9694 -- iter: 25/55\n","Training Step: 10159  | total loss: \u001b[1m\u001b[32m0.56144\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 924 | loss: 0.56144 - acc: 0.9725 -- iter: 30/55\n","Training Step: 10160  | total loss: \u001b[1m\u001b[32m0.50626\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 924 | loss: 0.50626 - acc: 0.9753 -- iter: 35/55\n","Training Step: 10161  | total loss: \u001b[1m\u001b[32m0.45588\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 924 | loss: 0.45588 - acc: 0.9777 -- iter: 40/55\n","Training Step: 10162  | total loss: \u001b[1m\u001b[32m0.41175\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 924 | loss: 0.41175 - acc: 0.9800 -- iter: 45/55\n","Training Step: 10163  | total loss: \u001b[1m\u001b[32m0.37135\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 924 | loss: 0.37135 - acc: 0.9820 -- iter: 50/55\n","Training Step: 10164  | total loss: \u001b[1m\u001b[32m0.33452\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 924 | loss: 0.33452 - acc: 0.9838 -- iter: 55/55\n","--\n","Training Step: 10165  | total loss: \u001b[1m\u001b[32m0.30137\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 925 | loss: 0.30137 - acc: 0.9868 -- iter: 05/55\n","Training Step: 10166  | total loss: \u001b[1m\u001b[32m0.24595\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 925 | loss: 0.24595 - acc: 0.9882 -- iter: 10/55\n","Training Step: 10167  | total loss: \u001b[1m\u001b[32m0.22460\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 925 | loss: 0.22460 - acc: 0.9893 -- iter: 15/55\n","Training Step: 10168  | total loss: \u001b[1m\u001b[32m0.20385\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 925 | loss: 0.20385 - acc: 0.9904 -- iter: 20/55\n","Training Step: 10169  | total loss: \u001b[1m\u001b[32m0.18498\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 925 | loss: 0.18498 - acc: 0.9914 -- iter: 25/55\n","Training Step: 10170  | total loss: \u001b[1m\u001b[32m0.16748\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 925 | loss: 0.16748 - acc: 0.9922 -- iter: 30/55\n","Training Step: 10171  | total loss: \u001b[1m\u001b[32m0.15228\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 925 | loss: 0.15228 - acc: 0.9922 -- iter: 35/55\n","Training Step: 10172  | total loss: \u001b[1m\u001b[32m0.13778\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 925 | loss: 0.13778 - acc: 0.9930 -- iter: 40/55\n","Training Step: 10173  | total loss: \u001b[1m\u001b[32m0.12513\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 925 | loss: 0.12513 - acc: 0.9937 -- iter: 45/55\n","Training Step: 10174  | total loss: \u001b[1m\u001b[32m0.11398\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 925 | loss: 0.11398 - acc: 0.9943 -- iter: 50/55\n","Training Step: 10175  | total loss: \u001b[1m\u001b[32m0.10317\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 925 | loss: 0.10317 - acc: 0.9949 -- iter: 55/55\n","--\n","Training Step: 10176  | total loss: \u001b[1m\u001b[32m0.09344\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 926 | loss: 0.09344 - acc: 0.9954 -- iter: 05/55\n","Training Step: 10177  | total loss: \u001b[1m\u001b[32m0.08467\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 926 | loss: 0.08467 - acc: 0.9959 -- iter: 10/55\n","Training Step: 10178  | total loss: \u001b[1m\u001b[32m0.07759\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 926 | loss: 0.07759 - acc: 0.9963 -- iter: 15/55\n","Training Step: 10179  | total loss: \u001b[1m\u001b[32m0.07051\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 926 | loss: 0.07051 - acc: 0.9970 -- iter: 20/55\n","Training Step: 10180  | total loss: \u001b[1m\u001b[32m0.06447\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 926 | loss: 0.06447 - acc: 0.9973 -- iter: 25/55\n","Training Step: 10181  | total loss: \u001b[1m\u001b[32m0.05858\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 926 | loss: 0.05858 - acc: 0.9976 -- iter: 30/55\n","Training Step: 10182  | total loss: \u001b[1m\u001b[32m0.05400\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 926 | loss: 0.05400 - acc: 0.9978 -- iter: 35/55\n","Training Step: 10183  | total loss: \u001b[1m\u001b[32m0.04957\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 926 | loss: 0.04957 - acc: 0.9980 -- iter: 40/55\n","Training Step: 10184  | total loss: \u001b[1m\u001b[32m0.04635\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 926 | loss: 0.04635 - acc: 0.9982 -- iter: 45/55\n","Training Step: 10185  | total loss: \u001b[1m\u001b[32m0.04243\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 926 | loss: 0.04243 - acc: 0.9982 -- iter: 50/55\n","Training Step: 10186  | total loss: \u001b[1m\u001b[32m0.04243\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 926 | loss: 0.04243 - acc: 0.9986 -- iter: 55/55\n","--\n","Training Step: 10187  | total loss: \u001b[1m\u001b[32m0.03835\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 927 | loss: 0.03835 - acc: 0.9987 -- iter: 05/55\n","Training Step: 10188  | total loss: \u001b[1m\u001b[32m0.03523\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 927 | loss: 0.03523 - acc: 0.9987 -- iter: 10/55\n","Training Step: 10189  | total loss: \u001b[1m\u001b[32m0.03241\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 927 | loss: 0.03241 - acc: 0.9990 -- iter: 15/55\n","Training Step: 10190  | total loss: \u001b[1m\u001b[32m0.03083\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 927 | loss: 0.03083 - acc: 0.9991 -- iter: 20/55\n","Training Step: 10191  | total loss: \u001b[1m\u001b[32m0.02860\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 927 | loss: 0.02860 - acc: 0.8992 -- iter: 25/55\n","Training Step: 10192  | total loss: \u001b[1m\u001b[32m1.24075\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 927 | loss: 1.24075 - acc: 0.9092 -- iter: 30/55\n","Training Step: 10193  | total loss: \u001b[1m\u001b[32m1.11711\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 927 | loss: 1.11711 - acc: 0.9183 -- iter: 35/55\n","Training Step: 10194  | total loss: \u001b[1m\u001b[32m1.00650\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 927 | loss: 1.00650 - acc: 0.9265 -- iter: 40/55\n","Training Step: 10195  | total loss: \u001b[1m\u001b[32m0.90618\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 927 | loss: 0.90618 - acc: 0.9265 -- iter: 45/55\n","Training Step: 10196  | total loss: \u001b[1m\u001b[32m0.81602\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 927 | loss: 0.81602 - acc: 0.9338 -- iter: 50/55\n","Training Step: 10197  | total loss: \u001b[1m\u001b[32m0.73573\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 927 | loss: 0.73573 - acc: 0.9404 -- iter: 55/55\n","--\n","Training Step: 10198  | total loss: \u001b[1m\u001b[32m0.66313\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 928 | loss: 0.66313 - acc: 0.9464 -- iter: 05/55\n","Training Step: 10199  | total loss: \u001b[1m\u001b[32m0.59894\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 928 | loss: 0.59894 - acc: 0.9518 -- iter: 10/55\n","Training Step: 10200  | total loss: \u001b[1m\u001b[32m0.54008\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 928 | loss: 0.54008 - acc: 0.9566 -- iter: 15/55\n","Training Step: 10201  | total loss: \u001b[1m\u001b[32m0.48709\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 928 | loss: 0.48709 - acc: 0.9609 -- iter: 20/55\n","Training Step: 10202  | total loss: \u001b[1m\u001b[32m0.43931\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 928 | loss: 0.43931 - acc: 0.9684 -- iter: 25/55\n","Training Step: 10203  | total loss: \u001b[1m\u001b[32m0.39674\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 928 | loss: 0.39674 - acc: 0.8715 -- iter: 30/55\n","Training Step: 10204  | total loss: \u001b[1m\u001b[32m2.11137\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 928 | loss: 2.11137 - acc: 0.8844 -- iter: 35/55\n","Training Step: 10205  | total loss: \u001b[1m\u001b[32m1.90079\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 928 | loss: 1.90079 - acc: 0.8959 -- iter: 40/55\n","Training Step: 10206  | total loss: \u001b[1m\u001b[32m1.71223\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 928 | loss: 1.71223 - acc: 0.9063 -- iter: 45/55\n","Training Step: 10207  | total loss: \u001b[1m\u001b[32m1.54129\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 928 | loss: 1.54129 - acc: 0.9157 -- iter: 50/55\n","Training Step: 10208  | total loss: \u001b[1m\u001b[32m1.38759\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 928 | loss: 1.38759 - acc: 0.9241 -- iter: 55/55\n","--\n","Training Step: 10209  | total loss: \u001b[1m\u001b[32m1.24948\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 929 | loss: 1.24948 - acc: 0.9317 -- iter: 05/55\n","Training Step: 10210  | total loss: \u001b[1m\u001b[32m1.12550\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 929 | loss: 1.12550 - acc: 0.9385 -- iter: 10/55\n","Training Step: 10211  | total loss: \u001b[1m\u001b[32m1.01491\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 929 | loss: 1.01491 - acc: 0.9447 -- iter: 15/55\n","Training Step: 10212  | total loss: \u001b[1m\u001b[32m0.91403\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 929 | loss: 0.91403 - acc: 0.9502 -- iter: 20/55\n","Training Step: 10213  | total loss: \u001b[1m\u001b[32m0.82324\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 929 | loss: 0.82324 - acc: 0.9552 -- iter: 25/55\n","Training Step: 10214  | total loss: \u001b[1m\u001b[32m0.74238\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 929 | loss: 0.74238 - acc: 0.9597 -- iter: 30/55\n","Training Step: 10215  | total loss: \u001b[1m\u001b[32m0.67092\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 929 | loss: 0.67092 - acc: 0.8837 -- iter: 35/55\n","Training Step: 10216  | total loss: \u001b[1m\u001b[32m2.24232\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 929 | loss: 2.24232 - acc: 0.8953 -- iter: 40/55\n","Training Step: 10217  | total loss: \u001b[1m\u001b[32m2.01972\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 929 | loss: 2.01972 - acc: 0.9058 -- iter: 45/55\n","Training Step: 10218  | total loss: \u001b[1m\u001b[32m1.81814\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 929 | loss: 1.81814 - acc: 0.9152 -- iter: 50/55\n","Training Step: 10219  | total loss: \u001b[1m\u001b[32m1.63759\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 929 | loss: 1.63759 - acc: 0.9237 -- iter: 55/55\n","--\n","Training Step: 10220  | total loss: \u001b[1m\u001b[32m1.47633\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 930 | loss: 1.47633 - acc: 0.9313 -- iter: 05/55\n","Training Step: 10221  | total loss: \u001b[1m\u001b[32m1.32917\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 930 | loss: 1.32917 - acc: 0.9382 -- iter: 10/55\n","Training Step: 10222  | total loss: \u001b[1m\u001b[32m1.07915\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 930 | loss: 1.07915 - acc: 0.9382 -- iter: 15/55\n","Training Step: 10223  | total loss: \u001b[1m\u001b[32m0.97191\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 930 | loss: 0.97191 - acc: 0.9499 -- iter: 20/55\n","Training Step: 10224  | total loss: \u001b[1m\u001b[32m0.87538\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 930 | loss: 0.87538 - acc: 0.9549 -- iter: 25/55\n","Training Step: 10225  | total loss: \u001b[1m\u001b[32m0.87538\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 930 | loss: 0.87538 - acc: 0.9595 -- iter: 30/55\n","Training Step: 10226  | total loss: \u001b[1m\u001b[32m0.78886\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 930 | loss: 0.78886 - acc: 0.9595 -- iter: 35/55\n","Training Step: 10227  | total loss: \u001b[1m\u001b[32m0.71056\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 930 | loss: 0.71056 - acc: 0.9635 -- iter: 40/55\n","Training Step: 10228  | total loss: \u001b[1m\u001b[32m1.67198\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 930 | loss: 1.67198 - acc: 0.8872 -- iter: 45/55\n","Training Step: 10229  | total loss: \u001b[1m\u001b[32m1.50608\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 930 | loss: 1.50608 - acc: 0.8984 -- iter: 50/55\n","Training Step: 10230  | total loss: \u001b[1m\u001b[32m1.22207\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 930 | loss: 1.22207 - acc: 0.9177 -- iter: 55/55\n","--\n","Training Step: 10231  | total loss: \u001b[1m\u001b[32m1.10169\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 931 | loss: 1.10169 - acc: 0.9260 -- iter: 05/55\n","Training Step: 10232  | total loss: \u001b[1m\u001b[32m0.99289\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 931 | loss: 0.99289 - acc: 0.9334 -- iter: 10/55\n","Training Step: 10233  | total loss: \u001b[1m\u001b[32m0.89439\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 931 | loss: 0.89439 - acc: 0.9400 -- iter: 15/55\n","Training Step: 10234  | total loss: \u001b[1m\u001b[32m0.80581\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 931 | loss: 0.80581 - acc: 0.9460 -- iter: 20/55\n","Training Step: 10235  | total loss: \u001b[1m\u001b[32m0.72581\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 931 | loss: 0.72581 - acc: 0.9514 -- iter: 25/55\n","Training Step: 10236  | total loss: \u001b[1m\u001b[32m0.65381\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 931 | loss: 0.65381 - acc: 0.9563 -- iter: 30/55\n","Training Step: 10237  | total loss: \u001b[1m\u001b[32m0.58919\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 931 | loss: 0.58919 - acc: 0.9607 -- iter: 35/55\n","Training Step: 10238  | total loss: \u001b[1m\u001b[32m0.53080\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 931 | loss: 0.53080 - acc: 0.9646 -- iter: 40/55\n","Training Step: 10239  | total loss: \u001b[1m\u001b[32m2.43462\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 931 | loss: 2.43462 - acc: 0.8681 -- iter: 45/55\n","Training Step: 10240  | total loss: \u001b[1m\u001b[32m2.43462\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 931 | loss: 2.43462 - acc: 0.8813 -- iter: 50/55\n","Training Step: 10241  | total loss: \u001b[1m\u001b[32m1.97363\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 931 | loss: 1.97363 - acc: 0.8932 -- iter: 55/55\n","--\n","Training Step: 10242  | total loss: \u001b[1m\u001b[32m1.97363\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 932 | loss: 1.97363 - acc: 0.9039 -- iter: 05/55\n","Training Step: 10243  | total loss: \u001b[1m\u001b[32m1.77829\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 932 | loss: 1.77829 - acc: 0.9135 -- iter: 10/55\n","Training Step: 10244  | total loss: \u001b[1m\u001b[32m1.44162\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 932 | loss: 1.44162 - acc: 0.9221 -- iter: 15/55\n","Training Step: 10245  | total loss: \u001b[1m\u001b[32m1.29822\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 932 | loss: 1.29822 - acc: 0.9299 -- iter: 20/55\n","Training Step: 10246  | total loss: \u001b[1m\u001b[32m1.16883\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 932 | loss: 1.16883 - acc: 0.9369 -- iter: 25/55\n","Training Step: 10247  | total loss: \u001b[1m\u001b[32m1.05337\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 932 | loss: 1.05337 - acc: 0.9432 -- iter: 30/55\n","Training Step: 10382  | total loss: \u001b[1m\u001b[32m0.40557\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 944 | loss: 0.40557 - acc: 0.9564 -- iter: 45/55\n","Training Step: 10383  | total loss: \u001b[1m\u001b[32m0.36623\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 944 | loss: 0.36623 - acc: 0.9564 -- iter: 50/55\n","Training Step: 10384  | total loss: \u001b[1m\u001b[32m1.70128\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 944 | loss: 1.70128 - acc: 0.8607 -- iter: 55/55\n","--\n","Training Step: 10385  | total loss: \u001b[1m\u001b[32m1.53256\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 945 | loss: 1.53256 - acc: 0.8747 -- iter: 05/55\n","Training Step: 10386  | total loss: \u001b[1m\u001b[32m1.38005\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 945 | loss: 1.38005 - acc: 0.8872 -- iter: 10/55\n","Training Step: 10387  | total loss: \u001b[1m\u001b[32m1.24270\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 945 | loss: 1.24270 - acc: 0.8985 -- iter: 15/55\n","Training Step: 10388  | total loss: \u001b[1m\u001b[32m1.12052\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 945 | loss: 1.12052 - acc: 0.9086 -- iter: 20/55\n","Training Step: 10389  | total loss: \u001b[1m\u001b[32m1.01032\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 945 | loss: 1.01032 - acc: 0.9178 -- iter: 25/55\n","Training Step: 10390  | total loss: \u001b[1m\u001b[32m0.91406\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 945 | loss: 0.91406 - acc: 0.9260 -- iter: 30/55\n","Training Step: 10391  | total loss: \u001b[1m\u001b[32m0.82562\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 945 | loss: 0.82562 - acc: 0.9334 -- iter: 35/55\n","Training Step: 10392  | total loss: \u001b[1m\u001b[32m0.74405\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 945 | loss: 0.74405 - acc: 0.9401 -- iter: 40/55\n","Training Step: 10393  | total loss: \u001b[1m\u001b[32m0.67063\u001b[0m\u001b[0m | time: 0.065s\n","| Adam | epoch: 945 | loss: 0.67063 - acc: 0.9460 -- iter: 45/55\n","Training Step: 10394  | total loss: \u001b[1m\u001b[32m0.60410\u001b[0m\u001b[0m | time: 0.072s\n","| Adam | epoch: 945 | loss: 0.60410 - acc: 0.9514 -- iter: 50/55\n","Training Step: 10395  | total loss: \u001b[1m\u001b[32m0.54532\u001b[0m\u001b[0m | time: 0.077s\n","| Adam | epoch: 945 | loss: 0.54532 - acc: 0.9563 -- iter: 55/55\n","--\n","Training Step: 10396  | total loss: \u001b[1m\u001b[32m1.10368\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 946 | loss: 1.10368 - acc: 0.9007 -- iter: 05/55\n","Training Step: 10397  | total loss: \u001b[1m\u001b[32m0.99502\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 946 | loss: 0.99502 - acc: 0.9106 -- iter: 10/55\n","Training Step: 10398  | total loss: \u001b[1m\u001b[32m0.89808\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 946 | loss: 0.89808 - acc: 0.9195 -- iter: 15/55\n","Training Step: 10399  | total loss: \u001b[1m\u001b[32m0.80969\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 946 | loss: 0.80969 - acc: 0.9276 -- iter: 20/55\n","Training Step: 10400  | total loss: \u001b[1m\u001b[32m0.73004\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 946 | loss: 0.73004 - acc: 0.9348 -- iter: 25/55\n","Training Step: 10401  | total loss: \u001b[1m\u001b[32m0.65923\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 946 | loss: 0.65923 - acc: 0.9413 -- iter: 30/55\n","Training Step: 10402  | total loss: \u001b[1m\u001b[32m0.59491\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 946 | loss: 0.59491 - acc: 0.9472 -- iter: 35/55\n","Training Step: 10403  | total loss: \u001b[1m\u001b[32m0.53769\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 946 | loss: 0.53769 - acc: 0.9525 -- iter: 40/55\n","Training Step: 10404  | total loss: \u001b[1m\u001b[32m0.48698\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 946 | loss: 0.48698 - acc: 0.9572 -- iter: 45/55\n","Training Step: 10405  | total loss: \u001b[1m\u001b[32m0.44107\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 946 | loss: 0.44107 - acc: 0.9615 -- iter: 50/55\n","Training Step: 10406  | total loss: \u001b[1m\u001b[32m0.39817\u001b[0m\u001b[0m | time: 0.062s\n","| Adam | epoch: 946 | loss: 0.39817 - acc: 0.9654 -- iter: 55/55\n","--\n","Training Step: 10407  | total loss: \u001b[1m\u001b[32m0.36062\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 947 | loss: 0.36062 - acc: 0.9688 -- iter: 05/55\n","Training Step: 10408  | total loss: \u001b[1m\u001b[32m0.32647\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 947 | loss: 0.32647 - acc: 0.9719 -- iter: 10/55\n","Training Step: 10409  | total loss: \u001b[1m\u001b[32m0.29512\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 947 | loss: 0.29512 - acc: 0.9748 -- iter: 15/55\n","Training Step: 10410  | total loss: \u001b[1m\u001b[32m0.26863\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 947 | loss: 0.26863 - acc: 0.9773 -- iter: 20/55\n","Training Step: 10411  | total loss: \u001b[1m\u001b[32m0.24483\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 947 | loss: 0.24483 - acc: 0.9816 -- iter: 25/55\n","Training Step: 10412  | total loss: \u001b[1m\u001b[32m0.22173\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 947 | loss: 0.22173 - acc: 0.9834 -- iter: 30/55\n","Training Step: 10413  | total loss: \u001b[1m\u001b[32m0.20041\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 947 | loss: 0.20041 - acc: 0.9851 -- iter: 35/55\n","Training Step: 10414  | total loss: \u001b[1m\u001b[32m0.18166\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 947 | loss: 0.18166 - acc: 0.9866 -- iter: 40/55\n","Training Step: 10415  | total loss: \u001b[1m\u001b[32m0.16363\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 947 | loss: 0.16363 - acc: 0.9879 -- iter: 45/55\n","Training Step: 10416  | total loss: \u001b[1m\u001b[32m0.14993\u001b[0m\u001b[0m | time: 0.059s\n","| Adam | epoch: 947 | loss: 0.14993 - acc: 0.9879 -- iter: 50/55\n","Training Step: 10417  | total loss: \u001b[1m\u001b[32m0.13758\u001b[0m\u001b[0m | time: 0.067s\n","| Adam | epoch: 947 | loss: 0.13758 - acc: 0.9891 -- iter: 55/55\n","--\n","Training Step: 10418  | total loss: \u001b[1m\u001b[32m0.12610\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 948 | loss: 0.12610 - acc: 0.9902 -- iter: 05/55\n","Training Step: 10419  | total loss: \u001b[1m\u001b[32m0.11410\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 948 | loss: 0.11410 - acc: 0.9912 -- iter: 10/55\n","Training Step: 10420  | total loss: \u001b[1m\u001b[32m0.46480\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 948 | loss: 0.46480 - acc: 0.9321 -- iter: 15/55\n","Training Step: 10421  | total loss: \u001b[1m\u001b[32m0.41970\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 948 | loss: 0.41970 - acc: 0.9389 -- iter: 20/55\n","Training Step: 10422  | total loss: \u001b[1m\u001b[32m0.38067\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 948 | loss: 0.38067 - acc: 0.9450 -- iter: 25/55\n","Training Step: 10423  | total loss: \u001b[1m\u001b[32m0.34405\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 948 | loss: 0.34405 - acc: 0.9505 -- iter: 30/55\n","Training Step: 10424  | total loss: \u001b[1m\u001b[32m0.31011\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 948 | loss: 0.31011 - acc: 0.9554 -- iter: 35/55\n","Training Step: 10425  | total loss: \u001b[1m\u001b[32m0.28076\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 948 | loss: 0.28076 - acc: 0.9599 -- iter: 40/55\n","Training Step: 10426  | total loss: \u001b[1m\u001b[32m0.25417\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 948 | loss: 0.25417 - acc: 0.9639 -- iter: 45/55\n","Training Step: 10427  | total loss: \u001b[1m\u001b[32m0.23020\u001b[0m\u001b[0m | time: 0.065s\n","| Adam | epoch: 948 | loss: 0.23020 - acc: 0.9675 -- iter: 50/55\n","Training Step: 10428  | total loss: \u001b[1m\u001b[32m0.20953\u001b[0m\u001b[0m | time: 0.070s\n","| Adam | epoch: 948 | loss: 0.20953 - acc: 0.9737 -- iter: 55/55\n","--\n","Training Step: 10429  | total loss: \u001b[1m\u001b[32m0.19097\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 949 | loss: 0.19097 - acc: 0.9737 -- iter: 05/55\n","Training Step: 10430  | total loss: \u001b[1m\u001b[32m0.17417\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 949 | loss: 0.17417 - acc: 0.9763 -- iter: 10/55\n","Training Step: 10431  | total loss: \u001b[1m\u001b[32m0.15874\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 949 | loss: 0.15874 - acc: 0.9787 -- iter: 15/55\n","Training Step: 10432  | total loss: \u001b[1m\u001b[32m0.14325\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 949 | loss: 0.14325 - acc: 0.9808 -- iter: 20/55\n","Training Step: 10433  | total loss: \u001b[1m\u001b[32m0.13030\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 949 | loss: 0.13030 - acc: 0.9827 -- iter: 25/55\n","Training Step: 10434  | total loss: \u001b[1m\u001b[32m0.11974\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 949 | loss: 0.11974 - acc: 0.9845 -- iter: 30/55\n","Training Step: 10435  | total loss: \u001b[1m\u001b[32m0.11003\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 949 | loss: 0.11003 - acc: 0.9860 -- iter: 35/55\n","Training Step: 10436  | total loss: \u001b[1m\u001b[32m0.10007\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 949 | loss: 0.10007 - acc: 0.9874 -- iter: 40/55\n","Training Step: 10437  | total loss: \u001b[1m\u001b[32m0.09178\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 949 | loss: 0.09178 - acc: 0.9887 -- iter: 45/55\n","Training Step: 10438  | total loss: \u001b[1m\u001b[32m0.08328\u001b[0m\u001b[0m | time: 0.062s\n","| Adam | epoch: 949 | loss: 0.08328 - acc: 0.9898 -- iter: 50/55\n","Training Step: 10439  | total loss: \u001b[1m\u001b[32m0.07570\u001b[0m\u001b[0m | time: 0.069s\n","| Adam | epoch: 949 | loss: 0.07570 - acc: 0.9908 -- iter: 55/55\n","--\n","Training Step: 10440  | total loss: \u001b[1m\u001b[32m0.06929\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 950 | loss: 0.06929 - acc: 0.9917 -- iter: 05/55\n","Training Step: 10441  | total loss: \u001b[1m\u001b[32m0.06352\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 950 | loss: 0.06352 - acc: 0.9926 -- iter: 10/55\n","Training Step: 10442  | total loss: \u001b[1m\u001b[32m0.05995\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 950 | loss: 0.05995 - acc: 0.9933 -- iter: 15/55\n","Training Step: 10443  | total loss: \u001b[1m\u001b[32m0.05610\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 950 | loss: 0.05610 - acc: 0.9940 -- iter: 20/55\n","Training Step: 10444  | total loss: \u001b[1m\u001b[32m0.05155\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 950 | loss: 0.05155 - acc: 0.9946 -- iter: 25/55\n","Training Step: 10445  | total loss: \u001b[1m\u001b[32m0.04692\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 950 | loss: 0.04692 - acc: 0.9951 -- iter: 30/55\n","Training Step: 10446  | total loss: \u001b[1m\u001b[32m0.04381\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 950 | loss: 0.04381 - acc: 0.9956 -- iter: 35/55\n","Training Step: 10447  | total loss: \u001b[1m\u001b[32m0.04035\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 950 | loss: 0.04035 - acc: 0.9961 -- iter: 40/55\n","Training Step: 10448  | total loss: \u001b[1m\u001b[32m0.03715\u001b[0m\u001b[0m | time: 0.060s\n","| Adam | epoch: 950 | loss: 0.03715 - acc: 0.9964 -- iter: 45/55\n","Training Step: 10449  | total loss: \u001b[1m\u001b[32m0.03526\u001b[0m\u001b[0m | time: 0.067s\n","| Adam | epoch: 950 | loss: 0.03526 - acc: 0.9968 -- iter: 50/55\n","Training Step: 10450  | total loss: \u001b[1m\u001b[32m0.03650\u001b[0m\u001b[0m | time: 0.074s\n","| Adam | epoch: 950 | loss: 0.03650 - acc: 0.9971 -- iter: 55/55\n","--\n","Training Step: 10451  | total loss: \u001b[1m\u001b[32m0.03070\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 951 | loss: 0.03070 - acc: 0.9974 -- iter: 05/55\n","Training Step: 10452  | total loss: \u001b[1m\u001b[32m0.03070\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 951 | loss: 0.03070 - acc: 0.9977 -- iter: 10/55\n","Training Step: 10453  | total loss: \u001b[1m\u001b[32m0.02799\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 951 | loss: 0.02799 - acc: 0.9979 -- iter: 15/55\n","Training Step: 10454  | total loss: \u001b[1m\u001b[32m0.02713\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 951 | loss: 0.02713 - acc: 0.9981 -- iter: 20/55\n","Training Step: 10455  | total loss: \u001b[1m\u001b[32m0.02598\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 951 | loss: 0.02598 - acc: 0.9985 -- iter: 25/55\n","Training Step: 10456  | total loss: \u001b[1m\u001b[32m0.02475\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 951 | loss: 0.02475 - acc: 0.9986 -- iter: 30/55\n","Training Step: 10457  | total loss: \u001b[1m\u001b[32m0.02404\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 951 | loss: 0.02404 - acc: 0.9988 -- iter: 35/55\n","Training Step: 10458  | total loss: \u001b[1m\u001b[32m0.02385\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 951 | loss: 0.02385 - acc: 0.9989 -- iter: 40/55\n","Training Step: 10459  | total loss: \u001b[1m\u001b[32m0.02305\u001b[0m\u001b[0m | time: 0.060s\n","| Adam | epoch: 951 | loss: 0.02305 - acc: 0.9990 -- iter: 45/55\n","Training Step: 10460  | total loss: \u001b[1m\u001b[32m0.02170\u001b[0m\u001b[0m | time: 0.067s\n","| Adam | epoch: 951 | loss: 0.02170 - acc: 0.9991 -- iter: 50/55\n","Training Step: 10461  | total loss: \u001b[1m\u001b[32m0.02003\u001b[0m\u001b[0m | time: 0.075s\n","| Adam | epoch: 951 | loss: 0.02003 - acc: 0.9991 -- iter: 55/55\n","--\n","Training Step: 10462  | total loss: \u001b[1m\u001b[32m0.01945\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 952 | loss: 0.01945 - acc: 0.9992 -- iter: 05/55\n","Training Step: 10463  | total loss: \u001b[1m\u001b[32m0.01869\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 952 | loss: 0.01869 - acc: 0.9993 -- iter: 10/55\n","Training Step: 10464  | total loss: \u001b[1m\u001b[32m0.01901\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 952 | loss: 0.01901 - acc: 0.9993 -- iter: 15/55\n","Training Step: 10465  | total loss: \u001b[1m\u001b[32m0.01928\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 952 | loss: 0.01928 - acc: 0.9994 -- iter: 20/55\n","Training Step: 10466  | total loss: \u001b[1m\u001b[32m0.01939\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 952 | loss: 0.01939 - acc: 0.9995 -- iter: 25/55\n","Training Step: 10467  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 952 | loss: 0.01788 - acc: 0.9996 -- iter: 30/55\n","Training Step: 10468  | total loss: \u001b[1m\u001b[32m0.01718\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 952 | loss: 0.01718 - acc: 0.9996 -- iter: 35/55\n","Training Step: 10469  | total loss: \u001b[1m\u001b[32m0.01578\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 952 | loss: 0.01578 - acc: 0.9997 -- iter: 40/55\n","Training Step: 10470  | total loss: \u001b[1m\u001b[32m0.01531\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 952 | loss: 0.01531 - acc: 0.9997 -- iter: 45/55\n","Training Step: 10471  | total loss: \u001b[1m\u001b[32m0.01544\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 952 | loss: 0.01544 - acc: 0.9997 -- iter: 50/55\n","Training Step: 10472  | total loss: \u001b[1m\u001b[32m0.01606\u001b[0m\u001b[0m | time: 0.064s\n","| Adam | epoch: 952 | loss: 0.01606 - acc: 0.9997 -- iter: 55/55\n","--\n","Training Step: 10473  | total loss: \u001b[1m\u001b[32m0.01499\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 953 | loss: 0.01499 - acc: 0.9998 -- iter: 05/55\n","Training Step: 10474  | total loss: \u001b[1m\u001b[32m0.01617\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 953 | loss: 0.01617 - acc: 0.9998 -- iter: 10/55\n","Training Step: 10475  | total loss: \u001b[1m\u001b[32m0.01550\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 953 | loss: 0.01550 - acc: 0.9998 -- iter: 15/55\n","Training Step: 10476  | total loss: \u001b[1m\u001b[32m0.01544\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 953 | loss: 0.01544 - acc: 0.9998 -- iter: 20/55\n","Training Step: 10477  | total loss: \u001b[1m\u001b[32m0.01538\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 953 | loss: 0.01538 - acc: 0.9998 -- iter: 25/55\n","Training Step: 10478  | total loss: \u001b[1m\u001b[32m0.01418\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 953 | loss: 0.01418 - acc: 0.9999 -- iter: 30/55\n","Training Step: 10479  | total loss: \u001b[1m\u001b[32m0.01455\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 953 | loss: 0.01455 - acc: 0.9999 -- iter: 35/55\n","Training Step: 10480  | total loss: \u001b[1m\u001b[32m0.01374\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 953 | loss: 0.01374 - acc: 0.9999 -- iter: 40/55\n","Training Step: 10481  | total loss: \u001b[1m\u001b[32m0.01492\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 953 | loss: 0.01492 - acc: 0.9999 -- iter: 45/55\n","Training Step: 10482  | total loss: \u001b[1m\u001b[32m0.01511\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 953 | loss: 0.01511 - acc: 0.9999 -- iter: 50/55\n","Training Step: 10483  | total loss: \u001b[1m\u001b[32m0.01488\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 953 | loss: 0.01488 - acc: 0.9999 -- iter: 55/55\n","--\n","Training Step: 10484  | total loss: \u001b[1m\u001b[32m0.01363\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 954 | loss: 0.01363 - acc: 0.9999 -- iter: 05/55\n","Training Step: 10485  | total loss: \u001b[1m\u001b[32m0.01373\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 954 | loss: 0.01373 - acc: 0.9999 -- iter: 10/55\n","Training Step: 10486  | total loss: \u001b[1m\u001b[32m0.01365\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 954 | loss: 0.01365 - acc: 0.9999 -- iter: 15/55\n","Training Step: 10487  | total loss: \u001b[1m\u001b[32m0.01365\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 954 | loss: 0.01365 - acc: 0.9999 -- iter: 20/55\n","Training Step: 10488  | total loss: \u001b[1m\u001b[32m0.01365\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 954 | loss: 0.01365 - acc: 1.0000 -- iter: 25/55\n","Training Step: 10489  | total loss: \u001b[1m\u001b[32m0.01363\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 954 | loss: 0.01363 - acc: 1.0000 -- iter: 30/55\n","Training Step: 10490  | total loss: \u001b[1m\u001b[32m0.01333\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 954 | loss: 0.01333 - acc: 1.0000 -- iter: 35/55\n","Training Step: 10491  | total loss: \u001b[1m\u001b[32m0.01258\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 954 | loss: 0.01258 - acc: 1.0000 -- iter: 40/55\n","Training Step: 10492  | total loss: \u001b[1m\u001b[32m0.01295\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 954 | loss: 0.01295 - acc: 1.0000 -- iter: 45/55\n","Training Step: 10493  | total loss: \u001b[1m\u001b[32m0.01208\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 954 | loss: 0.01208 - acc: 1.0000 -- iter: 50/55\n","Training Step: 10494  | total loss: \u001b[1m\u001b[32m0.01226\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 954 | loss: 0.01226 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 10495  | total loss: \u001b[1m\u001b[32m0.01334\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 955 | loss: 0.01334 - acc: 1.0000 -- iter: 05/55\n","Training Step: 10496  | total loss: \u001b[1m\u001b[32m0.01293\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 955 | loss: 0.01293 - acc: 1.0000 -- iter: 10/55\n","Training Step: 10497  | total loss: \u001b[1m\u001b[32m0.01214\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 955 | loss: 0.01214 - acc: 1.0000 -- iter: 15/55\n","Training Step: 10498  | total loss: \u001b[1m\u001b[32m0.01286\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 955 | loss: 0.01286 - acc: 1.0000 -- iter: 20/55\n","Training Step: 10499  | total loss: \u001b[1m\u001b[32m0.01263\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 955 | loss: 0.01263 - acc: 1.0000 -- iter: 25/55\n","Training Step: 10500  | total loss: \u001b[1m\u001b[32m0.01241\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 955 | loss: 0.01241 - acc: 1.0000 -- iter: 30/55\n","Training Step: 10501  | total loss: \u001b[1m\u001b[32m0.01210\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 955 | loss: 0.01210 - acc: 1.0000 -- iter: 35/55\n","Training Step: 10502  | total loss: \u001b[1m\u001b[32m0.01249\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 955 | loss: 0.01249 - acc: 1.0000 -- iter: 40/55\n","Training Step: 10503  | total loss: \u001b[1m\u001b[32m0.01221\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 955 | loss: 0.01221 - acc: 1.0000 -- iter: 45/55\n","Training Step: 10504  | total loss: \u001b[1m\u001b[32m0.01192\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 955 | loss: 0.01192 - acc: 1.0000 -- iter: 50/55\n","Training Step: 10505  | total loss: \u001b[1m\u001b[32m0.01208\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 955 | loss: 0.01208 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 10506  | total loss: \u001b[1m\u001b[32m0.01220\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 956 | loss: 0.01220 - acc: 1.0000 -- iter: 05/55\n","Training Step: 10507  | total loss: \u001b[1m\u001b[32m0.01206\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 956 | loss: 0.01206 - acc: 1.0000 -- iter: 10/55\n","Training Step: 10508  | total loss: \u001b[1m\u001b[32m0.01210\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 956 | loss: 0.01210 - acc: 1.0000 -- iter: 15/55\n","Training Step: 10509  | total loss: \u001b[1m\u001b[32m0.01270\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 956 | loss: 0.01270 - acc: 1.0000 -- iter: 20/55\n","Training Step: 10510  | total loss: \u001b[1m\u001b[32m0.01251\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 956 | loss: 0.01251 - acc: 1.0000 -- iter: 25/55\n","Training Step: 10511  | total loss: \u001b[1m\u001b[32m0.01240\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 956 | loss: 0.01240 - acc: 1.0000 -- iter: 30/55\n","Training Step: 10512  | total loss: \u001b[1m\u001b[32m0.01229\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 956 | loss: 0.01229 - acc: 1.0000 -- iter: 35/55\n","Training Step: 10513  | total loss: \u001b[1m\u001b[32m0.01279\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 956 | loss: 0.01279 - acc: 1.0000 -- iter: 40/55\n","Training Step: 10514  | total loss: \u001b[1m\u001b[32m0.01189\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 956 | loss: 0.01189 - acc: 1.0000 -- iter: 45/55\n","Training Step: 10515  | total loss: \u001b[1m\u001b[32m0.01141\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 956 | loss: 0.01141 - acc: 1.0000 -- iter: 50/55\n","Training Step: 10516  | total loss: \u001b[1m\u001b[32m0.01049\u001b[0m\u001b[0m | time: 0.065s\n","| Adam | epoch: 956 | loss: 0.01049 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 10517  | total loss: \u001b[1m\u001b[32m0.01049\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 957 | loss: 0.01049 - acc: 1.0000 -- iter: 05/55\n","Training Step: 10518  | total loss: \u001b[1m\u001b[32m0.01010\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 957 | loss: 0.01010 - acc: 1.0000 -- iter: 10/55\n","Training Step: 10519  | total loss: \u001b[1m\u001b[32m0.01088\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 957 | loss: 0.01088 - acc: 1.0000 -- iter: 15/55\n","Training Step: 10520  | total loss: \u001b[1m\u001b[32m0.01140\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 957 | loss: 0.01140 - acc: 1.0000 -- iter: 20/55\n","Training Step: 10521  | total loss: \u001b[1m\u001b[32m0.01140\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 957 | loss: 0.01140 - acc: 1.0000 -- iter: 25/55\n","Training Step: 10522  | total loss: \u001b[1m\u001b[32m0.01041\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 957 | loss: 0.01041 - acc: 1.0000 -- iter: 30/55\n","Training Step: 10523  | total loss: \u001b[1m\u001b[32m0.01147\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 957 | loss: 0.01147 - acc: 1.0000 -- iter: 35/55\n","Training Step: 10524  | total loss: \u001b[1m\u001b[32m0.01243\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 957 | loss: 0.01243 - acc: 1.0000 -- iter: 40/55\n","Training Step: 10525  | total loss: \u001b[1m\u001b[32m0.01243\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 957 | loss: 0.01243 - acc: 1.0000 -- iter: 45/55\n","Training Step: 10526  | total loss: \u001b[1m\u001b[32m0.01175\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 957 | loss: 0.01175 - acc: 1.0000 -- iter: 50/55\n","Training Step: 10527  | total loss: \u001b[1m\u001b[32m0.01160\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 957 | loss: 0.01160 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 10528  | total loss: \u001b[1m\u001b[32m0.01241\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 958 | loss: 0.01241 - acc: 1.0000 -- iter: 05/55\n","Training Step: 10529  | total loss: \u001b[1m\u001b[32m0.01328\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 958 | loss: 0.01328 - acc: 1.0000 -- iter: 10/55\n","Training Step: 10530  | total loss: \u001b[1m\u001b[32m0.01328\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 958 | loss: 0.01328 - acc: 1.0000 -- iter: 15/55\n","Training Step: 10531  | total loss: \u001b[1m\u001b[32m0.01399\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 958 | loss: 0.01399 - acc: 1.0000 -- iter: 20/55\n","Training Step: 10532  | total loss: \u001b[1m\u001b[32m0.01399\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 958 | loss: 0.01399 - acc: 1.0000 -- iter: 25/55\n","Training Step: 10533  | total loss: \u001b[1m\u001b[32m0.01373\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 958 | loss: 0.01373 - acc: 1.0000 -- iter: 30/55\n","Training Step: 10534  | total loss: \u001b[1m\u001b[32m0.01311\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 958 | loss: 0.01311 - acc: 1.0000 -- iter: 35/55\n","Training Step: 10535  | total loss: \u001b[1m\u001b[32m0.01279\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 958 | loss: 0.01279 - acc: 1.0000 -- iter: 40/55\n","Training Step: 10536  | total loss: \u001b[1m\u001b[32m0.01255\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 958 | loss: 0.01255 - acc: 1.0000 -- iter: 45/55\n","Training Step: 10537  | total loss: \u001b[1m\u001b[32m0.01233\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 958 | loss: 0.01233 - acc: 1.0000 -- iter: 50/55\n","Training Step: 10538  | total loss: \u001b[1m\u001b[32m0.01103\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 958 | loss: 0.01103 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 10539  | total loss: \u001b[1m\u001b[32m0.01018\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 959 | loss: 0.01018 - acc: 1.0000 -- iter: 05/55\n","Training Step: 10540  | total loss: \u001b[1m\u001b[32m0.01118\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 959 | loss: 0.01118 - acc: 1.0000 -- iter: 10/55\n","Training Step: 10541  | total loss: \u001b[1m\u001b[32m0.01118\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 959 | loss: 0.01118 - acc: 1.0000 -- iter: 15/55\n","Training Step: 10542  | total loss: \u001b[1m\u001b[32m0.01162\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 959 | loss: 0.01162 - acc: 1.0000 -- iter: 20/55\n","Training Step: 10543  | total loss: \u001b[1m\u001b[32m0.01162\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 959 | loss: 0.01162 - acc: 1.0000 -- iter: 25/55\n","Training Step: 10544  | total loss: \u001b[1m\u001b[32m0.01129\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 959 | loss: 0.01129 - acc: 1.0000 -- iter: 30/55\n","Training Step: 10545  | total loss: \u001b[1m\u001b[32m0.01173\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 959 | loss: 0.01173 - acc: 1.0000 -- iter: 35/55\n","Training Step: 10546  | total loss: \u001b[1m\u001b[32m0.01140\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 959 | loss: 0.01140 - acc: 1.0000 -- iter: 40/55\n","Training Step: 10547  | total loss: \u001b[1m\u001b[32m0.01102\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 959 | loss: 0.01102 - acc: 1.0000 -- iter: 45/55\n","Training Step: 10548  | total loss: \u001b[1m\u001b[32m0.01052\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 959 | loss: 0.01052 - acc: 1.0000 -- iter: 50/55\n","Training Step: 10549  | total loss: \u001b[1m\u001b[32m0.01007\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 959 | loss: 0.01007 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 10550  | total loss: \u001b[1m\u001b[32m0.00991\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 960 | loss: 0.00991 - acc: 1.0000 -- iter: 05/55\n","Training Step: 10551  | total loss: \u001b[1m\u001b[32m0.00960\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 960 | loss: 0.00960 - acc: 1.0000 -- iter: 10/55\n","Training Step: 10552  | total loss: \u001b[1m\u001b[32m0.00938\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 960 | loss: 0.00938 - acc: 1.0000 -- iter: 15/55\n","Training Step: 10553  | total loss: \u001b[1m\u001b[32m0.00968\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 960 | loss: 0.00968 - acc: 1.0000 -- iter: 20/55\n","Training Step: 10554  | total loss: \u001b[1m\u001b[32m0.00955\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 960 | loss: 0.00955 - acc: 1.0000 -- iter: 25/55\n","Training Step: 10555  | total loss: \u001b[1m\u001b[32m0.00937\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 960 | loss: 0.00937 - acc: 1.0000 -- iter: 30/55\n","Training Step: 10556  | total loss: \u001b[1m\u001b[32m0.00879\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 960 | loss: 0.00879 - acc: 1.0000 -- iter: 35/55\n","Training Step: 10557  | total loss: \u001b[1m\u001b[32m0.00879\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 960 | loss: 0.00879 - acc: 1.0000 -- iter: 40/55\n","Training Step: 10558  | total loss: \u001b[1m\u001b[32m0.00893\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 960 | loss: 0.00893 - acc: 1.0000 -- iter: 45/55\n","Training Step: 10559  | total loss: \u001b[1m\u001b[32m0.00987\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 960 | loss: 0.00987 - acc: 1.0000 -- iter: 50/55\n","Training Step: 10560  | total loss: \u001b[1m\u001b[32m0.00985\u001b[0m\u001b[0m | time: 0.062s\n","| Adam | epoch: 960 | loss: 0.00985 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 10561  | total loss: \u001b[1m\u001b[32m0.00982\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 961 | loss: 0.00982 - acc: 1.0000 -- iter: 05/55\n","Training Step: 10562  | total loss: \u001b[1m\u001b[32m0.00939\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 961 | loss: 0.00939 - acc: 1.0000 -- iter: 10/55\n","Training Step: 10563  | total loss: \u001b[1m\u001b[32m0.00897\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 961 | loss: 0.00897 - acc: 1.0000 -- iter: 15/55\n","Training Step: 10564  | total loss: \u001b[1m\u001b[32m0.82109\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 961 | loss: 0.82109 - acc: 0.9200 -- iter: 20/55\n","Training Step: 10565  | total loss: \u001b[1m\u001b[32m0.73960\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 961 | loss: 0.73960 - acc: 0.9280 -- iter: 25/55\n","Training Step: 10566  | total loss: \u001b[1m\u001b[32m0.66693\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 961 | loss: 0.66693 - acc: 0.9352 -- iter: 30/55\n","Training Step: 10567  | total loss: \u001b[1m\u001b[32m0.60138\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 961 | loss: 0.60138 - acc: 0.9417 -- iter: 35/55\n","Training Step: 10568  | total loss: \u001b[1m\u001b[32m0.54196\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 961 | loss: 0.54196 - acc: 0.9475 -- iter: 40/55\n","Training Step: 10569  | total loss: \u001b[1m\u001b[32m0.48986\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 961 | loss: 0.48986 - acc: 0.9528 -- iter: 45/55\n","Training Step: 10570  | total loss: \u001b[1m\u001b[32m0.44118\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 961 | loss: 0.44118 - acc: 0.9575 -- iter: 50/55\n","Training Step: 10571  | total loss: \u001b[1m\u001b[32m0.39789\u001b[0m\u001b[0m | time: 0.060s\n","| Adam | epoch: 961 | loss: 0.39789 - acc: 0.9617 -- iter: 55/55\n","--\n","Training Step: 10572  | total loss: \u001b[1m\u001b[32m0.35944\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 962 | loss: 0.35944 - acc: 0.9656 -- iter: 05/55\n","Training Step: 10573  | total loss: \u001b[1m\u001b[32m0.32486\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 962 | loss: 0.32486 - acc: 0.9690 -- iter: 10/55\n","Training Step: 10574  | total loss: \u001b[1m\u001b[32m0.29359\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 962 | loss: 0.29359 - acc: 0.9721 -- iter: 15/55\n","Training Step: 10575  | total loss: \u001b[1m\u001b[32m0.26487\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 962 | loss: 0.26487 - acc: 0.9749 -- iter: 20/55\n","Training Step: 10576  | total loss: \u001b[1m\u001b[32m0.23968\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 962 | loss: 0.23968 - acc: 0.9774 -- iter: 25/55\n","Training Step: 10577  | total loss: \u001b[1m\u001b[32m0.21617\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 962 | loss: 0.21617 - acc: 0.9797 -- iter: 30/55\n","Training Step: 10578  | total loss: \u001b[1m\u001b[32m0.19570\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 962 | loss: 0.19570 - acc: 0.9817 -- iter: 35/55\n","Training Step: 10579  | total loss: \u001b[1m\u001b[32m0.17780\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 962 | loss: 0.17780 - acc: 0.9835 -- iter: 40/55\n","Training Step: 10580  | total loss: \u001b[1m\u001b[32m0.16054\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 962 | loss: 0.16054 - acc: 0.9852 -- iter: 45/55\n","Training Step: 10581  | total loss: \u001b[1m\u001b[32m0.14610\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 962 | loss: 0.14610 - acc: 0.9880 -- iter: 50/55\n","Training Step: 10582  | total loss: \u001b[1m\u001b[32m0.13203\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 962 | loss: 0.13203 - acc: 0.9892 -- iter: 55/55\n","--\n","Training Step: 10583  | total loss: \u001b[1m\u001b[32m0.12168\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 963 | loss: 0.12168 - acc: 0.9892 -- iter: 05/55\n","Training Step: 10584  | total loss: \u001b[1m\u001b[32m0.11011\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 963 | loss: 0.11011 - acc: 0.9903 -- iter: 10/55\n","Training Step: 10585  | total loss: \u001b[1m\u001b[32m0.09969\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 963 | loss: 0.09969 - acc: 0.9912 -- iter: 15/55\n","Training Step: 10586  | total loss: \u001b[1m\u001b[32m0.09164\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 963 | loss: 0.09164 - acc: 0.9929 -- iter: 20/55\n","Training Step: 10587  | total loss: \u001b[1m\u001b[32m0.08443\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 963 | loss: 0.08443 - acc: 0.9929 -- iter: 25/55\n","Training Step: 10588  | total loss: \u001b[1m\u001b[32m1.88990\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 963 | loss: 1.88990 - acc: 0.9136 -- iter: 30/55\n","Training Step: 10589  | total loss: \u001b[1m\u001b[32m1.70137\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 963 | loss: 1.70137 - acc: 0.9223 -- iter: 35/55\n","Training Step: 10590  | total loss: \u001b[1m\u001b[32m1.53298\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 963 | loss: 1.53298 - acc: 0.9300 -- iter: 40/55\n","Training Step: 10591  | total loss: \u001b[1m\u001b[32m1.24549\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 963 | loss: 1.24549 - acc: 0.9370 -- iter: 45/55\n","Training Step: 10592  | total loss: \u001b[1m\u001b[32m1.12181\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 963 | loss: 1.12181 - acc: 0.9433 -- iter: 50/55\n","Training Step: 10593  | total loss: \u001b[1m\u001b[32m1.01076\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 963 | loss: 1.01076 - acc: 0.9490 -- iter: 55/55\n","--\n","Training Step: 10594  | total loss: \u001b[1m\u001b[32m1.01076\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 964 | loss: 1.01076 - acc: 0.9541 -- iter: 05/55\n","Training Step: 10595  | total loss: \u001b[1m\u001b[32m0.91034\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 964 | loss: 0.91034 - acc: 0.9587 -- iter: 10/55\n","Training Step: 10596  | total loss: \u001b[1m\u001b[32m0.82017\u001b[0m\u001b[0m | time: 0.014s\n","| Adam | epoch: 964 | loss: 0.82017 - acc: 0.9628 -- iter: 15/55\n","Training Step: 10597  | total loss: \u001b[1m\u001b[32m0.73909\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 964 | loss: 0.73909 - acc: 0.9665 -- iter: 20/55\n","Training Step: 10598  | total loss: \u001b[1m\u001b[32m0.60058\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 964 | loss: 0.60058 - acc: 0.9699 -- iter: 25/55\n","Training Step: 10599  | total loss: \u001b[1m\u001b[32m0.54226\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 964 | loss: 0.54226 - acc: 0.9756 -- iter: 30/55\n","Training Step: 10600  | total loss: \u001b[1m\u001b[32m0.48915\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 964 | loss: 0.48915 - acc: 0.9780 -- iter: 35/55\n","Training Step: 10601  | total loss: \u001b[1m\u001b[32m0.44092\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 964 | loss: 0.44092 - acc: 0.9780 -- iter: 40/55\n","Training Step: 10602  | total loss: \u001b[1m\u001b[32m0.39848\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 964 | loss: 0.39848 - acc: 0.9802 -- iter: 45/55\n","Training Step: 10603  | total loss: \u001b[1m\u001b[32m0.36000\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 964 | loss: 0.36000 - acc: 0.9822 -- iter: 50/55\n","Training Step: 10604  | total loss: \u001b[1m\u001b[32m0.36000\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 964 | loss: 0.36000 - acc: 0.9840 -- iter: 55/55\n","--\n","Training Step: 10605  | total loss: \u001b[1m\u001b[32m0.32639\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 965 | loss: 0.32639 - acc: 0.9856 -- iter: 05/55\n","Training Step: 10606  | total loss: \u001b[1m\u001b[32m0.29399\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 965 | loss: 0.29399 - acc: 0.9870 -- iter: 10/55\n","Training Step: 10607  | total loss: \u001b[1m\u001b[32m0.26585\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 965 | loss: 0.26585 - acc: 0.9883 -- iter: 15/55\n","Training Step: 10608  | total loss: \u001b[1m\u001b[32m0.24060\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 965 | loss: 0.24060 - acc: 0.9895 -- iter: 20/55\n","Training Step: 10609  | total loss: \u001b[1m\u001b[32m0.21786\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 965 | loss: 0.21786 - acc: 0.9905 -- iter: 25/55\n","Training Step: 10610  | total loss: \u001b[1m\u001b[32m0.17943\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 965 | loss: 0.17943 - acc: 0.9915 -- iter: 30/55\n","Training Step: 10611  | total loss: \u001b[1m\u001b[32m0.16251\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 965 | loss: 0.16251 - acc: 0.9923 -- iter: 35/55\n","Training Step: 10612  | total loss: \u001b[1m\u001b[32m0.14666\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 965 | loss: 0.14666 - acc: 0.9931 -- iter: 40/55\n","Training Step: 10613  | total loss: \u001b[1m\u001b[32m0.13281\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 965 | loss: 0.13281 - acc: 0.9938 -- iter: 45/55\n","Training Step: 10614  | total loss: \u001b[1m\u001b[32m0.12083\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 965 | loss: 0.12083 - acc: 0.9944 -- iter: 50/55\n","Training Step: 10615  | total loss: \u001b[1m\u001b[32m0.10972\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 965 | loss: 0.10972 - acc: 0.9950 -- iter: 55/55\n","--\n","Training Step: 10616  | total loss: \u001b[1m\u001b[32m0.10972\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 966 | loss: 0.10972 - acc: 0.9959 -- iter: 05/55\n","Training Step: 10617  | total loss: \u001b[1m\u001b[32m0.09997\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 966 | loss: 0.09997 - acc: 0.9959 -- iter: 10/55\n","Training Step: 10618  | total loss: \u001b[1m\u001b[32m0.09126\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 966 | loss: 0.09126 - acc: 0.9967 -- iter: 15/55\n","Training Step: 10619  | total loss: \u001b[1m\u001b[32m0.08445\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 966 | loss: 0.08445 - acc: 0.9970 -- iter: 20/55\n","Training Step: 10620  | total loss: \u001b[1m\u001b[32m0.07741\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 966 | loss: 0.07741 - acc: 0.9973 -- iter: 25/55\n","Training Step: 10621  | total loss: \u001b[1m\u001b[32m0.07106\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 966 | loss: 0.07106 - acc: 0.9976 -- iter: 30/55\n","Training Step: 10622  | total loss: \u001b[1m\u001b[32m0.06506\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 966 | loss: 0.06506 - acc: 0.9978 -- iter: 35/55\n","Training Step: 10623  | total loss: \u001b[1m\u001b[32m0.05445\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 966 | loss: 0.05445 - acc: 0.9981 -- iter: 40/55\n","Training Step: 10624  | total loss: \u001b[1m\u001b[32m0.04986\u001b[0m\u001b[0m | time: 0.064s\n","| Adam | epoch: 966 | loss: 0.04986 - acc: 0.9982 -- iter: 45/55\n","Training Step: 10625  | total loss: \u001b[1m\u001b[32m0.04729\u001b[0m\u001b[0m | time: 0.069s\n","| Adam | epoch: 966 | loss: 0.04729 - acc: 0.9984 -- iter: 50/55\n","Training Step: 10626  | total loss: \u001b[1m\u001b[32m0.04510\u001b[0m\u001b[0m | time: 0.077s\n","| Adam | epoch: 966 | loss: 0.04510 - acc: 0.9986 -- iter: 55/55\n","--\n","Training Step: 10627  | total loss: \u001b[1m\u001b[32m0.04188\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 967 | loss: 0.04188 - acc: 0.9987 -- iter: 05/55\n","Training Step: 10628  | total loss: \u001b[1m\u001b[32m0.03822\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 967 | loss: 0.03822 - acc: 0.9989 -- iter: 10/55\n","Training Step: 10629  | total loss: \u001b[1m\u001b[32m0.03822\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 967 | loss: 0.03822 - acc: 0.9989 -- iter: 15/55\n","Training Step: 10630  | total loss: \u001b[1m\u001b[32m0.03523\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 967 | loss: 0.03523 - acc: 0.9990 -- iter: 20/55\n","Training Step: 10631  | total loss: \u001b[1m\u001b[32m0.03000\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 967 | loss: 0.03000 - acc: 0.9991 -- iter: 25/55\n","Training Step: 10632  | total loss: \u001b[1m\u001b[32m0.03000\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 967 | loss: 0.03000 - acc: 0.9992 -- iter: 30/55\n","Training Step: 10633  | total loss: \u001b[1m\u001b[32m0.02772\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 967 | loss: 0.02772 - acc: 0.9992 -- iter: 35/55\n","Training Step: 10634  | total loss: \u001b[1m\u001b[32m0.02539\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 967 | loss: 0.02539 - acc: 0.9993 -- iter: 40/55\n","Training Step: 10635  | total loss: \u001b[1m\u001b[32m0.02374\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 967 | loss: 0.02374 - acc: 0.9994 -- iter: 45/55\n","Training Step: 10636  | total loss: \u001b[1m\u001b[32m0.02213\u001b[0m\u001b[0m | time: 0.065s\n","| Adam | epoch: 967 | loss: 0.02213 - acc: 0.9995 -- iter: 50/55\n","Training Step: 10637  | total loss: \u001b[1m\u001b[32m0.02125\u001b[0m\u001b[0m | time: 0.070s\n","| Adam | epoch: 967 | loss: 0.02125 - acc: 0.9995 -- iter: 55/55\n","--\n","Training Step: 10638  | total loss: \u001b[1m\u001b[32m0.02081\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 968 | loss: 0.02081 - acc: 0.9996 -- iter: 05/55\n","Training Step: 10639  | total loss: \u001b[1m\u001b[32m0.02028\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 968 | loss: 0.02028 - acc: 0.9996 -- iter: 10/55\n","Training Step: 10640  | total loss: \u001b[1m\u001b[32m0.01928\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 968 | loss: 0.01928 - acc: 0.9996 -- iter: 15/55\n","Training Step: 10641  | total loss: \u001b[1m\u001b[32m0.01710\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 968 | loss: 0.01710 - acc: 0.9997 -- iter: 20/55\n","Training Step: 10642  | total loss: \u001b[1m\u001b[32m0.01710\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 968 | loss: 0.01710 - acc: 0.9997 -- iter: 25/55\n","Training Step: 10643  | total loss: \u001b[1m\u001b[32m0.01534\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 968 | loss: 0.01534 - acc: 0.9997 -- iter: 30/55\n","Training Step: 10712  | total loss: \u001b[1m\u001b[32m0.12120\u001b[0m\u001b[0m | time: 0.073s\n","| Adam | epoch: 974 | loss: 0.12120 - acc: 0.9947 -- iter: 45/55\n","Training Step: 10713  | total loss: \u001b[1m\u001b[32m0.11011\u001b[0m\u001b[0m | time: 0.081s\n","| Adam | epoch: 974 | loss: 0.11011 - acc: 0.9952 -- iter: 50/55\n","Training Step: 10714  | total loss: \u001b[1m\u001b[32m0.10003\u001b[0m\u001b[0m | time: 0.087s\n","| Adam | epoch: 974 | loss: 0.10003 - acc: 0.9957 -- iter: 55/55\n","--\n","Training Step: 10715  | total loss: \u001b[1m\u001b[32m0.09137\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 975 | loss: 0.09137 - acc: 0.9961 -- iter: 05/55\n","Training Step: 10716  | total loss: \u001b[1m\u001b[32m0.08292\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 975 | loss: 0.08292 - acc: 0.9965 -- iter: 10/55\n","Training Step: 10717  | total loss: \u001b[1m\u001b[32m0.07531\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 975 | loss: 0.07531 - acc: 0.9969 -- iter: 15/55\n","Training Step: 10718  | total loss: \u001b[1m\u001b[32m0.06847\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 975 | loss: 0.06847 - acc: 0.9972 -- iter: 20/55\n","Training Step: 10719  | total loss: \u001b[1m\u001b[32m0.06253\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 975 | loss: 0.06253 - acc: 0.9975 -- iter: 25/55\n","Training Step: 10720  | total loss: \u001b[1m\u001b[32m0.05771\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 975 | loss: 0.05771 - acc: 0.9977 -- iter: 30/55\n","Training Step: 10721  | total loss: \u001b[1m\u001b[32m0.05279\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 975 | loss: 0.05279 - acc: 0.9979 -- iter: 35/55\n","Training Step: 10722  | total loss: \u001b[1m\u001b[32m0.04838\u001b[0m\u001b[0m | time: 0.059s\n","| Adam | epoch: 975 | loss: 0.04838 - acc: 0.9982 -- iter: 40/55\n","Training Step: 10723  | total loss: \u001b[1m\u001b[32m0.04469\u001b[0m\u001b[0m | time: 0.068s\n","| Adam | epoch: 975 | loss: 0.04469 - acc: 0.9983 -- iter: 45/55\n","Training Step: 10724  | total loss: \u001b[1m\u001b[32m0.04184\u001b[0m\u001b[0m | time: 0.074s\n","| Adam | epoch: 975 | loss: 0.04184 - acc: 0.9985 -- iter: 50/55\n","Training Step: 10725  | total loss: \u001b[1m\u001b[32m0.03854\u001b[0m\u001b[0m | time: 0.082s\n","| Adam | epoch: 975 | loss: 0.03854 - acc: 0.9987 -- iter: 55/55\n","--\n","Training Step: 10726  | total loss: \u001b[1m\u001b[32m0.03561\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 976 | loss: 0.03561 - acc: 0.9988 -- iter: 05/55\n","Training Step: 10727  | total loss: \u001b[1m\u001b[32m0.03260\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 976 | loss: 0.03260 - acc: 0.9989 -- iter: 10/55\n","Training Step: 10728  | total loss: \u001b[1m\u001b[32m0.03146\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 976 | loss: 0.03146 - acc: 0.9990 -- iter: 15/55\n","Training Step: 10729  | total loss: \u001b[1m\u001b[32m0.03042\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 976 | loss: 0.03042 - acc: 0.9991 -- iter: 20/55\n","Training Step: 10730  | total loss: \u001b[1m\u001b[32m0.02779\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 976 | loss: 0.02779 - acc: 0.9992 -- iter: 25/55\n","Training Step: 10731  | total loss: \u001b[1m\u001b[32m0.02581\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 976 | loss: 0.02581 - acc: 0.9993 -- iter: 30/55\n","Training Step: 10732  | total loss: \u001b[1m\u001b[32m2.03372\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 976 | loss: 2.03372 - acc: 0.8994 -- iter: 35/55\n","Training Step: 10733  | total loss: \u001b[1m\u001b[32m1.83140\u001b[0m\u001b[0m | time: 0.059s\n","| Adam | epoch: 976 | loss: 1.83140 - acc: 0.9094 -- iter: 40/55\n","Training Step: 10734  | total loss: \u001b[1m\u001b[32m1.64919\u001b[0m\u001b[0m | time: 0.065s\n","| Adam | epoch: 976 | loss: 1.64919 - acc: 0.9185 -- iter: 45/55\n","Training Step: 10735  | total loss: \u001b[1m\u001b[32m1.48564\u001b[0m\u001b[0m | time: 0.074s\n","| Adam | epoch: 976 | loss: 1.48564 - acc: 0.9266 -- iter: 50/55\n","Training Step: 10736  | total loss: \u001b[1m\u001b[32m1.33736\u001b[0m\u001b[0m | time: 0.079s\n","| Adam | epoch: 976 | loss: 1.33736 - acc: 0.9340 -- iter: 55/55\n","--\n","Training Step: 10737  | total loss: \u001b[1m\u001b[32m1.20504\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 977 | loss: 1.20504 - acc: 0.9406 -- iter: 05/55\n","Training Step: 10738  | total loss: \u001b[1m\u001b[32m1.08556\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 977 | loss: 1.08556 - acc: 0.9465 -- iter: 10/55\n","Training Step: 10739  | total loss: \u001b[1m\u001b[32m0.97854\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 977 | loss: 0.97854 - acc: 0.9519 -- iter: 15/55\n","Training Step: 10740  | total loss: \u001b[1m\u001b[32m0.88126\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 977 | loss: 0.88126 - acc: 0.9567 -- iter: 20/55\n","Training Step: 10741  | total loss: \u001b[1m\u001b[32m0.79371\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 977 | loss: 0.79371 - acc: 0.9610 -- iter: 25/55\n","Training Step: 10742  | total loss: \u001b[1m\u001b[32m0.71523\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 977 | loss: 0.71523 - acc: 0.9649 -- iter: 30/55\n","Training Step: 10743  | total loss: \u001b[1m\u001b[32m0.64423\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 977 | loss: 0.64423 - acc: 0.9684 -- iter: 35/55\n","Training Step: 10744  | total loss: \u001b[1m\u001b[32m2.23495\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 977 | loss: 2.23495 - acc: 0.8716 -- iter: 40/55\n","Training Step: 10745  | total loss: \u001b[1m\u001b[32m2.01260\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 977 | loss: 2.01260 - acc: 0.8960 -- iter: 45/55\n","Training Step: 10746  | total loss: \u001b[1m\u001b[32m1.81223\u001b[0m\u001b[0m | time: 0.063s\n","| Adam | epoch: 977 | loss: 1.81223 - acc: 0.9064 -- iter: 50/55\n","Training Step: 10747  | total loss: \u001b[1m\u001b[32m1.63140\u001b[0m\u001b[0m | time: 0.069s\n","| Adam | epoch: 977 | loss: 1.63140 - acc: 0.9157 -- iter: 55/55\n","--\n","Training Step: 10748  | total loss: \u001b[1m\u001b[32m1.46879\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 978 | loss: 1.46879 - acc: 0.9157 -- iter: 05/55\n","Training Step: 10749  | total loss: \u001b[1m\u001b[32m1.32396\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 978 | loss: 1.32396 - acc: 0.9317 -- iter: 10/55\n","Training Step: 10750  | total loss: \u001b[1m\u001b[32m1.19321\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 978 | loss: 1.19321 - acc: 0.9386 -- iter: 15/55\n","Training Step: 10751  | total loss: \u001b[1m\u001b[32m1.07557\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 978 | loss: 1.07557 - acc: 0.9386 -- iter: 20/55\n","Training Step: 10752  | total loss: \u001b[1m\u001b[32m0.96836\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 978 | loss: 0.96836 - acc: 0.9447 -- iter: 25/55\n","Training Step: 10753  | total loss: \u001b[1m\u001b[32m0.87187\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 978 | loss: 0.87187 - acc: 0.9502 -- iter: 30/55\n","Training Step: 10754  | total loss: \u001b[1m\u001b[32m0.78509\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 978 | loss: 0.78509 - acc: 0.9552 -- iter: 35/55\n","Training Step: 10755  | total loss: \u001b[1m\u001b[32m0.70817\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 978 | loss: 0.70817 - acc: 0.9597 -- iter: 40/55\n","Training Step: 10756  | total loss: \u001b[1m\u001b[32m0.63823\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 978 | loss: 0.63823 - acc: 0.9637 -- iter: 45/55\n","Training Step: 10757  | total loss: \u001b[1m\u001b[32m0.57510\u001b[0m\u001b[0m | time: 0.060s\n","| Adam | epoch: 978 | loss: 0.57510 - acc: 0.9674 -- iter: 50/55\n","Training Step: 10758  | total loss: \u001b[1m\u001b[32m0.51847\u001b[0m\u001b[0m | time: 0.068s\n","| Adam | epoch: 978 | loss: 0.51847 - acc: 0.9706 -- iter: 55/55\n","--\n","Training Step: 10759  | total loss: \u001b[1m\u001b[32m0.46829\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 979 | loss: 0.46829 - acc: 0.9736 -- iter: 05/55\n","Training Step: 10760  | total loss: \u001b[1m\u001b[32m0.42297\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 979 | loss: 0.42297 - acc: 0.9762 -- iter: 10/55\n","Training Step: 10761  | total loss: \u001b[1m\u001b[32m0.38096\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 979 | loss: 0.38096 - acc: 0.9786 -- iter: 15/55\n","Training Step: 10762  | total loss: \u001b[1m\u001b[32m0.34414\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 979 | loss: 0.34414 - acc: 0.9807 -- iter: 20/55\n","Training Step: 10763  | total loss: \u001b[1m\u001b[32m0.31082\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 979 | loss: 0.31082 - acc: 0.9827 -- iter: 25/55\n","Training Step: 10764  | total loss: \u001b[1m\u001b[32m0.28061\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 979 | loss: 0.28061 - acc: 0.9844 -- iter: 30/55\n","Training Step: 10765  | total loss: \u001b[1m\u001b[32m0.25343\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 979 | loss: 0.25343 - acc: 0.9859 -- iter: 35/55\n","Training Step: 10766  | total loss: \u001b[1m\u001b[32m0.22883\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 979 | loss: 0.22883 - acc: 0.9874 -- iter: 40/55\n","Training Step: 10767  | total loss: \u001b[1m\u001b[32m0.20669\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 979 | loss: 0.20669 - acc: 0.9886 -- iter: 45/55\n","Training Step: 10768  | total loss: \u001b[1m\u001b[32m0.18781\u001b[0m\u001b[0m | time: 0.059s\n","| Adam | epoch: 979 | loss: 0.18781 - acc: 0.9898 -- iter: 50/55\n","Training Step: 10769  | total loss: \u001b[1m\u001b[32m0.17037\u001b[0m\u001b[0m | time: 0.065s\n","| Adam | epoch: 979 | loss: 0.17037 - acc: 0.9908 -- iter: 55/55\n","--\n","Training Step: 10770  | total loss: \u001b[1m\u001b[32m0.15438\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 980 | loss: 0.15438 - acc: 0.9917 -- iter: 05/55\n","Training Step: 10771  | total loss: \u001b[1m\u001b[32m0.14049\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 980 | loss: 0.14049 - acc: 0.9925 -- iter: 10/55\n","Training Step: 10772  | total loss: \u001b[1m\u001b[32m0.12697\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 980 | loss: 0.12697 - acc: 0.9940 -- iter: 15/55\n","Training Step: 10773  | total loss: \u001b[1m\u001b[32m0.11526\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 980 | loss: 0.11526 - acc: 0.9940 -- iter: 20/55\n","Training Step: 10774  | total loss: \u001b[1m\u001b[32m0.10435\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 980 | loss: 0.10435 - acc: 0.9946 -- iter: 25/55\n","Training Step: 10775  | total loss: \u001b[1m\u001b[32m0.09592\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 980 | loss: 0.09592 - acc: 0.9951 -- iter: 30/55\n","Training Step: 10776  | total loss: \u001b[1m\u001b[32m0.08712\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 980 | loss: 0.08712 - acc: 0.9956 -- iter: 35/55\n","Training Step: 10777  | total loss: \u001b[1m\u001b[32m0.07918\u001b[0m\u001b[0m | time: 0.063s\n","| Adam | epoch: 980 | loss: 0.07918 - acc: 0.9960 -- iter: 40/55\n","Training Step: 10778  | total loss: \u001b[1m\u001b[32m0.07231\u001b[0m\u001b[0m | time: 0.070s\n","| Adam | epoch: 980 | loss: 0.07231 - acc: 0.9964 -- iter: 45/55\n","Training Step: 10779  | total loss: \u001b[1m\u001b[32m0.06627\u001b[0m\u001b[0m | time: 0.078s\n","| Adam | epoch: 980 | loss: 0.06627 - acc: 0.9968 -- iter: 50/55\n","Training Step: 10780  | total loss: \u001b[1m\u001b[32m0.05989\u001b[0m\u001b[0m | time: 0.084s\n","| Adam | epoch: 980 | loss: 0.05989 - acc: 0.9971 -- iter: 55/55\n","--\n","Training Step: 10781  | total loss: \u001b[1m\u001b[32m0.05437\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 981 | loss: 0.05437 - acc: 0.9974 -- iter: 05/55\n","Training Step: 10782  | total loss: \u001b[1m\u001b[32m0.05059\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 981 | loss: 0.05059 - acc: 0.9977 -- iter: 10/55\n","Training Step: 10783  | total loss: \u001b[1m\u001b[32m0.04651\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 981 | loss: 0.04651 - acc: 0.9979 -- iter: 15/55\n","Training Step: 10784  | total loss: \u001b[1m\u001b[32m0.04331\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 981 | loss: 0.04331 - acc: 0.9981 -- iter: 20/55\n","Training Step: 10785  | total loss: \u001b[1m\u001b[32m0.04018\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 981 | loss: 0.04018 - acc: 0.9983 -- iter: 25/55\n","Training Step: 10786  | total loss: \u001b[1m\u001b[32m0.03674\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 981 | loss: 0.03674 - acc: 0.9985 -- iter: 30/55\n","Training Step: 10787  | total loss: \u001b[1m\u001b[32m0.03468\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 981 | loss: 0.03468 - acc: 0.9986 -- iter: 35/55\n","Training Step: 10788  | total loss: \u001b[1m\u001b[32m0.03224\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 981 | loss: 0.03224 - acc: 0.9988 -- iter: 40/55\n","Training Step: 10789  | total loss: \u001b[1m\u001b[32m0.03005\u001b[0m\u001b[0m | time: 0.059s\n","| Adam | epoch: 981 | loss: 0.03005 - acc: 0.9989 -- iter: 45/55\n","Training Step: 10790  | total loss: \u001b[1m\u001b[32m0.02720\u001b[0m\u001b[0m | time: 0.068s\n","| Adam | epoch: 981 | loss: 0.02720 - acc: 0.9990 -- iter: 50/55\n","Training Step: 10791  | total loss: \u001b[1m\u001b[32m0.02558\u001b[0m\u001b[0m | time: 0.075s\n","| Adam | epoch: 981 | loss: 0.02558 - acc: 0.9991 -- iter: 55/55\n","--\n","Training Step: 10792  | total loss: \u001b[1m\u001b[32m0.02373\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 982 | loss: 0.02373 - acc: 0.9992 -- iter: 05/55\n","Training Step: 10793  | total loss: \u001b[1m\u001b[32m0.02068\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 982 | loss: 0.02068 - acc: 0.9993 -- iter: 10/55\n","Training Step: 10794  | total loss: \u001b[1m\u001b[32m0.02068\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 982 | loss: 0.02068 - acc: 0.9993 -- iter: 15/55\n","Training Step: 10795  | total loss: \u001b[1m\u001b[32m0.01933\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 982 | loss: 0.01933 - acc: 0.9994 -- iter: 20/55\n","Training Step: 10796  | total loss: \u001b[1m\u001b[32m0.01831\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 982 | loss: 0.01831 - acc: 0.9995 -- iter: 25/55\n","Training Step: 10797  | total loss: \u001b[1m\u001b[32m0.01806\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 982 | loss: 0.01806 - acc: 0.9995 -- iter: 30/55\n","Training Step: 10798  | total loss: \u001b[1m\u001b[32m0.01814\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 982 | loss: 0.01814 - acc: 0.9996 -- iter: 35/55\n","Training Step: 10799  | total loss: \u001b[1m\u001b[32m0.01771\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 982 | loss: 0.01771 - acc: 0.9996 -- iter: 40/55\n","Training Step: 10800  | total loss: \u001b[1m\u001b[32m0.01655\u001b[0m\u001b[0m | time: 0.059s\n","| Adam | epoch: 982 | loss: 0.01655 - acc: 0.9996 -- iter: 45/55\n","Training Step: 10801  | total loss: \u001b[1m\u001b[32m0.01549\u001b[0m\u001b[0m | time: 0.065s\n","| Adam | epoch: 982 | loss: 0.01549 - acc: 0.9997 -- iter: 50/55\n","Training Step: 10802  | total loss: \u001b[1m\u001b[32m0.01464\u001b[0m\u001b[0m | time: 0.074s\n","| Adam | epoch: 982 | loss: 0.01464 - acc: 0.9997 -- iter: 55/55\n","--\n","Training Step: 10803  | total loss: \u001b[1m\u001b[32m0.01305\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 983 | loss: 0.01305 - acc: 0.9997 -- iter: 05/55\n","Training Step: 10804  | total loss: \u001b[1m\u001b[32m0.01305\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 983 | loss: 0.01305 - acc: 0.9998 -- iter: 10/55\n","Training Step: 10805  | total loss: \u001b[1m\u001b[32m0.01257\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 983 | loss: 0.01257 - acc: 0.9998 -- iter: 15/55\n","Training Step: 10806  | total loss: \u001b[1m\u001b[32m0.01208\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 983 | loss: 0.01208 - acc: 0.9998 -- iter: 20/55\n","Training Step: 10807  | total loss: \u001b[1m\u001b[32m0.01230\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 983 | loss: 0.01230 - acc: 0.9998 -- iter: 25/55\n","Training Step: 10808  | total loss: \u001b[1m\u001b[32m0.01173\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 983 | loss: 0.01173 - acc: 0.9999 -- iter: 30/55\n","Training Step: 10809  | total loss: \u001b[1m\u001b[32m0.01126\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 983 | loss: 0.01126 - acc: 0.9999 -- iter: 35/55\n","Training Step: 10810  | total loss: \u001b[1m\u001b[32m0.01111\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 983 | loss: 0.01111 - acc: 0.9999 -- iter: 40/55\n","Training Step: 10811  | total loss: \u001b[1m\u001b[32m0.01136\u001b[0m\u001b[0m | time: 0.065s\n","| Adam | epoch: 983 | loss: 0.01136 - acc: 0.9999 -- iter: 45/55\n","Training Step: 10812  | total loss: \u001b[1m\u001b[32m0.01124\u001b[0m\u001b[0m | time: 0.072s\n","| Adam | epoch: 983 | loss: 0.01124 - acc: 0.9999 -- iter: 50/55\n","Training Step: 10813  | total loss: \u001b[1m\u001b[32m0.01113\u001b[0m\u001b[0m | time: 0.079s\n","| Adam | epoch: 983 | loss: 0.01113 - acc: 0.9999 -- iter: 55/55\n","--\n","Training Step: 10814  | total loss: \u001b[1m\u001b[32m0.01118\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 984 | loss: 0.01118 - acc: 0.9999 -- iter: 05/55\n","Training Step: 10815  | total loss: \u001b[1m\u001b[32m0.01076\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 984 | loss: 0.01076 - acc: 0.9999 -- iter: 10/55\n","Training Step: 10816  | total loss: \u001b[1m\u001b[32m0.01034\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 984 | loss: 0.01034 - acc: 0.9999 -- iter: 15/55\n","Training Step: 10817  | total loss: \u001b[1m\u001b[32m0.00949\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 984 | loss: 0.00949 - acc: 0.9999 -- iter: 20/55\n","Training Step: 10818  | total loss: \u001b[1m\u001b[32m0.00929\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 984 | loss: 0.00929 - acc: 0.9999 -- iter: 25/55\n","Training Step: 10819  | total loss: \u001b[1m\u001b[32m0.00979\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 984 | loss: 0.00979 - acc: 1.0000 -- iter: 30/55\n","Training Step: 10820  | total loss: \u001b[1m\u001b[32m0.01029\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 984 | loss: 0.01029 - acc: 1.0000 -- iter: 35/55\n","Training Step: 10821  | total loss: \u001b[1m\u001b[32m0.01013\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 984 | loss: 0.01013 - acc: 1.0000 -- iter: 40/55\n","Training Step: 10822  | total loss: \u001b[1m\u001b[32m0.00987\u001b[0m\u001b[0m | time: 0.059s\n","| Adam | epoch: 984 | loss: 0.00987 - acc: 1.0000 -- iter: 45/55\n","Training Step: 10823  | total loss: \u001b[1m\u001b[32m0.00965\u001b[0m\u001b[0m | time: 0.069s\n","| Adam | epoch: 984 | loss: 0.00965 - acc: 1.0000 -- iter: 50/55\n","Training Step: 10824  | total loss: \u001b[1m\u001b[32m0.00966\u001b[0m\u001b[0m | time: 0.074s\n","| Adam | epoch: 984 | loss: 0.00966 - acc: 1.0000 -- iter: 55/55\n","--\n","Training Step: 10825  | total loss: \u001b[1m\u001b[32m0.00966\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 985 | loss: 0.00966 - acc: 1.0000 -- iter: 05/55\n","Training Step: 10826  | total loss: \u001b[1m\u001b[32m0.00976\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 985 | loss: 0.00976 - acc: 1.0000 -- iter: 10/55\n","Training Step: 10827  | total loss: \u001b[1m\u001b[32m0.00936\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 985 | loss: 0.00936 - acc: 1.0000 -- iter: 15/55\n","Training Step: 10828  | total loss: \u001b[1m\u001b[32m1.05735\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 985 | loss: 1.05735 - acc: 0.9000 -- iter: 20/55\n","Training Step: 10829  | total loss: \u001b[1m\u001b[32m0.95242\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 985 | loss: 0.95242 - acc: 0.9100 -- iter: 25/55\n","Training Step: 10830  | total loss: \u001b[1m\u001b[32m0.85876\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 985 | loss: 0.85876 - acc: 0.9190 -- iter: 30/55\n","Training Step: 10831  | total loss: \u001b[1m\u001b[32m0.77440\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 985 | loss: 0.77440 - acc: 0.9271 -- iter: 35/55\n","Training Step: 10832  | total loss: \u001b[1m\u001b[32m0.69780\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 985 | loss: 0.69780 - acc: 0.9344 -- iter: 40/55\n","Training Step: 10833  | total loss: \u001b[1m\u001b[32m0.62934\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 985 | loss: 0.62934 - acc: 0.9409 -- iter: 45/55\n","Training Step: 10834  | total loss: \u001b[1m\u001b[32m0.56680\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 985 | loss: 0.56680 - acc: 0.9468 -- iter: 50/55\n","Training Step: 10835  | total loss: \u001b[1m\u001b[32m0.51064\u001b[0m\u001b[0m | time: 0.067s\n","| Adam | epoch: 985 | loss: 0.51064 - acc: 0.9522 -- iter: 55/55\n","--\n","Training Step: 10836  | total loss: \u001b[1m\u001b[32m0.45991\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 986 | loss: 0.45991 - acc: 0.9569 -- iter: 05/55\n","Training Step: 10837  | total loss: \u001b[1m\u001b[32m0.37390\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 986 | loss: 0.37390 - acc: 0.9613 -- iter: 10/55\n","Training Step: 10838  | total loss: \u001b[1m\u001b[32m0.37390\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 986 | loss: 0.37390 - acc: 0.9651 -- iter: 15/55\n","Training Step: 10839  | total loss: \u001b[1m\u001b[32m0.33727\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 986 | loss: 0.33727 - acc: 0.9686 -- iter: 20/55\n","Training Step: 10840  | total loss: \u001b[1m\u001b[32m1.70256\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 986 | loss: 1.70256 - acc: 0.8718 -- iter: 25/55\n","Training Step: 10841  | total loss: \u001b[1m\u001b[32m1.53315\u001b[0m\u001b[0m | time: 0.037s\n","| Adam | epoch: 986 | loss: 1.53315 - acc: 0.8846 -- iter: 30/55\n","Training Step: 10842  | total loss: \u001b[1m\u001b[32m1.38023\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 986 | loss: 1.38023 - acc: 0.8961 -- iter: 35/55\n","Training Step: 10843  | total loss: \u001b[1m\u001b[32m1.24405\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 986 | loss: 1.24405 - acc: 0.9065 -- iter: 40/55\n","Training Step: 10844  | total loss: \u001b[1m\u001b[32m1.12097\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 986 | loss: 1.12097 - acc: 0.9159 -- iter: 45/55\n","Training Step: 10845  | total loss: \u001b[1m\u001b[32m1.00963\u001b[0m\u001b[0m | time: 0.066s\n","| Adam | epoch: 986 | loss: 1.00963 - acc: 0.9243 -- iter: 50/55\n","Training Step: 10846  | total loss: \u001b[1m\u001b[32m0.90976\u001b[0m\u001b[0m | time: 0.072s\n","| Adam | epoch: 986 | loss: 0.90976 - acc: 0.9318 -- iter: 55/55\n","--\n","Training Step: 10847  | total loss: \u001b[1m\u001b[32m0.81933\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 987 | loss: 0.81933 - acc: 0.9387 -- iter: 05/55\n","Training Step: 10848  | total loss: \u001b[1m\u001b[32m0.66583\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 987 | loss: 0.66583 - acc: 0.9448 -- iter: 10/55\n","Training Step: 10849  | total loss: \u001b[1m\u001b[32m0.66583\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 987 | loss: 0.66583 - acc: 0.9503 -- iter: 15/55\n","Training Step: 10850  | total loss: \u001b[1m\u001b[32m0.60015\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 987 | loss: 0.60015 - acc: 0.9553 -- iter: 20/55\n","Training Step: 10851  | total loss: \u001b[1m\u001b[32m0.54072\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 987 | loss: 0.54072 - acc: 0.9598 -- iter: 25/55\n","Training Step: 10852  | total loss: \u001b[1m\u001b[32m1.78778\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 987 | loss: 1.78778 - acc: 0.8638 -- iter: 30/55\n","Training Step: 10853  | total loss: \u001b[1m\u001b[32m1.61019\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 987 | loss: 1.61019 - acc: 0.8774 -- iter: 35/55\n","Training Step: 10854  | total loss: \u001b[1m\u001b[32m1.45028\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 987 | loss: 1.45028 - acc: 0.8897 -- iter: 40/55\n","Training Step: 10855  | total loss: \u001b[1m\u001b[32m1.30677\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 987 | loss: 1.30677 - acc: 0.9007 -- iter: 45/55\n","Training Step: 10856  | total loss: \u001b[1m\u001b[32m1.17723\u001b[0m\u001b[0m | time: 0.064s\n","| Adam | epoch: 987 | loss: 1.17723 - acc: 0.9106 -- iter: 50/55\n","Training Step: 10857  | total loss: \u001b[1m\u001b[32m1.06002\u001b[0m\u001b[0m | time: 0.068s\n","| Adam | epoch: 987 | loss: 1.06002 - acc: 0.9196 -- iter: 55/55\n","--\n","Training Step: 10858  | total loss: \u001b[1m\u001b[32m0.95503\u001b[0m\u001b[0m | time: 0.008s\n","| Adam | epoch: 988 | loss: 0.95503 - acc: 0.9276 -- iter: 05/55\n","Training Step: 10859  | total loss: \u001b[1m\u001b[32m0.86013\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 988 | loss: 0.86013 - acc: 0.9348 -- iter: 10/55\n","Training Step: 10860  | total loss: \u001b[1m\u001b[32m0.77568\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 988 | loss: 0.77568 - acc: 0.9414 -- iter: 15/55\n","Training Step: 10861  | total loss: \u001b[1m\u001b[32m0.63050\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 988 | loss: 0.63050 - acc: 0.9472 -- iter: 20/55\n","Training Step: 10862  | total loss: \u001b[1m\u001b[32m0.63050\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 988 | loss: 0.63050 - acc: 0.9525 -- iter: 25/55\n","Training Step: 10863  | total loss: \u001b[1m\u001b[32m2.12232\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 988 | loss: 2.12232 - acc: 0.9573 -- iter: 30/55\n","Training Step: 10864  | total loss: \u001b[1m\u001b[32m2.12232\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 988 | loss: 2.12232 - acc: 0.8615 -- iter: 35/55\n","Training Step: 10865  | total loss: \u001b[1m\u001b[32m1.91104\u001b[0m\u001b[0m | time: 0.054s\n","| Adam | epoch: 988 | loss: 1.91104 - acc: 0.8878 -- iter: 40/55\n","Training Step: 10866  | total loss: \u001b[1m\u001b[32m1.72073\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 988 | loss: 1.72073 - acc: 0.8878 -- iter: 45/55\n","Training Step: 10867  | total loss: \u001b[1m\u001b[32m1.54986\u001b[0m\u001b[0m | time: 0.068s\n","| Adam | epoch: 988 | loss: 1.54986 - acc: 0.9091 -- iter: 50/55\n","Training Step: 10868  | total loss: \u001b[1m\u001b[32m1.39550\u001b[0m\u001b[0m | time: 0.074s\n","| Adam | epoch: 988 | loss: 1.39550 - acc: 0.9091 -- iter: 55/55\n","--\n","Training Step: 10869  | total loss: \u001b[1m\u001b[32m1.25730\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 989 | loss: 1.25730 - acc: 0.9182 -- iter: 05/55\n","Training Step: 10870  | total loss: \u001b[1m\u001b[32m1.13272\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 989 | loss: 1.13272 - acc: 0.9264 -- iter: 10/55\n","Training Step: 10871  | total loss: \u001b[1m\u001b[32m1.02064\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 989 | loss: 1.02064 - acc: 0.9338 -- iter: 15/55\n","Training Step: 10872  | total loss: \u001b[1m\u001b[32m0.91955\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 989 | loss: 0.91955 - acc: 0.9404 -- iter: 20/55\n","Training Step: 10873  | total loss: \u001b[1m\u001b[32m0.82858\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 989 | loss: 0.82858 - acc: 0.9517 -- iter: 25/55\n","Training Step: 10874  | total loss: \u001b[1m\u001b[32m0.74683\u001b[0m\u001b[0m | time: 0.050s\n","| Adam | epoch: 989 | loss: 0.74683 - acc: 0.9565 -- iter: 30/55\n","Training Step: 10875  | total loss: \u001b[1m\u001b[32m0.67378\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 989 | loss: 0.67378 - acc: 0.8809 -- iter: 35/55\n","Training Step: 10876  | total loss: \u001b[1m\u001b[32m1.97750\u001b[0m\u001b[0m | time: 0.065s\n","| Adam | epoch: 989 | loss: 1.97750 - acc: 0.8809 -- iter: 40/55\n","Training Step: 10877  | total loss: \u001b[1m\u001b[32m1.78057\u001b[0m\u001b[0m | time: 0.073s\n","| Adam | epoch: 989 | loss: 1.78057 - acc: 0.8928 -- iter: 45/55\n","Training Step: 10878  | total loss: \u001b[1m\u001b[32m1.60478\u001b[0m\u001b[0m | time: 0.083s\n","| Adam | epoch: 989 | loss: 1.60478 - acc: 0.9035 -- iter: 50/55\n","Training Step: 10879  | total loss: \u001b[1m\u001b[32m1.44531\u001b[0m\u001b[0m | time: 0.090s\n","| Adam | epoch: 989 | loss: 1.44531 - acc: 0.9132 -- iter: 55/55\n","--\n","Training Step: 10880  | total loss: \u001b[1m\u001b[32m1.30315\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 990 | loss: 1.30315 - acc: 0.9219 -- iter: 05/55\n","Training Step: 10881  | total loss: \u001b[1m\u001b[32m1.17429\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 990 | loss: 1.17429 - acc: 0.9297 -- iter: 10/55\n","Training Step: 10882  | total loss: \u001b[1m\u001b[32m1.05748\u001b[0m\u001b[0m | time: 0.026s\n","| Adam | epoch: 990 | loss: 1.05748 - acc: 0.9367 -- iter: 15/55\n","Training Step: 10883  | total loss: \u001b[1m\u001b[32m0.95265\u001b[0m\u001b[0m | time: 0.036s\n","| Adam | epoch: 990 | loss: 0.95265 - acc: 0.9430 -- iter: 20/55\n","Training Step: 10884  | total loss: \u001b[1m\u001b[32m0.85834\u001b[0m\u001b[0m | time: 0.040s\n","| Adam | epoch: 990 | loss: 0.85834 - acc: 0.9539 -- iter: 25/55\n","Training Step: 10885  | total loss: \u001b[1m\u001b[32m0.77350\u001b[0m\u001b[0m | time: 0.044s\n","| Adam | epoch: 990 | loss: 0.77350 - acc: 0.9585 -- iter: 30/55\n","Training Step: 10886  | total loss: \u001b[1m\u001b[32m0.69766\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 990 | loss: 0.69766 - acc: 0.9626 -- iter: 35/55\n","Training Step: 10887  | total loss: \u001b[1m\u001b[32m0.62846\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 990 | loss: 0.62846 - acc: 0.8664 -- iter: 40/55\n","Training Step: 10888  | total loss: \u001b[1m\u001b[32m2.03042\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 990 | loss: 2.03042 - acc: 0.8797 -- iter: 45/55\n","Training Step: 10889  | total loss: \u001b[1m\u001b[32m1.82825\u001b[0m\u001b[0m | time: 0.063s\n","| Adam | epoch: 990 | loss: 1.82825 - acc: 0.8918 -- iter: 50/55\n","Training Step: 10890  | total loss: \u001b[1m\u001b[32m1.64767\u001b[0m\u001b[0m | time: 0.067s\n","| Adam | epoch: 990 | loss: 1.64767 - acc: 0.9026 -- iter: 55/55\n","--\n","Training Step: 10891  | total loss: \u001b[1m\u001b[32m1.48321\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 991 | loss: 1.48321 - acc: 0.9123 -- iter: 05/55\n","Training Step: 10892  | total loss: \u001b[1m\u001b[32m1.33639\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 991 | loss: 1.33639 - acc: 0.9211 -- iter: 10/55\n","Training Step: 10893  | total loss: \u001b[1m\u001b[32m1.20346\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 991 | loss: 1.20346 - acc: 0.9290 -- iter: 15/55\n","Training Step: 10894  | total loss: \u001b[1m\u001b[32m0.97696\u001b[0m\u001b[0m | time: 0.018s\n","| Adam | epoch: 991 | loss: 0.97696 - acc: 0.9361 -- iter: 20/55\n","Training Step: 10895  | total loss: \u001b[1m\u001b[32m0.88033\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 991 | loss: 0.88033 - acc: 0.9425 -- iter: 25/55\n","Training Step: 10896  | total loss: \u001b[1m\u001b[32m0.79337\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 991 | loss: 0.79337 - acc: 0.9425 -- iter: 30/55\n","Training Step: 10897  | total loss: \u001b[1m\u001b[32m0.71504\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 991 | loss: 0.71504 - acc: 0.9482 -- iter: 35/55\n","Training Step: 10898  | total loss: \u001b[1m\u001b[32m0.64563\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 991 | loss: 0.64563 - acc: 0.9534 -- iter: 40/55\n","Training Step: 10899  | total loss: \u001b[1m\u001b[32m2.07265\u001b[0m\u001b[0m | time: 0.045s\n","| Adam | epoch: 991 | loss: 2.07265 - acc: 0.9581 -- iter: 45/55\n","Training Step: 10900  | total loss: \u001b[1m\u001b[32m1.86628\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 991 | loss: 1.86628 - acc: 0.8823 -- iter: 50/55\n","Training Step: 10901  | total loss: \u001b[1m\u001b[32m1.68044\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 991 | loss: 1.68044 - acc: 0.8940 -- iter: 55/55\n","--\n","Training Step: 10902  | total loss: \u001b[1m\u001b[32m1.51409\u001b[0m\u001b[0m | time: 0.005s\n","| Adam | epoch: 992 | loss: 1.51409 - acc: 0.9046 -- iter: 05/55\n","Training Step: 10903  | total loss: \u001b[1m\u001b[32m1.36423\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 992 | loss: 1.36423 - acc: 0.9142 -- iter: 10/55\n","Training Step: 10904  | total loss: \u001b[1m\u001b[32m1.22958\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 992 | loss: 1.22958 - acc: 0.9227 -- iter: 15/55\n","Training Step: 10905  | total loss: \u001b[1m\u001b[32m1.10800\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 992 | loss: 1.10800 - acc: 0.9305 -- iter: 20/55\n","Training Step: 10906  | total loss: \u001b[1m\u001b[32m0.99955\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 992 | loss: 0.99955 - acc: 0.9374 -- iter: 25/55\n","Training Step: 10907  | total loss: \u001b[1m\u001b[32m0.90019\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 992 | loss: 0.90019 - acc: 0.9437 -- iter: 30/55\n","Training Step: 10908  | total loss: \u001b[1m\u001b[32m0.81074\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 992 | loss: 0.81074 - acc: 0.9493 -- iter: 35/55\n","Training Step: 10909  | total loss: \u001b[1m\u001b[32m0.73008\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 992 | loss: 0.73008 - acc: 0.9544 -- iter: 40/55\n","Training Step: 10910  | total loss: \u001b[1m\u001b[32m0.65832\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 992 | loss: 0.65832 - acc: 0.9589 -- iter: 45/55\n","Training Step: 10911  | total loss: \u001b[1m\u001b[32m2.20623\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 992 | loss: 2.20623 - acc: 0.9631 -- iter: 50/55\n","Training Step: 10912  | total loss: \u001b[1m\u001b[32m1.98682\u001b[0m\u001b[0m | time: 0.060s\n","| Adam | epoch: 992 | loss: 1.98682 - acc: 0.8667 -- iter: 55/55\n","--\n","Training Step: 10913  | total loss: \u001b[1m\u001b[32m1.78919\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 993 | loss: 1.78919 - acc: 0.8801 -- iter: 05/55\n","Training Step: 10914  | total loss: \u001b[1m\u001b[32m1.61179\u001b[0m\u001b[0m | time: 0.012s\n","| Adam | epoch: 993 | loss: 1.61179 - acc: 0.8921 -- iter: 10/55\n","Training Step: 10915  | total loss: \u001b[1m\u001b[32m1.45259\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 993 | loss: 1.45259 - acc: 0.9029 -- iter: 15/55\n","Training Step: 10916  | total loss: \u001b[1m\u001b[32m1.30888\u001b[0m\u001b[0m | time: 0.023s\n","| Adam | epoch: 993 | loss: 1.30888 - acc: 0.9126 -- iter: 20/55\n","Training Step: 10917  | total loss: \u001b[1m\u001b[32m1.17879\u001b[0m\u001b[0m | time: 0.029s\n","| Adam | epoch: 993 | loss: 1.17879 - acc: 0.9292 -- iter: 25/55\n","Training Step: 10918  | total loss: \u001b[1m\u001b[32m1.17879\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 993 | loss: 1.17879 - acc: 0.9292 -- iter: 30/55\n","Training Step: 10919  | total loss: \u001b[1m\u001b[32m1.06340\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 993 | loss: 1.06340 - acc: 0.9363 -- iter: 35/55\n","Training Step: 10920  | total loss: \u001b[1m\u001b[32m0.95925\u001b[0m\u001b[0m | time: 0.048s\n","| Adam | epoch: 993 | loss: 0.95925 - acc: 0.9426 -- iter: 40/55\n","Training Step: 10921  | total loss: \u001b[1m\u001b[32m0.86553\u001b[0m\u001b[0m | time: 0.055s\n","| Adam | epoch: 993 | loss: 0.86553 - acc: 0.9484 -- iter: 45/55\n","Training Step: 10922  | total loss: \u001b[1m\u001b[32m0.70328\u001b[0m\u001b[0m | time: 0.061s\n","| Adam | epoch: 993 | loss: 0.70328 - acc: 0.9582 -- iter: 50/55\n","Training Step: 10923  | total loss: \u001b[1m\u001b[32m0.63380\u001b[0m\u001b[0m | time: 0.066s\n","| Adam | epoch: 993 | loss: 0.63380 - acc: 0.9624 -- iter: 55/55\n","--\n","Training Step: 10924  | total loss: \u001b[1m\u001b[32m0.57181\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 994 | loss: 0.57181 - acc: 0.9624 -- iter: 05/55\n","Training Step: 10925  | total loss: \u001b[1m\u001b[32m0.51689\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 994 | loss: 0.51689 - acc: 0.9661 -- iter: 10/55\n","Training Step: 10926  | total loss: \u001b[1m\u001b[32m0.46728\u001b[0m\u001b[0m | time: 0.017s\n","| Adam | epoch: 994 | loss: 0.46728 - acc: 0.9695 -- iter: 15/55\n","Training Step: 10927  | total loss: \u001b[1m\u001b[32m0.46728\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 994 | loss: 0.46728 - acc: 0.9726 -- iter: 20/55\n","Training Step: 10928  | total loss: \u001b[1m\u001b[32m0.42272\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 994 | loss: 0.42272 - acc: 0.9753 -- iter: 25/55\n","Training Step: 10929  | total loss: \u001b[1m\u001b[32m0.38201\u001b[0m\u001b[0m | time: 0.032s\n","| Adam | epoch: 994 | loss: 0.38201 - acc: 0.9778 -- iter: 30/55\n","Training Step: 10930  | total loss: \u001b[1m\u001b[32m0.34432\u001b[0m\u001b[0m | time: 0.038s\n","| Adam | epoch: 994 | loss: 0.34432 - acc: 0.9800 -- iter: 35/55\n","Training Step: 10931  | total loss: \u001b[1m\u001b[32m0.31097\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 994 | loss: 0.31097 - acc: 0.9820 -- iter: 40/55\n","Training Step: 10932  | total loss: \u001b[1m\u001b[32m0.28018\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 994 | loss: 0.28018 - acc: 0.9838 -- iter: 45/55\n","Training Step: 10933  | total loss: \u001b[1m\u001b[32m0.25246\u001b[0m\u001b[0m | time: 0.052s\n","| Adam | epoch: 994 | loss: 0.25246 - acc: 0.9854 -- iter: 50/55\n","Training Step: 10934  | total loss: \u001b[1m\u001b[32m0.22772\u001b[0m\u001b[0m | time: 0.056s\n","| Adam | epoch: 994 | loss: 0.22772 - acc: 0.9869 -- iter: 55/55\n","--\n","Training Step: 10935  | total loss: \u001b[1m\u001b[32m0.20623\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 995 | loss: 0.20623 - acc: 0.9882 -- iter: 05/55\n","Training Step: 10936  | total loss: \u001b[1m\u001b[32m1.60748\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 995 | loss: 1.60748 - acc: 0.8894 -- iter: 10/55\n","Training Step: 10937  | total loss: \u001b[1m\u001b[32m1.60748\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 995 | loss: 1.60748 - acc: 0.9004 -- iter: 15/55\n","Training Step: 10938  | total loss: \u001b[1m\u001b[32m1.44752\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 995 | loss: 1.44752 - acc: 0.9104 -- iter: 20/55\n","Training Step: 10939  | total loss: \u001b[1m\u001b[32m1.30398\u001b[0m\u001b[0m | time: 0.021s\n","| Adam | epoch: 995 | loss: 1.30398 - acc: 0.9194 -- iter: 25/55\n","Training Step: 10940  | total loss: \u001b[1m\u001b[32m1.17430\u001b[0m\u001b[0m | time: 0.027s\n","| Adam | epoch: 995 | loss: 1.17430 - acc: 0.9274 -- iter: 30/55\n","Training Step: 10941  | total loss: \u001b[1m\u001b[32m1.05876\u001b[0m\u001b[0m | time: 0.033s\n","| Adam | epoch: 995 | loss: 1.05876 - acc: 0.9347 -- iter: 35/55\n","Training Step: 10942  | total loss: \u001b[1m\u001b[32m0.95393\u001b[0m\u001b[0m | time: 0.039s\n","| Adam | epoch: 995 | loss: 0.95393 - acc: 0.9412 -- iter: 40/55\n","Training Step: 10943  | total loss: \u001b[1m\u001b[32m0.86305\u001b[0m\u001b[0m | time: 0.043s\n","| Adam | epoch: 995 | loss: 0.86305 - acc: 0.9471 -- iter: 45/55\n","Training Step: 10944  | total loss: \u001b[1m\u001b[32m0.69964\u001b[0m\u001b[0m | time: 0.049s\n","| Adam | epoch: 995 | loss: 0.69964 - acc: 0.9524 -- iter: 50/55\n","Training Step: 10945  | total loss: \u001b[1m\u001b[32m0.63100\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 995 | loss: 0.63100 - acc: 0.9571 -- iter: 55/55\n","--\n","Training Step: 10946  | total loss: \u001b[1m\u001b[32m0.56903\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 996 | loss: 0.56903 - acc: 0.9614 -- iter: 05/55\n","Training Step: 10947  | total loss: \u001b[1m\u001b[32m0.51269\u001b[0m\u001b[0m | time: 0.010s\n","| Adam | epoch: 996 | loss: 0.51269 - acc: 0.9688 -- iter: 10/55\n","Training Step: 10948  | total loss: \u001b[1m\u001b[32m0.46320\u001b[0m\u001b[0m | time: 0.015s\n","| Adam | epoch: 996 | loss: 0.46320 - acc: 0.9719 -- iter: 15/55\n","Training Step: 10949  | total loss: \u001b[1m\u001b[32m0.41805\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 996 | loss: 0.41805 - acc: 0.9747 -- iter: 20/55\n","Training Step: 10950  | total loss: \u001b[1m\u001b[32m0.37720\u001b[0m\u001b[0m | time: 0.025s\n","| Adam | epoch: 996 | loss: 0.37720 - acc: 0.9772 -- iter: 25/55\n","Training Step: 10951  | total loss: \u001b[1m\u001b[32m0.34079\u001b[0m\u001b[0m | time: 0.031s\n","| Adam | epoch: 996 | loss: 0.34079 - acc: 0.9795 -- iter: 30/55\n","Training Step: 10952  | total loss: \u001b[1m\u001b[32m0.30937\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 996 | loss: 0.30937 - acc: 0.9816 -- iter: 35/55\n","Training Step: 10953  | total loss: \u001b[1m\u001b[32m0.28002\u001b[0m\u001b[0m | time: 0.042s\n","| Adam | epoch: 996 | loss: 0.28002 - acc: 0.9834 -- iter: 40/55\n","Training Step: 10954  | total loss: \u001b[1m\u001b[32m0.25302\u001b[0m\u001b[0m | time: 0.046s\n","| Adam | epoch: 996 | loss: 0.25302 - acc: 0.9851 -- iter: 45/55\n","Training Step: 10955  | total loss: \u001b[1m\u001b[32m0.22834\u001b[0m\u001b[0m | time: 0.051s\n","| Adam | epoch: 996 | loss: 0.22834 - acc: 0.9866 -- iter: 50/55\n","Training Step: 10956  | total loss: \u001b[1m\u001b[32m0.20612\u001b[0m\u001b[0m | time: 0.057s\n","| Adam | epoch: 996 | loss: 0.20612 - acc: 0.9879 -- iter: 55/55\n","--\n","Training Step: 10957  | total loss: \u001b[1m\u001b[32m0.18621\u001b[0m\u001b[0m | time: 0.004s\n","| Adam | epoch: 997 | loss: 0.18621 - acc: 0.9891 -- iter: 05/55\n","Training Step: 10958  | total loss: \u001b[1m\u001b[32m0.16919\u001b[0m\u001b[0m | time: 0.009s\n","| Adam | epoch: 997 | loss: 0.16919 - acc: 0.9902 -- iter: 10/55\n","Training Step: 10959  | total loss: \u001b[1m\u001b[32m0.15405\u001b[0m\u001b[0m | time: 0.013s\n","| Adam | epoch: 997 | loss: 0.15405 - acc: 0.9902 -- iter: 15/55\n","Training Step: 10960  | total loss: \u001b[1m\u001b[32m0.14075\u001b[0m\u001b[0m | time: 0.020s\n","| Adam | epoch: 997 | loss: 0.14075 - acc: 0.9912 -- iter: 20/55\n","Training Step: 10961  | total loss: \u001b[1m\u001b[32m0.12985\u001b[0m\u001b[0m | time: 0.024s\n","| Adam | epoch: 997 | loss: 0.12985 - acc: 0.9921 -- iter: 25/55\n","Training Step: 10962  | total loss: \u001b[1m\u001b[32m0.11753\u001b[0m\u001b[0m | time: 0.028s\n","| Adam | epoch: 997 | loss: 0.11753 - acc: 0.9929 -- iter: 30/55\n","Training Step: 10963  | total loss: \u001b[1m\u001b[32m0.10695\u001b[0m\u001b[0m | time: 0.034s\n","| Adam | epoch: 997 | loss: 0.10695 - acc: 0.9936 -- iter: 35/55\n","Training Step: 10964  | total loss: \u001b[1m\u001b[32m0.09805\u001b[0m\u001b[0m | time: 0.041s\n","| Adam | epoch: 997 | loss: 0.09805 - acc: 0.9942 -- iter: 40/55\n","Training Step: 10965  | total loss: \u001b[1m\u001b[32m0.08904\u001b[0m\u001b[0m | time: 0.047s\n","| Adam | epoch: 997 | loss: 0.08904 - acc: 0.9948 -- iter: 45/55\n","Training Step: 10966  | total loss: \u001b[1m\u001b[32m0.08904\u001b[0m\u001b[0m | time: 0.053s\n","| Adam | epoch: 997 | loss: 0.08904 - acc: 0.9953 -- iter: 50/55\n","Training Step: 10967  | total loss: \u001b[1m\u001b[32m0.08124\u001b[0m\u001b[0m | time: 0.058s\n","| Adam | epoch: 997 | loss: 0.08124 - acc: 0.9958 -- iter: 55/55\n","--\n","Training Step: 10968  | total loss: \u001b[1m\u001b[32m0.06720\u001b[0m\u001b[0m | time: 0.006s\n","| Adam | epoch: 998 | loss: 0.06720 - acc: 0.9962 -- iter: 05/55\n","Training Step: 10969  | total loss: \u001b[1m\u001b[32m0.06080\u001b[0m\u001b[0m | time: 0.011s\n","| Adam | epoch: 998 | loss: 0.06080 - acc: 0.9966 -- iter: 10/55\n","Training Step: 10970  | total loss: \u001b[1m\u001b[32m0.06080\u001b[0m\u001b[0m | time: 0.016s\n","| Adam | epoch: 998 | loss: 0.06080 - acc: 0.9969 -- iter: 15/55\n","Training Step: 10971  | total loss: \u001b[1m\u001b[32m0.05622\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 998 | loss: 0.05622 - acc: 0.9972 -- iter: 20/55\n","Training Step: 10972  | total loss: \u001b[1m\u001b[32m0.05189\u001b[0m\u001b[0m | time: 0.030s\n","| Adam | epoch: 998 | loss: 0.05189 - acc: 0.9975 -- iter: 25/55\n","Training Step: 10973  | total loss: \u001b[1m\u001b[32m0.04788\u001b[0m\u001b[0m | time: 0.035s\n","| Adam | epoch: 998 | loss: 0.04788 - acc: 0.9978 -- iter: 30/55\n"]}]},{"cell_type":"code","source":["#Creating a function to tokenize, stem and bag the input\n","def bag_of_words(s, words):\n","    bag = [0 for _ in range(len(words))]\n","\n","    s_words = nltk.word_tokenize(s)\n","    s_words = [stemmer.stem(word.lower()) for word in s_words]\n","\n","    for se in s_words:\n","        for i, w in enumerate(words):\n","            if w == se:\n","                bag[i] = 1\n","            \n","    return numpy.array(bag)\n","\n","#Creating a funtion for taking inputs from the user,applying the model to it and printing out a random response from the tag with the highest probability\n","def chat():\n","    print(\"Hello, I am Kora,the moringa school FAQ bot (type quit to stop)!\")\n","    while True:\n","        inp = input(\"You: \")\n","        if inp.lower() == \"quit\":\n","            break\n","\n","        results = model.predict([bag_of_words(inp, words)])[0]\n","        results_index = numpy.argmax(results)\n","        tag = labels[results_index]\n","\n","        if results[results_index] >0.5:\n","            for tg in data[\"intents\"]:\n","              if tg['tag'] == tag:\n","                  responses = tg['responses']\n","\n","            print(random.choice(responses))\n","        else:\n","          print('I did not understand, kindly ask another question')\n","\n","chat()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":565},"id":"_Y1lwgH4SvTm","outputId":"b2562bba-7b81-43b1-aa65-ac0334b37e94","executionInfo":{"status":"error","timestamp":1681210948126,"user_tz":-180,"elapsed":297020,"user":{"displayName":"Antony Kiarie","userId":"16570573470675130201"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, I am Kora,the moringa school FAQ bot (type quit to stop)!\n","You: hi\n","Hello!\n","You: whats my name\n","I did not understand, kindly ask another question\n","You: do you offer accomodation\n","No, we currently don’t provide accommodation. However, there are a number of private hostels along Ngong road, which are close to the school.\n","You: how much do i pay\n","Yes, students that have enrolled to our remote classes are provided with data bundles\n","You: which courses are offered at moringa\n","No, we do not offer installment plans for the Prep course but we do for the core course\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-3d4292f52d8a>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'I did not understand, kindly ask another question'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-3d4292f52d8a>\u001b[0m in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello, I am Kora,the moringa school FAQ bot (type quit to stop)!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]}]}